{"config":{"lang":["en"],"separator":"[\\s\\-\\.]","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"opendesk","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>This Get Started guide explains how <code>opendesk</code> works, how to install <code>opendesk</code> on your preferred operating system, and how to create your first <code>opendesk</code> strategy! </p> <p>Getting Started</p> <p></p> <p>Main concepts introduces you to <code>opendesk</code>'s base model and development flow. You'll learn what makes <code>opendesk</code> the most powerful way to build systematic strategies, including the ability to customize existing setup. </p> <p></p> <p>Create Blocks following the three-step process outlined in <code>opendesk</code>'s core guidelines: process raw data, rank and compare these results, create and map a sequence of scores.</p> <p></p> <p>Optimize Blocks, which is the process of selecting the optimal mix of assets in a portfolio, with respect to the alpha scores, in order to maximize returns while minimizing risk. This method has been implemented to streamline the process of optimization and facilitate the integration of backtesting.</p> <p></p> <p>Backtest Blocks through the <code>opendesk</code> API. It enables the systematic evaluation of one or multiple strategies simultaneously. It provides access to a comprehensive range of features, thereby affording users the ability to leverage its full functionality and holds significant potential for success within the financial markets.</p> <p></p> <p>Installation helps you set up your virtual environment and walks you through installing <code>opendesk</code> on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>This website is dedicated to providing documentation for a Python package that utilizes the \"blocks\" method for implementing financial strategies. This approach involves the sequential application of rule-based modular strategies that can be easily modified and added or removed from the sequence as needed. The method's effectiveness lies in its ability to generate a score for each block, enabling efficient testing and optimization, as well as facilitating research and machine learning integration. Furthermore, the method allows for the combination of various sources of alpha in a risk-efficient manner. </p> <p>The building blocks approach provides a comprehensive yet flexible means of financial strategy development and data analysis, making it an essential tool for professionals in the finance industry, whether experienced or novice.</p> <p>Documentation</p> <p></p> <p>Strategy is the main implementation of the library for portfolio construction, which initializes the execution of the sequence of blocks and allows to compute tilts/exposures, create and backtest target portfolios.</p> <p></p> <p>Signals constructs transfomed dataset, which are used to build strategies. Examples including High Frequency Indicators, Nowcasts/Forecasts or Alpha Signals.</p> <p></p> <p>Synthetic market or economic data refers to artificially generated datasets that mimic real-world economic or market conditions. Synthetic data can be used for various purposes, including research, testing economic models, simulating extreme events, evaluating strategies, or simply providing training data.</p> <p></p> <p>Workflow eliminates the need for manual resource creation and configuration, and also helps to manage the dependencies between resources. As a result, you can focus more on developing your applications that run on AWS, rather than spending time on resource management.</p>"},{"location":"blocks/en/","title":"Blocks","text":"<p><code>ActuarialCapital</code> is highly invested in staying abreast of the latest developments in quantitative analysis and continually seeks out new and innovative techniques and tools that can aid in making well-informed decisions. This webpage features a selection of internally conducted research and development aimed at enhancing our asset management approach. We make it a regular practice to share our latest research and advancements with the aim of providing valuable insights.</p> <p></p> <p>Arbitrage and Academic Publications - The objective of this research is to analyze the post-publication performance of investment strategies, focusing on the extent to which market anomalies can be exploited and whether profits decline once it gains widespread attention.</p> <p></p> <p>Building Blocks - The annualized long-term return of stock markets can be decomposed into three fundamental elements:</p> <ul> <li>Income</li> <li>Growth</li> <li>Valuation</li> </ul> <p></p> <p>Clustering methods are becoming increasingly common in portfolio management, aiming to address the inefficiencies of sectoral clustering analysis (particularly GICS). We have developed modeling techniques based on graph theory and community detection to enhance the clustering analysis.</p> <p></p> <p>Cost of Capital - The Equity Risk Premium (ERP) is the price of risk on the stock markets, and it is a key element in estimating the costs of equity and capital both in corporate finance and in valuation. Since portfolio construction relies on balancing risk and return, understanding the value of ERP remains crucial.</p> <p></p> <p>Market Cycle - Our objective is to determine and characterize different stages of market activity. Gaining insight into the analysis of market cycles, both within a given dataset and beyond, may assist in formulating optimal investment strategies.</p> <p></p> <p>Regime Switching and phase identification can be accomplished by using either a subjective knowledge or a data-driven approach:</p> <ol> <li>One method is to classify market regimes based on observable features such as volatility levels, shifts in monetary policy, and changes in investor sentiment</li> <li>In contrast, a data-driven approach involves utilizing historical data to identify market regimes automatically, ie the unsupervised learning page</li> </ol> <p></p> <p>Dynamic Time Warping (DTW) is a technique used in time series analysis to measure the similarity between two time series that may have differences in speed or frequency.</p> <p></p> <p>ESG - Factor investing is an investment strategy that seeks to identify and invest in specific factors that explain the returns of financial assets such as stocks and bonds. In this context, the effort undertaken by <code>ActurialCapital</code> is focused on the risk premium that ESG factors can potentially offer.</p> <p></p> <p>Nowcasting utilizes statistical techniques to provide early and short-term predictions for key quarterly indicators such as GDP growth, CPI, and OECD leading indicators.</p> <p></p> <p>Volatility Targeting is a popular technique employed by portfolio managers to manage risk by adjusting the portfolio exposure to maintain a desired level of volatility. The objective is to keep the portfolio's volatility as close as possible to the target value.</p> <p></p> <p>Network Graph The correlation structure of securities is a key quantity in describing and managing the risk properties of portfolios. Relatedly, and more recently, application of network theory within financial markets has begun to provide insight into the interconnected nature of securities.</p> <p></p> <p>Short Interests data is often used by traders and investors to gauge market sentiment, as a high level of short interest can indicate that many investors are bearish on a particular stock.</p> <p></p> <p>opendesk PRO Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/applied_symbolic_regression/","title":"Symbolic Regression","text":""},{"location":"blocks/en/applied_symbolic_regression/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/arbitrage_et_publication_academique/","title":"Arbitrage and Publications","text":""},{"location":"blocks/en/arbitrage_et_publication_academique/#definition","title":"Definition","text":"<p>The objective of this research is to analyze the post-publication performance of investment strategies, focusing on the extent to which market anomalies can be exploited and whether profits decline once it gains widespread attention. Additionally, we explore the possibility of synchronization between different investment strategies.</p> <p>In economics and finance, the analysis of market anomalies aims to identify profitable trading strategies based on market inefficiencies. However, the exploitation of these anomalies is confined by arbitrage limits, which originate from the risks and expenses associated with carrying out these trades. Our investigation delves into the concept of arbitrage limits as they relate to market anomalies and the impact of human psychology. These are frequently discussed matters of interest.</p>"},{"location":"blocks/en/arbitrage_et_publication_academique/#arbitrage-limits","title":"Arbitrage Limits","text":"<p>In financial markets, arbitrage is a strategy aimed at taking advantage of observed price divergences between two markets or similar financial instruments in order to make a profit. This technique relies on the assumption that the prices of similar assets should be identical or very close, given market conditions and associated risk factors. In the context of market efficiency, which assumes that asset prices reflect all available information, arbitrage opportunities are therefore limited.</p> <p>Arbitrage 101</p> <p>For example, arbitrage is a financial strategy that involves exploiting price discrepancies observed between financial markets to generate profits without taking significant risks. When an asset is traded at a lower price on two different markets, an arbitrageur can buy this asset on the market where it is undervalued and immediately sell it on the market where it is overvalued, thus realizing a net profit. This strategy relies on the assumption that markets are not perfectly efficient and that it is possible to identify short-term arbitrage opportunities. However, competition among arbitrageurs usually limits the duration of these opportunities and leads to a reduction in the observed price gap between the relevant markets.</p> <p>This practice requires a thorough analysis and understanding of the different markets, the financial instruments traded therein, distortions caused by unforeseen events, or irrational investor behavior. These strategies are based on systematic models theories.</p> <p>The classical definition of arbitrage implies a risk-free exchange that requires no capital investment. However, in practice, any arbitrage activity involves some degree of risk and capital investment:</p> <ul> <li>Price discrepancies may correct more slowly or more quickly than expected</li> <li>Unforeseen events may change market conditions</li> <li>Regulatory constraints on the use of leverage, making it difficult to fully exploit market inefficiencies</li> </ul> <p>Moreover, market anomalies exhibit returns that vary over time, and a significant portion of returns may be preserved due to the risks associated with arbitrage. This suggests that the efficiency of arbitrage (in the sense that the price of an asset reflects its fundamental value) remains limited, especially in extreme market conditions.</p> <p>An example of an arbitrage limit is the \"dispersion of risk premiums,\" which refers to the variation in security returns within a given market. This dispersion creates arbitrage opportunities for investors, but these opportunities are limited by transaction costs, counterparty risks, and leverage constraints. Frazzini and Pedersen (2014) show that hedge funds have suboptimal positions due to leverage constraints and transaction costs and thus cannot fully exploit arbitrage opportunities.</p>"},{"location":"blocks/en/arbitrage_et_publication_academique/#returns-and-publications","title":"Returns and Publications","text":"<p>Academic research on market anomalies can have a significant impact on the profitability of these strategies. When researchers publish academic studies on market anomalies, it can affect the profitability of strategies. Although similar strategy funds may experience a decrease in profitability after disclosure, it is important to note that returns do not immediately decrease. In fact, a significant proportion of profitability is still present. Studies have shown that returns of published strategies may decrease up to 26% out of sample and up to 58% during the five years following publication (Novy-Marx, 2013). However, even after five years, a non-negligible proportion of the anomaly-related profitability is still preserved and can still be used profitably in a diversified portfolio as a risk factor (smart beta).</p>"},{"location":"blocks/en/arbitrage_et_publication_academique/#factor-timing","title":"Factor Timing","text":"<p>Est-il possible de compenser la d\u00e9t\u00e9rioration des performances en choisissant correctement le moment d'ex\u00e9cuter des strat\u00e9gies ou des anomalies? La litt\u00e9rature reconna\u00eet diverses approches pour planifier le moment d'ex\u00e9cution des strat\u00e9gies d'anomalies:</p> <p>Is it possible to offset the deterioration in performance by timing the execution of strategies or anomalies correctly? The literature recognizes various approaches to timing anomaly detections strategies:</p> <ul> <li>Huang and Huang, in their study titled \"Real-Time Profitability of Published Anomalies: An Out-of-Sample Test,\" consider a long-only strategy with a universe of published anomalies and recursively choose the best performance during a given training period each year. According to this study, the strategy can outperform the stock market even in the presence of transaction costs. The results suggest that anomalies can persist even after controlling for optimization bias.</li> <li>Basu and Hung, in their study titled \"Anomaly Timing,\" constructed arbitrage portfolios based on historical market returns. These portfolios have a similar or higher Sharpe ratio even with transaction costs and lower volatility. Such portfolios have significant alphas even for factor models that can explain portfolio returns.</li> <li>Yang, in his study titled \"Decomposing Factor Momentum,\" shows us that the profitability of the momentum factor, when periodically readjusted, is empirically superior to factor timing.</li> </ul>"},{"location":"blocks/en/arbitrage_et_publication_academique/#key-takeaways","title":"Key Takeaways","text":"<p>Based on academic literature, arbitrage strategies have limitations due to factors such as transaction costs, market uncertainty, limited capital capacity, and associated risks (Fama, 1998; Lakonishok et al., 1994). In addition, the publication of research results can lead to a decrease in profits due to increased competition and imitation of strategies (Chan and Lakonishok, 1993).</p> <p>However, some researchers have proposed ways to mitigate these negative effects. For example, Hirshleifer and Shumway (2003) suggested that investors could anticipate the publication of research results by being alert to research publications, thus allowing for early adoption of ideas before official publication. This strategy has been successfully adopted by some investors (Fung and Hsieh, 2000).</p> <p>Another approach is to rotate possible arbitrages for a given asset class (Asness et al., 2013). It is also advisable to diversify portfolios by including a set of arbitrages rather than focusing on a single unique strategy (Frazzini and Pedersen, 2014).</p> <p>Finally, it is important to note that investors must adapt research ideas to their own investment context, taking into account their risk tolerance, investment universe, and portfolio composition (Ang et al., 2010). Investors can also combine research ideas with other strategies to reduce the concentration of a single strategy (Asness et al., 2013).</p>"},{"location":"blocks/en/arbitrage_et_publication_academique/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/asset_class_correlations/","title":"Asset Class Correlations","text":""},{"location":"blocks/en/asset_class_correlations/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/building_blocks/","title":"Building Blocks","text":""},{"location":"blocks/en/building_blocks/#definition","title":"Definition","text":"<p>The annualized long-term return of stock markets can be decomposed into three fundamental elements:</p> <ul> <li>Income: Empirically, dividend yield represents over half of the nominal market return and 70% of its real return. This suggests that investors should pay particular attention to companies that pay regular and reliable dividends.</li> <li>Growth: Earnings growth per share is another important source of stock returns. This growth can be influenced by factors such as innovation, economies of scale, and operational improvements. However, it is important to note that inflation can also play a significant role in earnings growth. Therefore, financial analysts must take into account the impact of inflation on earnings growth projections.</li> <li>Valuation: Finally, stock valuation, which refers to the price of stocks relative to earnings or cash flows, is a key factor in capital appreciation. Investors must be aware that high valuations can be a source of risk for stocks, as a correction downward can lead to significant losses. Therefore, it is important to closely monitor stock valuation levels before making an investment decision.</li> </ul> <p>The building block methodology reflects a total stock return approach - taking into account both income and capital appreciation (i.e., price change over time). Therefore, our forecast building blocks consist of estimates for return (as an income factor) as well as earnings and sales growth and valuation change (as capital appreciation factors).</p> <p>It is a valuable tool for financial analysts as it allows them to analyze the different sources of stock returns and evaluate their relative contribution to overall market performance. By using this type of method, analysts can better understand long-term trends in stock market returns and provide more accurate forecasts for investors.</p>"},{"location":"blocks/en/building_blocks/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/cds_based_models/","title":"CDS Based Models","text":""},{"location":"blocks/en/cds_based_models/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/clustering/","title":"Clustering","text":""},{"location":"blocks/en/clustering/#definition","title":"Definition","text":"<p>Clustering methods are becoming increasingly common in portfolio management, aiming to address the inefficiencies of sectoral clustering analysis (particularly GICS). This study focuses on the most popular clustering methods, such as Partitioning Around Medoids (PAM), Hierarchical Clustering, and Gaussian Mixture Model, and provides examples of their use in portfolio management. We have developed modeling techniques based on graph theory and community detection to enhance the clustering analysis.</p>"},{"location":"blocks/en/clustering/#clustering-techniques","title":"Clustering Techniques","text":"<p>Preprocessing</p> <p>To enhance the accuracy of clustering operations, it is preferable to transform the data prior to any analysis. Some clustering techniques are more reliable when the parameters are normalized and reduced to a few dimensions, for instance, Principal Component Analysis (PCA) and Multi-Dimensional Scaling (MDS). Nonetheless, it is crucial to consider the data characteristics and the selected sample. Research indicates that normalizing the data can boost the efficiency of clustering techniques, particularly when dealing with heterogeneous data.</p> <p>References: </p> <ul> <li>Wang, F. et al. (2011). Improved K-Means Clustering Algorithm Based on Data Normalization. Journal of Software Engineering and Applications, 4(4), 265-270. doi:10.4236/jsea.2011.44030</li> <li>Juszczak, P. (2015). The influence of normalization on clustering of data with high dimensionality. Journal of Medical Informatics &amp; Technologies, 24(2), 79-84. doi:10.2478/jmit-2014-0010)</li> </ul>"},{"location":"blocks/en/clustering/#partitioning-around-medoids","title":"Partitioning Around Medoids","text":"<p>Partitioning Around Medoids (PAM) is a non-hierarchical clustering method developed by Kaufman and Rousseeuw in 1987. PAM utilizes the concept of a centroid, which is the central point of a cluster. Unlike mean-based clustering methods (such as k-means), PAM uses medoids as centroids, which are actual objects within the cluster rather than mean points (Jankowski, 2020).</p> <p>PAM has been extensively studied and applied in various fields, including biology, finance, engineering, and psychology, as evidenced by the research papers by Davari (2015), Mu (2017), and Kwong (2008). It is regarded as one of the simplest and most effective clustering methods, particularly for non-Gaussian, high-dimensional, or outlier-prone data, as presented by De Soete (1986), Kuo, Liang, and Liang (2013).</p> <p>The PAM clustering is an iterative process, which involves several steps. </p> <ul> <li>First, a predetermined number of clusters is chosen, and random objects are assigned to each cluster</li> <li>Then, a medoid is selected for each cluster, which is the object in the cluster with the minimum average dissimilarity to all other objects in the cluster</li> <li>Next, objects are assigned to clusters based on their dissimilarity to the medoid of each cluster</li> <li>A new iteration begins with a new choice of medoids for each cluster, and so on until the clusters no longer change.</li> </ul> <p>PAM is often compared to k-means in terms of performance and results. According to several comparative studies, PAM is often considered superior to k-means in terms of cluster stability, robustness to outliers, and accuracy.</p> <p>In conclusion, PAM is a popular and effective clustering method that relies on medoids to create clusters. This method is widely used in various research fields and offers an interesting alternative to the k-means method.</p>"},{"location":"blocks/en/clustering/#hierarchical-clustering","title":"Hierarchical Clustering","text":"<p>Hierarchical Clustering is a hierarchical clustering method that groups similar objects into successive clusters, forming a tree-like structure called a dendrogram (Jain, Murty, &amp; Flynn 1999). Unlike non-hierarchical clustering methods such as k-means, Hierarchical Clustering does not require specifying a number of clusters in advance, but determines the number of clusters from the dendrogram structure.</p> <p>There are two types of Hierarchical Clustering: </p> <ul> <li>Agglomerative: The agglomerative algorithm starts by considering each object as a cluster and gradually merges the most similar clusters to form larger clusters.</li> <li>Divisive.  The divisive algorithm starts with all objects in a single cluster and gradually divides the cluster into smaller clusters by separating the least similar objects. </li> </ul> <p>One advantage of Hierarchical Clustering is that it provides a visualization of the cluster structure in the form of a dendrogram, which allows identifying the different levels of clusters and understanding the similarity between them. However, the algorithm can be costly in terms of computation, especially for large amounts of data.</p> <p>Hierarchical Clustering is commonly used in various applications such as bioinformatics, customer segmentation, and document classification (Berkhin 2006). Hybrid approaches that combine the advantages of hierarchical and non-hierarchical clustering methods are also widely studied in the literature (Zhang, Ramakrishnan, Livny 1996).</p> <p>In summary, Hierarchical Clustering is a useful clustering method for identifying the structure of clusters and determining the number of clusters from the dendrogram structure. It is widely used in various fields, but its computational cost should be considered for large datasets. Hybrid approaches that combine the advantages of hierarchical and non-hierarchical clustering methods are also worth exploring.</p>"},{"location":"blocks/en/clustering/#gaussian-mixture","title":"Gaussian Mixture","text":"<p>The Gaussian Mixture is a clustering method that models each cluster as a multidimensional Gaussian probability distribution (McLachlan, Peel 2000). Unlike centroid-based clustering methods, the Gaussian Mixture allows for more flexible modeling of the data distribution by considering that each cluster can have a different distribution shape.</p> <p>The Gaussian Mixture algorithm seeks to estimate the parameters of each cluster's Gaussian distribution, such as mean and covariance matrix, as well as the relative weights of each cluster in the model. The model parameters are estimated using an optimization method such as the EM (Expectation-Maximization) algorithm.</p> <p>One of the advantages of Gaussian Mixture is its ability to model clusters with complex and non-spherical distribution shapes, which is often the case in real data. It is also possible to determine the optimal number of clusters using information criteria such as the Akaike criterion or the Bayes criterion.</p> <p>The Gaussian Mixture is widely used in various applications such as pattern recognition, image segmentation, and data modeling for machine learning (Bishop 2006). However, as with any clustering method, the quality of the results heavily depends on the choice of algorithm, optimization method, and parameter selection.</p>"},{"location":"blocks/en/clustering/#supervised-methods-optimal-number-of-clusters-selection","title":"Supervised Methods: Optimal Number of Clusters Selection","text":"<p>Methodes</p> <p>The optimal number of clusters selection is a crucial step in clustering techniques. The methods presented here, the Elbow Method, the Silhouette Method, and the Gap Statistic, are scientific methods commonly used to determine the appropriate number of clusters and thus ensure an adequate representation of the data.</p> <p>The selection of the appropriate number of clusters is crucial to achieve meaningful results, as too few or too many clusters can lead to oversimplification or overcomplication of the data structure. Different approaches can be used to determine the optimal number of clusters, such as visualization techniques, statistical methods, and performance evaluation metrics.</p>"},{"location":"blocks/en/clustering/#elbow-method","title":"Elbow Method","text":"<p>The Elbow Method is a scientific technique commonly used to determine the appropriate number of clusters. It involves plotting the explained variation of data points against the number of clusters. The decision rule for determining the appropriate number of clusters is based on selecting the angle of the elbow in the curve, which means finding a \"break\" in the elbow diagram. Although simple and quick, this method can be subjective and may not always work with complex or unstructured data (Jambu 2016).</p>"},{"location":"blocks/en/clustering/#silhouette-method","title":"Silhouette Method","text":"<p>The Silhouette Method is another commonly used method for determining the optimal number of clusters. It is based on measuring the similarity of an object to its own cluster compared to the other clusters. This measure is based on the average distance of an object to the other clusters. The silhouette ranges from -1 to 1, where a high value indicates that the object is well-matched to its own cluster and poorly matched to the other clusters. If most objects have a high silhouette value, the clustering is considered well-done. On the other hand, if many objects have a low silhouette value, the clustering may have too few or too many clusters. The Silhouette Method provides a representation of how each object has been classified. This method is more objective than the Elbow Method, but it can be more computationally expensive.</p>"},{"location":"blocks/en/clustering/#gap-statistic","title":"Gap Statistic","text":"<p>In the Gap Statistic method, the dispersion within the cluster, represented by \\(W\\), is standardized by taking the logarithm of it and comparing it to its expected value under a null reference distribution of the data. The optimal number of clusters is then determined as the one that maximizes the Gap Statistic. This method allows for determining whether the clustering structure significantly deviates from a random uniform distribution. Although the Gap Statistic method is objective and robust for identifying the optimal number of clusters, it may be more challenging to implement than the preceding methods (Tibshirani, Walther, Hastie 2001).</p>"},{"location":"blocks/en/clustering/#clustering-and-risk-parity","title":"Clustering and Risk Parity","text":"<p>Asset allocation is a key issue in quantitative finance. The risk parity method is one of the most common approaches to balancing risks by assigning weights to assets in inverse proportion to their volatility. However, this method does not take into account the diversity and composition of the portfolio, which limits its effectiveness.</p> <p>Clustering offers an alternative for portfolio management by:</p> <ul> <li>Simplifying management by dividing assets into groups</li> <li>Improving understanding of portfolio behavior in different market conditions</li> <li>Creating more diversified portfolios to reduce risk</li> </ul> <p>Clustering can also improve the risk parity method by overcoming its limitations. By grouping assets into categories, this method can be applied to each group to build more diversified and market-resistant multi-asset portfolios. Studies have confirmed the effectiveness of this approach, including the one conducted by Meucci and Ardia in \"Beyond Black-Litterman: Clustering and Improved Dynamic Portfolio Allocation\" (2011), which demonstrated the effectiveness of clustering in constructing multi-asset portfolios with the risk parity method.</p> <p>Risk parity method is used in combination with clustering. This method can help investors manage risk while maximizing their returns.</p>"},{"location":"blocks/en/clustering/#weighting-scheme","title":"Weighting Scheme","text":"<p>In the context of portfolio management using clustering techniques, it is essential to determine the appropriate weights to be assigned to the assets within each cluster. Different approaches can be considered for assigning weights, such as based on the assets' level of volatility, their contribution to the overall risk of the portfolio, or their historical performance.</p>"},{"location":"blocks/en/clustering/#intra-cluster-risk-parity","title":"Intra Cluster Risk Parity","text":"<p>The intra-cluster risk parity method (Roncalli, 2013) is a prevalent technique for assigning weights to assets. This method seeks to balance the risk across assets within a cluster by assigning weights based on their respective contributions to the overall volatility. As a result, this approach allows for the creation of multi-asset portfolios that are more diversified and more resistant to market changes within each cluster.</p>"},{"location":"blocks/en/clustering/#markowitz-optimal-weighting","title":"Markowitz Optimal Weighting","text":"<p>The Markowitz optimal weighting method (Markowitz, 1952) is another commonly used approach for weighting assets within a cluster. This method aims to maximize portfolio returns while minimizing risk. Consequently, this approach can be employed to assign weights to assets within each cluster in order to optimize portfolio performance.</p>"},{"location":"blocks/en/clustering/#weighted-risk-parity","title":"Weighted Risk Parity","text":"<p>In addition, there are techniques that seek to merge the risk parity and Markowitz optimal weighting approaches to construct more diversified and performant multi-asset portfolios. One such method is the weighted risk parity method (Choueifaty and Froidure, 2012), which strives to balance both intra-cluster and inter-cluster risks by utilizing Markowitz optimal weighting for assets between clusters. By doing so, this method allows for the creation of multi-asset portfolios that are more diversified and able to withstand market changes both within and between clusters.</p>"},{"location":"blocks/en/clustering/#key-takeaways","title":"Key Takeaways","text":"<p>Optimizing a portfolio requires integrating the clustering method with risk parity weighting. The clustering analysis allows us to group similar assets based on their risk characteristics. However, equal weighting should not be used within or between clusters. As a result, cluster weighting should be based on the risks associated with each cluster (Choueifaty and Coignard, 2008).</p> <p>Nonetheless, clustering is not always an effective means of portfolio optimization, as it may not improve portfolio performance or the Sharpe ratio in all circumstances. In some instances, clustering may even lead to a greater weighting of underperforming assets. Rather than identifying outperforming assets, clustering should be used to manage risk effectively. It is a useful technique for determining the number of similar assets/strategies in the portfolio and for diversifying them efficiently. However, clustering algorithms should be used in conjunction with a return predictor, such as momentum, to select suitable assets for the portfolio (Roncalli, 2013).</p>"},{"location":"blocks/en/clustering/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/cost_of_capital/","title":"Cost of Capital","text":""},{"location":"blocks/en/cost_of_capital/#definition","title":"Definition","text":"<p>Info</p> <p>The cost of capital introduces the concept of risk and return, according to which riskier investments should have a higher expected return than safer investments. In this regard, expected returns can be written as \\(r_n = k_d + k_e\\), where \\(k_d\\) is the cost of debt, or the risk-free rate (the nominal return on bonds), and \\(k_e\\) is the cost of equity, or Equity Risk Premium (ERP), an additional return for the risk of holding stocks.</p> <p>The Equity Risk Premium (ERP) is the price of risk on the stock markets, and it is a key element in estimating the costs of equity and capital both in corporate finance and in valuation. Since portfolio construction relies on balancing risk and return, understanding the value of ERP remains crucial.</p>"},{"location":"blocks/en/cost_of_capital/#methods","title":"Methods","text":"<p>We highlight four different concepts to calculate the Equity Risk Premium:</p>"},{"location":"blocks/en/cost_of_capital/#historical-equity-risk-premium","title":"Historical Equity Risk Premium","text":"<p>What stocks have returned in excess of bonds in the past.</p>"},{"location":"blocks/en/cost_of_capital/#required-equity-risk-premium","title":"Required Equity Risk Premium","text":"<p>The extra return on bonds that investors demand to invest more in stocks (often determined by surveys of investors and managers).</p>"},{"location":"blocks/en/cost_of_capital/#expected-equity-risk-premium","title":"Expected Equity Risk Premium","text":"<p>What stocks are expected to return in excess of bonds (this is a forecast of stock returns, which can therefore increase during periods of bubbles).</p>"},{"location":"blocks/en/cost_of_capital/#implied-equity-risk-premium","title":"Implied Equity Risk Premium","text":"<p>The excess return over bonds implied in the current market price, which must be estimated (e.g. growth assumptions). We focus on this latter concept in our internal research. Thus, there are different techniques that could be used:</p> <ul> <li>Dividend-based approach:<ul> <li>Evaluate dividends and share buybacks</li> <li>Calculate a discount rate</li> </ul> </li> <li>Earnings-based approach:<ul> <li>Base the valuation on earnings instead of dividends</li> <li>Earnings aggregate both income and growth</li> <li>Compensate for low or non-existent dividend payouts</li> </ul> </li> <li>Residual approach:<ul> <li>Companies' ability to earn more than is necessary to reinvest and grow the business</li> </ul> </li> <li>Others:<ul> <li>Cross-sectional regression</li> <li>Default risk</li> </ul> </li> </ul>"},{"location":"blocks/en/cost_of_capital/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/cycles_de_marche/","title":"Market Cycle","text":"<p>Our objective is to determine and characterize different stages of market activity. Gaining insight into the analysis of market cycles, both within a given dataset and beyond, may assist in formulating optimal investment strategies.</p>"},{"location":"blocks/en/cycles_de_marche/#introduction","title":"Introduction","text":"<p>While academics have focused on type I (false positive) and type II errors (false negative) for investment strategy development, they have failed to analyse changing market conditions. Investors are interested less in knowing the statistical significance of their strategies, than in knowing whether their strategy will work in the future. Whether it will work in the future is dependent on the durability of market conditions which existed while developing it. </p> <p>Phase identification enables investors to more accurately assess economic regimes through time, beyond the ongoing switch between market risk-on and off. It makes the point of new paradigm for both quant and fundamental PMs, in order to identify a cause-effect mechanism to develop a thematic and capturing asymmetrical distribution:</p> <ul> <li>Avoid all-regime strategies (Marcos Lopez de Prado, 2019)</li> <li>Allow regime-specific mean and standard deviation estimates</li> </ul> <p>Maximising returns means applying the below dynamic investment approach associated with a cycle:</p> <ul> <li>Contrarian: When the rate of change is null (peak/trough)<ul> <li>Long risky assets as they fall</li> <li>Short risky assets as they rise</li> </ul> </li> <li>Trend-following otherwise (momentum - crowded investment styles)<ul> <li>Long risky assets as they rise</li> <li>Short risky assets as they fall</li> </ul> </li> </ul> <p>Understanding the balance between the size of the two groups is essential for market behaviour:</p> <ul> <li>When contrarian strategies succeed for a while, they attract followers and markets may become too stable (slow to adjust to economic developments)</li> <li>Conversely, when momentum strategies succeed for a while, they attract new followers and markets may become too volatile (as such overreacting to news flows)</li> </ul> <p>Excess popularity of one approach eventually results in excess gains for the opposite style, a process which over time should drive toward a balance. Therefore, long term returns should be positive (negative) only if the overall average cycle trend is upward (downward) sloping. We call this phenomenon Growth or wealth creation (destruction).</p>"},{"location":"blocks/en/cycles_de_marche/#primer-on-economic-cycles","title":"Primer on Economic Cycles","text":"<p>In order to guide asset allocation decisions and allocate to industries at the optimal time, we believe the emerging markets growth cycle plays an important role. We combine top-down insights and expert bottom-up fundamental analysis to understand the dynamics and drivers that determine industry performance within the cycle. In addition, the duration of a traditional growth cycle is considered in terms of years and is often referenced in relation to the progression of credit growth and capital expenditure in a particular economy</p>"},{"location":"blocks/en/cycles_de_marche/#business-cycle","title":"Business Cycle","text":"<p>Business cycles are a type of fluctuation found in the aggregate economic activity of nations that organize their work mainly in business enterprises: a cycle consists of expansions occurring at about the same time in many economic activities, followed by similarly general recessions, contractions, and revivals which merge into the expansion phase of the next cycle. Burns and Mitchell (1946)</p> <p>The business cycle is meant to reproduce the cycle of the global level of activity of a country. The turning points of that cycle:</p> <ul> <li>Peak \u201cB\u201d</li> <li>Trough \u201cC\u201d</li> </ul> <p>are separate periods of recessions from periods of expansions.</p> <pre><code>flowchart LR\n  B --&gt; C;\n  C --&gt; B;</code></pre> <p>Burns and Mitchell (1946) point out two main stylised facts of the economic cycle:</p> <ul> <li>Correlation among individual economic variables: Most of macroeconomic time series evolve together along the cycle</li> <li>Non-linearity: The effect of a shock depends on the rest of the economic environment. In other words, economic dynamics during economically stressful times are potentially different from normal times. For instance, small shock, such as a decrease in housing prices, can sometimes have large effects, such as recessions.</li> </ul>"},{"location":"blocks/en/cycles_de_marche/#growth-cycle","title":"Growth Cycle","text":"<p>Definition introduced by Mintz (1974):</p> <ul> <li>Fluctuations of the GDP around its long-term trend</li> <li>Absolute prolonged declines in the level of economic activity tend to be rare events, so that in practice many economies do not very often exhibit recessions in classical terms</li> </ul> <p>Growth cycle turning points have a clear meaning: </p> <ul> <li>Peak \u201cA\u201d is reached when the growth rate decreases below the trend growth rate </li> <li>Trough \u201cD\u201d is reached when the growth rate overpasses it again</li> </ul> <pre><code>flowchart LR\n  A --&gt; D;\n  D --&gt; A;</code></pre> <p>Those downward and upward phases are respectively named slowdown and acceleration.</p> <p>Tips</p> <p>If the long-term trend is considered as the estimated potential level (the potential output is the maximum amount of goods and services an economy can turn out at full capacity), then the growth cycle equals the output gap.</p> <p>A turning point of the output gap occurs when the current growth rate of the activity is above or below the potential growth rate, thereby signalling increasing or decreasing inflation pressures.</p>"},{"location":"blocks/en/cycles_de_marche/#abcd-approach","title":"ABCD approach","text":"<p>Info</p> <p>This framework improves thus the classical analysis of economic cycles by allowing sometimes two distinct phases, if the slowdown is not severe enough to become a recession, and sometimes four distinct phases, if the growth rate of the economy becomes negative enough to provoke a recession. In other words, all recessions involve slowdowns, but not all slowdowns involve recessions.</p> <p>Traditional growth cycle models tend to assume a linear progression through four discrete phases within the growth cycle: </p> <ul> <li>Recovery</li> <li>Expansion</li> <li>Slowdown</li> <li>Contraction</li> </ul> <p>The ABCD approach, Anas and Ferrara (2004), refines the description of different economic phases by jointly considering the classical business cycle and the growth cycle:</p> <ul> <li>Let us suppose that the current growth rate of the activity is above the trend growth rate (acceleration phase)</li> <li>Point \u201cA\u201d: The downward movement will first materialize when the growth rate will decrease below the trend growth rate</li> <li>Point \u201cB\u201d: If the slowdown gains in intensity, the growth rate could become negative enough to provoke a recession</li> <li>Point \u201cC\u201d: Eventually, the economy should start to recover and exits from the recession</li> <li>Point \u201cD\u201d: As the recovery strengthens, the growth rate should overpass its trend. However, a slowdown will not automatically translate into a recession: if the slowdown is not severe enough to become a recession, then Point \u201cA\u201d will not be followed by Point \u201cB\u201d, but by Point \u201cD\u201d.</li> </ul> <pre><code>flowchart LR\n  A --- B &amp; D --- C;</code></pre>"},{"location":"blocks/en/cycles_de_marche/#non-linear-approach","title":"Non-linear approach","text":"<p>We believe in an alternative construct, where a growth cycle consists of six phases and progresses in a non-linear fashion, with each phase playing out over approximately four-to-six months. A six-phase model can, in our view, better explain the growth narrative as well as offer a consistent framework to understanding industry returns across emerging markets equities. Our six-phase growth model \u2014 constructed using a large set of high frequency macroeconomic indicators from different emerging economies \u2014 tracks the transition across the stages of the cycle in a more consistent manner when compared to traditional models, in our view. For example, our growth model can depict the transition from the recovery phase to the expansion phase of the cycle, while also accounting for the alternative scenario that can occur when a recovery fails to take hold \u2014 as it repeatedly did in emerging markets between 2011 and 2015. Similarly, a slowdown phase does not always transition into a contraction phase per the traditional model. Our growth model accounts for periods when a slowdown is followed by a re-acceleration in growth before slowing again, as emerging markets experienced between 2004 and 2007.</p> <pre><code>flowchart LR\n  AD -.-&gt; A --- B &amp; D --- C;\n  C -.-&gt; BC;\n  BC -.-&gt; C;\n  A -.-&gt; AD;</code></pre>"},{"location":"blocks/en/cycles_de_marche/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/deep_learning_and_macro_finance_models/","title":"Deep Macro Learning","text":""},{"location":"blocks/en/deep_learning_and_macro_finance_models/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/dynamic_porfolio_rebalancing/","title":"Dynamic Rebalancing","text":""},{"location":"blocks/en/dynamic_porfolio_rebalancing/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/dynamic_time_warping/","title":"Dynamic Time Warping","text":""},{"location":"blocks/en/dynamic_time_warping/#definition","title":"Definition","text":"<p>Dynamic Time Warping (DTW) is a technique used in time series analysis to measure the similarity between two time series that may have differences in speed or frequency. In portfolio management, DTW can be used to compare the performance of a momentum strategy with that of a benchmark index or another portfolio.</p>"},{"location":"blocks/en/dynamic_time_warping/#momentum-strategy","title":"Momentum Strategy","text":"<p>The momentum strategy involves investing in assets that have had strong performance in the recent past and selling assets that have had weak performance. This strategy is based on the assumption that an asset's recent trend is a reliable indicator of its future performance.</p> <p>DTW can be used to evaluate the performance of a momentum strategy by comparing the return trajectory of the momentum portfolio with that of the benchmark index or another portfolio. DTW can help determine the extent to which the momentum strategy follows the benchmark index or other portfolios, and whether there are significant differences in how the two portfolios behave over time.</p> <p>Using DTW, portfolio managers can identify periods when the momentum strategy has outperformed or underperformed relative to the benchmark index or other portfolios, and adjust their investment strategy accordingly. If the momentum strategy does not achieve the expected results, it may be necessary to reevaluate the assets included in the portfolio or revise the strategy parameters.</p>"},{"location":"blocks/en/dynamic_time_warping/#key-takeaways","title":"Key Takeaways","text":"<p>As a conclusion, DTW is a useful time series analysis technique for comparing the performance of a momentum strategy with that of a benchmark index or another portfolio, allowing portfolio managers to adjust their investment strategy based on the results obtained.</p>"},{"location":"blocks/en/dynamic_time_warping/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/esg/","title":"ESG","text":"<p>Factor investing is an investment strategy that seeks to identify and invest in specific factors that explain the returns of financial assets such as stocks and bonds. This approach involves a systematic analysis of various factors, including economic indicators, market trends, and other measurable variables, to determine which factors are most likely to generate the highest returns. By focusing on these factors, investors can potentially achieve superior risk-adjusted returns compared to traditional investment approaches.</p> <p>Example</p> <p>An example of factor investing is the use of the volatility factor. Investors seeking to exploit this factor may construct a portfolio that invests in stocks that tend to be less volatile than the overall market, which can result in a volatility premium. Investors can also use other factors such as valuation, company size, earnings quality, and liquidity to construct portfolios that have increased exposure to these factors. The goal is to achieve a superior long-term return compared to the benchmark index. In this way, factor investing involves a systematic approach to selecting and combining various factors that are expected to generate higher risk-adjusted returns than traditional investment strategies.</p> <p>In this context, the effort undertaken here is focused on the risk premium that ESG factors can potentially offer. Specifically, the aim is to identify and analyze the factors that are most likely to generate superior returns over the long term, taking into account their ESG characteristics. By systematically evaluating the ESG factors that are most relevant to financial performance, investors can potentially construct portfolios that offer both strong returns and positive environmental, social, and governance outcomes. Ultimately, the goal is to achieve a more sustainable and responsible approach to investing that benefits both investors and society as a whole.</p> <p>For the sake of transparency, here are some definitions:</p> <ul> <li>Risk premium: The risk premium is the additional compensation that an investor demands for taking on additional risk. It is often used to measure the expected return of an investment, based on its level of risk</li> <li>Alpha: In finance, alpha is a measure of an investment's risk-adjusted performance relative to a benchmark index such as the overall market. Alpha is crucial because it helps:</li> <li>Evaluate a portfolio manager's ability to generate returns that are superior to those of the market under similar risk conditions</li> <li>Identify the sources of an investment's returns. If an investment consistently generates a positive alpha, it may indicate that the portfolio manager is able to select securities that offer added value compared to the overall market or to exploit inefficiencies in the market.</li> </ul> <p>We believe there are two categories of innovation in factor investing and the consideration of risks related to climate transition and their impact on factors:</p> <ol> <li>The first is based on the creation of factors: These factors must be differentiating (after all, market finance is a zero-sum game) and statistically viable.</li> <li>The second is based on financial engineering: The portfolio construction phase. Studies conducted on the residual (the portion that cannot be explained by the model) show that it behaves purely randomly. Therefore, in theory, it is pointless to focus on discovering other factors.</li> </ol>"},{"location":"blocks/en/esg/#innovation-in-the-industrie","title":"Innovation in the Industrie","text":"<p>Risk factors can help investors understand market developments and construct high-performing portfolios. These factors can generate a added returns due to a distinct risk premium and are known to provide high risk-adjusted returns over a long period of time.</p> <p>Rewarded Factors</p> <p>When the market experiences poor performance, investors tend to become more risk-averse and demand higher returns to compensate for the additional risk they are taking. This increased demand for returns in times of crisis leads to an increase in the prices of assets that offer exposure to these factors, resulting in higher long-term returns. These factors offer a premium because they tend to perform well when investors need it the most.</p> <p>Academic research publication on market anomalies can have a significant impact on the profitability of these strategies. In previous research, it has been shown that published strategy returns can decrease by up to 26% out of sample and up to 58% in the five years following publication (Novy-Marx, 2013, Meilhoc Ricaume, ActurialCapital, 2023). However, even after five years, a non-negligible proportion of anomaly profitability is still preserved, and it can still be used profitably in a diversified portfolio.</p> <p>After significant loss experienced by traditional equity factors in 2019 and 2020, the industry has proposed a range of new factors such as sustainability, innovation, machine learning (ML) related or not to ESG, which have shown better performance than recent historical backtests in terms of returns. However, we can address the issues of \"factor zoo\" and \"p-hacking\" through various examples, especially in ESG.</p> <p>The term \"factor zoo\" is often used to describe the large number of investment factors that have been proposed in financial literature. The problem with the \"factor zoo\" is that some factors may be the result of random data or selection bias. It can be difficult for investors to determine which factors are truly important for generating long-term superior returns and which are mere coincidental.</p> <p>We present a method to streamline and identify viable ESG factors. These factors should possess the following characteristics:</p> <ul> <li>Persuasive : Demonstrated long-term evidence across different geographies and asset categories</li> <li>Robuste : Adjusted for costs and known factors, even after publication</li> <li>Plausible : Supported by evidence of an underlying economic mechanism.</li> </ul>"},{"location":"blocks/en/esg/#transition-risk","title":"Transition Risk","text":"<p>Transition Risks Information</p> <p>Transition risks move prices. This statement that is supported by two major criteria: 1. Evidence based on academic and professional research, including scientific literature, earnings calls, roadshows by financial analysts, discussions regarding these risks, and factual observations such as initiatives taken by asset managers and institutional investors 2. Evidence based on market returns, where the statistical analysis of returns from various markets such as derivatives, stocks, and bonds, as well as key events such as climate events, presidential elections (Trump vs. Biden), and policy changes related to climate change, can explain these returns.</p> <p>Understanding transition risks associated with climate change is crucial for investors seeking to achieve their financial and non-financial objectives. The transition to a low-carbon economy may require significant political, legal, technological, and market changes. Depending on the nature, speed, and direction of these changes, transition risks may pose a financial risk for organizations. Therefore, this risk has become a major concern for institutional investors.</p> <p>We explore market-based measures of transition risk and their use in constructing multi-factor portfolios for investors. A key challenge for portfolio construction is integrating these objectives in terms of exposure to style factors. </p> <p>Felix Goltz, PhD, Scientific Beta, stated that \"Climate transition risk might be embedded in equity style factors.\"</p> <p>Styles</p> <p>Style factors are portfolio characteristics used to identify stocks with similar risk and return profiles. They are often used in quantitative analysis to help build diversified portfolios and reduce overall risk. The most common style factors include value, growth, size, liquidity, and volatility.</p>"},{"location":"blocks/en/esg/#discretionnary-views","title":"Discretionnary Views","text":"<p>Empirical studies on this topic have mixed results: Carbon emissions data are backward-looking, and there is a risk asymmetry where risks depend on many factors such as management quality, technological progress, innovation capacity, competition, demand structure, and financial health. In some cases, carbon consumption is a necessary investment for environmental technological advancement. The price elasticity of demand differs among companies, and substitution technologies may have different costs for different companies. For example, a taxi company may not face the same constraints as an airline company when changing its fleet.</p>"},{"location":"blocks/en/esg/#quantitative-views","title":"Quantitative Views","text":"<p>An alternative method of measuring an asset's exposure to climate transition risk is by examining the variability of its returns, which reflects the information used by market participants. Empirical evidence suggests that market participants, including analysts and institutional investors, take climate transition risks into account, which in turn influences asset prices, such as stock returns, option prices, and bond prices. The prices of carbon, regulations, technological changes, and consumer preferences also affect a company's cash flows.</p> <p>To estimate an asset's exposure to transition risk, we use sector-specific data from the Climate Policy Relevant Sectors (CPRS) to construct market-neutral beta factors. This method helps control for market exposure and allows for the identification of assets with varying levels of sensitivity to transition risk using a \"transition beta\" obtained through regression.</p> <p>This approach allows for controlling the exposure to the market factor, thereby identifying the stocks that are sensitive to its variation. These estimates can help better evaluate the risk associated with assets and make informed investment decisions. </p>"},{"location":"blocks/en/esg/#integration","title":"Integration","text":"<p>By integrating the \"transition beta\" with style factors, which present differing exposures to transition risk, we suggest that these factors can help mitigate the impact of climate transition risk on portfolios.</p> <p>The \"transition beta\" provides an explicit, simple, and transparent tool for highlighting implicit and complex financial and non-financial risks. </p>"},{"location":"blocks/en/esg/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/excess_inflation_and_asset_class_returns/","title":"Inflation and Asset Returns","text":""},{"location":"blocks/en/excess_inflation_and_asset_class_returns/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/extreme_value_theories/","title":"Extreme Value Theories","text":""},{"location":"blocks/en/extreme_value_theories/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/factor_investing/","title":"Factor Investing","text":""},{"location":"blocks/en/factor_investing/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/forecasting_with_convolutional_neural_networks/","title":"Forecasting with CNN","text":""},{"location":"blocks/en/forecasting_with_convolutional_neural_networks/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/forecasting_with_deep_learning/","title":"Deep Learning","text":""},{"location":"blocks/en/forecasting_with_deep_learning/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/forecasting_with_generative_adversarial_networks/","title":"Forecasting with GAN","text":""},{"location":"blocks/en/forecasting_with_generative_adversarial_networks/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/forecasting_with_long_short_term_memory/","title":"Forecasting with LSTM","text":""},{"location":"blocks/en/forecasting_with_long_short_term_memory/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/forecasting_with_recurrent_neural_networks/","title":"Forecasting with RNN","text":""},{"location":"blocks/en/forecasting_with_recurrent_neural_networks/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/llms/","title":"LLMs Investment Adviser","text":""},{"location":"blocks/en/llms/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/network_graph/","title":"Network graph","text":""},{"location":"blocks/en/network_graph/#defintion","title":"Defintion","text":"<p>The correlation structure of securities is a key quantity in describing and managing the risk properties of portfolios. Relatedly, and more recently, application of network theory within financial markets has begun to provide insight into the interconnected nature of securities.</p> <p>By producing a network with nodes being stocks and edges corresponding to relationship (through a distance metric), we are able to model the interconnected structure of a universe of securities. This has been shown to provide a powerful framework to investigate individual company properties in the context of others. We investigated how filtered networks (namely Minimum Spanning Trees (MST) and Planar Maximally Filtered Graphs (PMFG)) can be used to explore the diversification properties of securities. The elegance of this method is such that the networks efficiently encode complex dependency structures whilst remaining computationally tractable. We then use this to construct portfolios based on subsets of stocks with different network properties. Using similar modelling techniques, we are able to explore the network topology for our stock properties within the portfolio. These properties may be exploited in order to enhance portfolio construction methods which may be particularly useful for managed volatility strategies.</p>"},{"location":"blocks/en/network_graph/#diversification","title":"Diversification","text":"<p>Risk is not uniformly spread across financial markets and this fact can be exploited to reduce investment risk within a given portfolio. By extracting the dependency structure of financial equities, a network approach can be used to build a well-diversified portfolio that effectively reduces investment risk. </p> <p>As widely accepted since Markowitz seminal work, an efficient diversification should aim to select stocks as anti-correlated as possible and remaining consistently anti-correlated over time. Identifying, from the study of historical behavior prior to the investment, baskets of stocks with a good likelihood to remain well-diversified over the future investment period is very challenging. Indeed, the structure of correlations between stocks is evolving over time and changes markedly during crises. For this reason the Markowitz approach is normally applied to a selection of stocks identified by using different criteria including the industrial sector and other macro- or micro-economic considerations. In this way, a relatively small set of stocks (typically 10 to 50) is individuated and on such 'basket' the Markowitz optimal portfolio is determined.</p>"},{"location":"blocks/en/network_graph/#invest-in-peripheral-stocks","title":"Invest in Peripheral stocks?","text":"<p>We hypothesise that investments in stocks that occupy peripheral, poorly connected regions in financial filtered networks are most successful in diversifying portfolios.</p>"},{"location":"blocks/en/network_graph/#neutralize-to-correlation-cluster","title":"Neutralize to correlation cluster?","text":"<p>An industry classification is based on a similarity criterion: stocks' membership in \"groups\" or \"clusters\" such as sectors, industries, sub-industries, etc. - the nomenclature varies from one industry classification scheme to another. Commonly used industry classifications such as GICS, BICS, ICB, NAICS, SIC, etc., are based on fundamental/economic data (such as companies' products and services and more generally their revenue sources, suppliers, competitors, partners, etc.). Such industry classifications are essentially independent of the pricing data and, if well-built, tend to be rather stable out-of-sample as companies seldom jump industries. </p>"},{"location":"blocks/en/network_graph/#the-graphical-lasso-algorithm","title":"The graphical lasso algorithm","text":""},{"location":"blocks/en/network_graph/#definition","title":"Definition","text":"<p>The Least Absolute Shrinkage and Selection Operator (LASSO) is a linear regression method used for both variable selection and regularization in statistical modeling and machine learning. It was introduced by Robert Tibshirani in 1996.</p> <p>In linear regression, the goal is to find a linear relationship between a set of predictor variables and a response variable. However, when there are many predictor variables, some of them may be irrelevant or redundant, leading to overfitting and reduced model interpretability. The LASSO addresses this problem by imposing a penalty on the absolute values of the regression coefficients.</p> <p>The key idea behind LASSO is to add a term to the ordinary least squares (OLS) objective function, which minimizes the sum of the squared differences between the predicted and actual response values. The added term, called the L1 penalty, is proportional to the sum of the absolute values of the regression coefficients multiplied by a tuning parameter (often denoted as \\(\\alpha\\)).</p> <p>By tuning the \\(\\alpha\\) parameter, the LASSO can shrink the coefficients of less important predictors towards zero, effectively performing variable selection. The higher the value of \\(\\alpha\\), the more coefficients are shrunk to zero, resulting in a sparser model with fewer predictors. This property makes LASSO useful in situations where feature selection or identifying the most relevant predictors is important.</p> <p>Compared to other variable selection techniques, such as stepwise regression or ridge regression, LASSO has the advantage of producing sparse solutions and providing a continuous range of solutions controlled by \\(\\alpha\\). It is particularly valuable when dealing with high-dimensional datasets with many predictors and with working with network graph, which aims to differentiate connected and unconnected nodes.</p> <p>In a nutshell, LASSO is a trade off between overly simplistic model and overfitting:</p> <ul> <li>Absolute Shrinkage: <ul> <li>Performs L1 regularization, i.e. adds a factor equivalent to the sum of absolute value of the magnitude of coefficients</li> <li>Agnostic to feature definition: Choose minimum magnitude = minimum sum of squared value among all models </li> <li>Minimization objective: , where  refers to the residual sum of squares, i.e. the linear regression objective without regularization, and   the actual regularization coefficient</li> <li>Optimized: The data reconstruction error is balanced with the regularization penalty</li> <li>Coordinate descent (derivative-free optimization algorithm) is used to find a local minimum </li> </ul> </li> <li>Selection: <ul> <li>It shrinks coefficients to exactly zero and act as a feature selector within independent variable (as it uses the absolute value \u2013 Ridge regression for instance, uses squared value, which push all values to be different than 0) </li> <li>Regularization:  provides a trade-off between balancing residual sum of squares and magnitude of coefficients:<ul> <li>\\(\\alpha = 0\\): Same coefficients as simple linear regression </li> <li>\\(\\alpha = \\inf\\): All coefficients are zero </li> <li>\\(0 &lt; \\alpha &lt; \\inf\\): coefficients between 0 and that of simple linear regression</li> </ul> </li> </ul> </li> </ul>"},{"location":"blocks/en/network_graph/#why-lasso","title":"Why LASSO?","text":"Method Financial Market characteristics Efficient with high multicollinearity High variance Efficient with large number of features High Correlation Identify independent variables High Collinearity (independent variables are highly correlated) Minimize the error between predicted and actual observations Need to identify idiosyncratic risk, factor risk"},{"location":"blocks/en/network_graph/#fast-unfolding-of-communities-in-large-networks","title":"Fast Unfolding of Communities in Large Networks","text":"<p>Warning</p> <p>The quality of the partitions resulting from these methods is often measured by the so-called modularity of the partition. The modularity of a partition is a scalar value between -1 and 1 that measures the density of links inside communities as compared to links between communities. </p> <p>Social, technological and information systems can often be described in terms of complex networks that have a topology of interconnected nodes combining organization and randomness. The typical size of large networks such as social network services, mobile phone networks or the web now counts in millions when not billions of nodes and these scales demand new methods to retrieve comprehensive information from their structure. A promising approach consists in decomposing the networks into sub-units or communities, which are sets of highly inter-connected nodes. </p> <p>The identification of these communities is of crucial importance as they may help to uncover a-priori unknown functional modules such as topics in information networks or cyber-communities in social networks. Moreover, the resulting meta-network, whose nodes are the communities, may then be used to visualize the original network structure.</p> <p>The problem of community detection requires the partition of a network into communities of densely connected nodes, with the nodes belonging to different communities being only sparsely connected. Precise formulations of this optimization problem are known to be computationally intractable. </p> <p>Several algorithms have therefore been proposed to find reasonably good partitions in a reasonably fast way. This search for fast algorithms has attracted much interest in recent years due to the increasing availability of large network data sets and the impact of networks on every day life. One can distinguish several types of community detection algorithms: </p> <ul> <li>Divisive algorithms detect inter-community links and remove them from the network</li> <li>Agglomerative algorithms merge similar nodes/communities recursively</li> <li>Optimization methods are based on the maximisation of an objective function</li> </ul>"},{"location":"blocks/en/network_graph/#other-application","title":"Other Application","text":"Network theory financial markets Degree distribution How information spreads Propagation, contagion, disease Stress tests Weighted degree, Intensity Direct relationship(s) from an asset to another Paths, diameter: Geodesic distance, weighted degree proportion, transitivity Indirect relationship(s) from an asset to another Centrality measures: Eigen, Betweeness Drivers, correlation/idiosyncratic risk Clustering methods, community structure Classifications/attribution/similarities between assets Random network formation Change in market behavior"},{"location":"blocks/en/network_graph/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/nowcasting/","title":"Nowcasting","text":""},{"location":"blocks/en/nowcasting/#introduction","title":"Introduction","text":"<p>Info</p> <p>Nowcasting involves predicting the current state of the economy, such as GDP, inflation, or OECD leading indicator, based on an estimation of the present and the immediate future.</p> <p>Nowcasting utilizes statistical techniques to provide early and short-term predictions for key quarterly indicators such as GDP growth, CPI, and OECD leading indicators. These indicators are typically published with significant delay, making timely forecasting a crucial task. The models used for Nowcasting must be able to handle the mixed frequency of the available data, which includes both monthly and weekly measurements. For example, the first official publication of the eurozone's GDP estimate, known as the \"flash estimate,\" is typically released six weeks after the end of the reference quarter.</p> <p>Nowcasting is important for institutions such as central banks or portfolio managers as it enables them to make investment and economic policy decisions based on information that is available more quickly.</p> <p>This approach also allows investors to make more accurate real-time evaluations of economic regimes and adjust their portfolios more swiftly in response to changing market conditions.</p> <p>When it comes to real-time economic evaluation, direct measures are generally more reliable, as they are not contingent on lagging statistical relationships. Short-term forecasting techniques are also more accurate than long-term ones, as they facilitate a quicker adaptation to changing market conditions, prompt identification of inflection points, and hence play a crucial role in risk management.</p>"},{"location":"blocks/en/nowcasting/#models","title":"Models","text":"<p>Aggregated Predictions: Bridge Equations, Dynamic Factor Models and MIDAS</p> <p>Targeted indicators are obtained by multiplying the output of the model with estimated coefficients. The aggregated forecasts are derived by either averaging individual predictions or by using weighting techniques to assign more weight to the most significant relationships.</p>"},{"location":"blocks/en/nowcasting/#bridge-equations","title":"Bridge Equations","text":"<p>The \"bridge equations\" method can be implemented using various econometric techniques, including regression, principal component analysis (PCA), and time series modeling. It is widely used by central banks and economic research institutes to provide real-time forecasts and quick estimates of the state of the economy.</p> <p>This method typically involves three main steps:</p> <ul> <li>Selection of high-frequency economic indicators (HFI): HFI refers to economic data that are available in almost real-time, often with only a few days or weeks lag, such as retail sales, industrial orders, consumption data, or business confidence indicators. These indicators are used to capture real-time economic fluctuations and are considered leading indicators of future economic trends</li> <li>Construction of statistical relationships: The relationships between the target economic indicators (such as GDP, inflation, or retail sales) and HFI are established using econometric models such as regression or factor analysis. These models allow for the identification of statistical relationships between high-frequency economic indicators and target economic indicators with a time lag. The term \"bridge\" refers to the relationships that bridge the gap between the high-frequency indicators and target indicators</li> <li>Aggregation of forecasts: Statistically significant indicators are aggregated</li> </ul> <p>The \"bridge equations\" method is considered particularly useful in periods of economic volatility or uncertainty, where traditional economic indicators may be insufficiently responsive or delayed relative to real-time economic developments.</p>"},{"location":"blocks/en/nowcasting/#dynamic-factor-models","title":"Dynamic Factor Models","text":"<p>Info</p> <p>Dynamic factor models postulate that a small number of unobserved \"factors\" can be used to explain a substantial portion of the variation and dynamics in a larger number of observed variables. </p> <p>A \"large\" model typically incorporates hundreds of observed variables. Estimating dynamic factors is similar to a dimension-reduction technique, producing estimates of the unobserved factors.</p> <p>The \"dynamic factor models\" (DFM) framework for nowcasting is another widely-used econometric method for producing real-time estimates of economic indicators. The DFM approach is based on the use of large high-frequency economic databases and economic activity indicators summarized by dynamic factors.</p> <p>The DFM method consists of several main steps:</p> <ul> <li>Selection of economic variables: The economic variables used for constructing factors are typically chosen based on their economic relevance and correlation with the target economic indicator. The economic variables used may include production, consumption, investment, employment, foreign trade, business and household confidence data, etc</li> <li>Construction of dynamic factors: Dynamic factors are constructed using statistical techniques, which allow the dimensionality of the data to be reduced by extracting the common information of a large number of economic variables into a few dynamic factors. Dynamic factors are thus synthetic variables that summarize the common movements of the economic variables</li> <li>Estimation of regression equations: Regression equations are used to link dynamic factors to target economic indicators. Regression equations can be estimated using various econometric techniques, such as linear regression or time series modeling</li> <li>Production of forecasts: Forecasts are produced by combining the forecasts of the factors with the estimated coefficients</li> </ul> <p>The DFM method allows for the significant reduction of data dimensionality by using synthetic dynamic factors that summarize the common movements of numerous economic variables. This approach is also useful for forecasting, as it enables the capture of complex economic relationships that may exist between various indicators.</p>"},{"location":"blocks/en/nowcasting/#midas","title":"MIDAS","text":"<p>The \"Mixed Data Sampling\" (MIDAS) model for nowcasting is another widely used econometric method for producing near-real-time estimates of economic indicators by using economic data of different frequencies, such as monthly, quarterly, and annual data.</p> <p>The MIDAS method involves several key steps:</p> <ul> <li>Selection of economic variables: Economic variables to be used in the MIDAS model construction are typically selected based on their economic relevance and correlation with the target economic indicator. The economic variables used can include production, consumption, investment, employment, foreign trade, business and household confidence data, etc</li> <li>Specification of the MIDAS model: The MIDAS model is specified using transfer functions that combine economic information at different frequencies. These transfer functions describe how economic data at different frequencies are related to the target economic indicator</li> <li>Estimation of model parameters: The parameters of the MIDAS model are estimated using econometric techniques such as linear regression or time series modeling. The estimated coefficients in the model are used to produce forecasts for the target economic indicator</li> <li>Production of forecasts: Forecasts are produced using the estimated coefficients in the MIDAS model and economic data available at different frequency levels</li> </ul> <p>The MIDAS method allows for the exploitation of information available at different frequencies to produce timely forecasts for an economic indicator.</p>"},{"location":"blocks/en/nowcasting/#svar","title":"SVAR","text":"<p>The Structural VAR (SVAR) model is an econometric analysis method used for causal analysis and macroeconomic forecasting. The SVAR model uses a system of simultaneous linear equations to model causal relationships between multiple economic variables.</p> <p>The SVAR method consists of several key steps:</p> <ul> <li>Model Specification: The SVAR model is specified by identifying relevant economic variables and constructing a system of simultaneous equations to model their causal relationships. Economic variables can include indicators such as GDP, inflation, unemployment rate, interest rates, etc.</li> <li>Estimation of Model Parameters: The parameters of the SVAR model are estimated using econometric techniques such as the generalized method of moments or maximum likelihood method. The estimated coefficients in the model are used to produce forecasts for economic variables.</li> <li>Causality Analysis: The SVAR model allows for the analysis of causality between economic variables using structural shocks. Structural shocks are exogenous events that affect a particular economic variable. Using the estimated coefficients in the model, it is possible to analyze the impact of these shocks on other economic variables in the model.</li> <li>Forecast Production: Forecasts are produced using the estimated coefficients in the SVAR model and available economic data. Forecasts can be adjusted using techniques such as smoothing or filtering to produce more accurate forecasts.</li> </ul> <p>The SVAR method allows for the modeling of complex causal relationships between economic variables. The method is also useful for policy analysis as it allows for the analysis of the impact of policy changes on the economy as a whole.</p>"},{"location":"blocks/en/nowcasting/#key-takeaways","title":"Key Takeaways","text":"<p>Each nowcasting model has its own set of strengths and weaknesses:</p> <ul> <li>Bridge equations models are useful for their flexibility and ability to model nonlinear relationships.</li> <li>Dynamic factor models are often used for their ability to capture real-time information on financial markets and economic indicators. However, they can be sensitive to changes in the relationships between economic variables and prone to larger errors during extreme variability (as evidenced by the Federal Reserve Bank of Atlanta's \"GDPNow\" indicator, which experienced significant forecasting errors during the COVID-19 period).</li> <li>MIDAS models have the advantage of being able to use high-frequency data to produce more accurate forecasts. However, their complexity can make them more challenging to use.</li> <li>SVAR models are often used for their ability to model complex causal relationships between economic variables. But, their use of structural shocks can make their forecasts more sensitive to changes in initial assumptions.</li> </ul> <p>We have developed several econometric methods that aim to produce quasi-real-time estimates of economic indicators. We believe that multiple methods are necessary to meet constraints of each phase of the cycle, obtain a more comprehensive picture of the current and future economic situation, and to reduce model risk.</p>"},{"location":"blocks/en/nowcasting/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/option_based_volatility_timing/","title":"Option Volatility Timing","text":""},{"location":"blocks/en/option_based_volatility_timing/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/option_factor_momentum/","title":"Option Factor Momentum","text":""},{"location":"blocks/en/option_factor_momentum/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/polymodels/","title":"Polymodels","text":""},{"location":"blocks/en/polymodels/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/regime_switching/","title":"Regime Switching","text":""},{"location":"blocks/en/regime_switching/#definition","title":"Definition","text":"<p>Info</p> <p>Financial markets change their behavior over time, which can create periods of persistent market conditions. Time-series Classification is a general task that can be useful to identify these regime changes, using labeled training data.</p> <p>Multi-label classification is a process of categorizing a given set of time interval data into multiple classes. It requires the machine to learn how to assign a class label to examples from the problem domain.</p> <p>Multi-label classification is a supervised machine learning module that is used for classifying time interval data into groups. The goal is to predict the categorical class labels. A vast body of research could be implemented, on top of pre-processing features that prepare the data for modeling. In python, we implemented over 18 ready-to-use algorithms and several plots to analyze the performance of trained models (e.g. ROC curve, confusion matrix).</p> <p>Regime identification can be accomplished by using either subjective knowledge and experience or a data-driven approach:</p> <ol> <li>One method is to classify market regimes based on observable features such as volatility levels, shifts in monetary policy, and changes in investor sentiment</li> <li>In contrast, a data-driven approach involves utilizing historical data to identify market regimes automatically and probabilistically, ie the unsupervised learning page</li> </ol> <p>This article focuses a the latter.</p>"},{"location":"blocks/en/regime_switching/#probabilistic-methods","title":"Probabilistic Methods","text":""},{"location":"blocks/en/regime_switching/#classification-gaussian-mixture-models","title":"Classification: Gaussian Mixture Models","text":"<p>Clustering is an unsupervised learning task that aims to identify groups of data points in a dataset that share similar characteristics. Gaussian Mixture Models (GMM) extend this approach by introducing the concept of probability, which associates a data point with a cluster in a probabilistic manner (soft clustering), unlike K-means, which uses a deterministic approach to assign data points to a single cluster (hard clustering).</p> <p>GMM applies multiple Gaussian distributions to model different segments of the data. For example, financial asset returns are known to have non-normal distributions. Therefore, a GMM would fit various Gaussian distributions to capture different portions of the asset's return distribution. Each distribution would have its own attributes, such as means and volatilities, allowing the GMM to model both the center and the tails of the asset's distribution using a combination of normal distributions. This approach is particularly useful for modeling financial assets, as their return distributions often exhibit significant skewness with a meaningful number of observations in the tails.</p> <p>Hyperparameters?</p> <p>The only hyperparameter in the GMM is the number of clusters, which can be selected using a cross-validation approach. The log-likelihood criterion is typically used to measure the goodness-of-fit, although other criteria such as AIC and p-value can also be used. The results obtained using these alternative criteria are generally similar to those obtained using log-likelihood (McAssey, 2013).</p> <p>Each period in the dataset is assumed to be independent and identically distributed. The GMM is used to identify periods where market conditions exhibit some persistence, although it has been shown that it is uncommon for one market condition to persist for extended periods without interruption from another. Short-term transient themes can emerge, and market conditions can change abruptly (as seen during the COVID market crisis in early 2020). The GMM is reactive to these changes, resulting in sudden market condition switches.</p> <p>A Gaussian Mixture is a function that is comprised of several Gaussians, each identified by \\(k \u2208 {1,\u2026, K}\\), where \\(K\\) is the number of clusters of our dataset. Each Gaussian  in the mixture is comprised of the following parameters:</p> <ul> <li>A mean \\(\\mu\\) that defines its centre</li> <li>A covariance \\(\\sum\\) that defines its width. This would be equivalent to the dimensions of an ellipsoid in a multivariate scenario</li> <li>A mixing probability \\(\\pi\\) that defines how large the Gaussian function is</li> </ul> <p>For instance, in a three Gaussian functions, \\(K=3\\), each Gaussian explains the data contained in each of the three clusters available. The mixing coefficients are themselves probabilities and equates to 1, such as </p> \\[ \\sum_{k=1}^{K} \\pi_k = 1 \\] <p>To optimize these parameters, each Gaussian fits the data points belonging to each cluster through maximum of likelihood.</p> <p>Tips</p> <p>GMM model answers the question: \u201cGiven a data point \\(x\\), what is the probability it came from Gaussian \\(k\\)?\u201d</p> <p>The Expectation \u2014 Maximization (EM)  is widely used for optimization problems where the objective function has complexities in the derivation process.</p> <p>Let the parameters of our model be \\(\\theta = \\{\\pi, \\mu, \\sum\\}\\)</p> <ol> <li>Initialise \\(\\theta\\): For instance, we can use the results obtained by a previous K-Means run as a good starting point for our algorithm</li> <li>Expectation step: Evaluate the probability the data point come from Gaussian \\(k\\) by maximum of likelihood, which is the result of calculating the joint probability of all observations and latent variables and is an extension of our initial derivations for \\(p(x)\\)</li> <li>Maximization step: Find the revised parameters in order to fin a local maximum</li> </ol>"},{"location":"blocks/en/regime_switching/#regression-markov-switching-models","title":"Regression: Markov Switching Models","text":"<p>The Markov Switching Dynamic Regression model is a type of Hidden Markov Model that can be used to represent phenomena in which some portion of the phenomenon is directly observed while the rest of it is hidden. The hidden part is derived using a Markov model, while the visible portion is inferred using a suitable time series regression model in such a way that, the mean and variance of the time series regression model change depending on which state the hidden Markov model is in. </p> <p>This replicates Hamilton\u2019s (1989) seminal paper introducing Markov-switching models. The model is an autoregressive model of order 4 in which the mean of the process switches between two regimes. It can be written such as:</p> <p>\\(y_t = \\mu_{S_t} + \\phi_1 (y_{t-1} - \\mu_{S_{t-1}}) + \\phi_2 (y_{t-2} - \\mu_{S_{t-2}}) + \\phi_3 (y_{t-3} - \\mu_{S_{t-3}}) + \\phi_4 (y_{t-4} - \\mu_{S_{t-4}}) + \\varepsilon_t\\)</p> <p>Each period, the regime transitions according to the following matrix of transition probabilities:</p> \\[ \\begin{split} P(S_t = s_t | S_{t-1} = s_{t-1}) = \\begin{bmatrix} p_{00} &amp; p_{10} \\\\ p_{01} &amp; p_{11} \\end{bmatrix}\\end{split} \\] <p>where \\(p_{ij}\\) is the probability of transitioning from regime \\(i\\), to regime \\(j\\).</p> <p>We will look at a general specification of the model consisting of a time indexed dependent variable \\(y\\), a matrix of regression variables \\(X\\), a fitted vector of coefficients \\(\\hat{\\beta}\\) and residual errors \\(\\epsilon\\):</p> <ul> <li>Dependent variable \\(y\\): Let \\(y\\) be a \\([n x 1]\\) vector of \\(n\\) time-indexed observations </li> <li>Regression variable matrix \\(X\\): Let \\(X\\) be an \\([n x (m+1)]\\) matrix of regression variables</li> <li>The first column is for the intercept</li> <li>\\(x_t\\) is one row of this matrix</li> <li>Fitted regression coefficients \\(\\hat{\\beta}\\): Let \\(\\hat{\\beta}\\) be an \\([(m+1) x 1]\\) vector of regression coefficients. \\(\\hat{\\beta}\\) indicates that it is the fitted value of the coefficient produced from training the model</li> </ul> <p>We now consider the following general specification of a time series model containing an additive error component:</p> \\[ y_t=\\hat{\\mu}_t + \\epsilon_t \\] <p>In the above model, the observed value \\(y_t\\) is the sum of the predicted value \\(\\hat{\\mu}_t\\) and the residual error \\(\\epsilon_t\\). We further assume that \\(\\epsilon_t \\sim N(0, \u03c3^2)\\) is a homoskedastic (constant variance) normally distributed random variable with a zero mean.</p> <p>Training of the MSDR model involves estimating the coefficients matrix \\(\\hat{\\beta}_s\\), the transition matrix \\(P\\) and the variance \\(\\sigma^2\\) of the dependent variable \\(y\\). The estimation procedure is usually Maximum Likelihood Estimation (MLE) or Expectation Maximization. Please see state space models and the Kalman filter on the MLE algorithm.</p> <p>We\u2019ll illustrate MLE which finds the values of \\(P\\), \\(\\hat{\\beta}_s\\) and \\(\\sigma^2\\) that would maximize the joint probability density of observing the entire training data set \\(y\\). In other words, we would want to maximize the following product:</p> \\[ L(\\hat{\\beta}_s; \\sigma^2 P | y) = \\prod_{t=1}^{n} f(y=y_t) \\]"},{"location":"blocks/en/regime_switching/#application-to-financial-decision","title":"Application to Financial Decision","text":"<p>These data-driven approaches propose a learning technique that employs a significant volume of historical data to identify and determine regimes. The resulting model output encompasses four clusters, or market conditions, which have been labeled as:</p> <ol> <li>Crisis</li> <li>Steady State</li> <li>Inflation</li> <li>Walking on Ice (WOI) </li> </ol> <p>Based on their respective characteristics. In order to identify periods of persistent market conditions, an analysis of their behavior throughout history was conducted. We found multiple potential applications for allocators:</p> <ul> <li>Risk management stands as one such application, as allocators can enhance their portfolio scenario analysis by sampling from the distributions of these identified market conditions, thereby subjecting their portfolios to stress tests</li> <li>Another application is related to asset allocation decisions. Predicting market returns over extended periods of time proves challenging, even in stable market conditions. The findings presented provide evidence of rapidly changing market conditions, thereby making it even more difficult to rely on long-term forecasts. Consequently, allocators may need to design portfolios capable of withstanding market condition volatility in the long run, while also considering opportunities for tactical shifts on shorter time horizons.</li> </ul>"},{"location":"blocks/en/regime_switching/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/short_interests/","title":"Short Interests Models","text":""},{"location":"blocks/en/short_interests/#introduction","title":"Introduction","text":"<p>Short selling is a trading strategy that involves selling a security that the seller does not possess or has not yet delivered. The process typically involves borrowing the security from a lender, who typically charges a fee for the transaction. It is important to note that many academic studies rely on the assumption that short selling can be done at low cost and without any constraints. The ability to short sell securities is an essential factor in empirical asset pricing research.</p>"},{"location":"blocks/en/short_interests/#securities-lending-factors-and-composite-models","title":"Securities lending factors and composite models","text":"<p>Several studies suggest that individuals who engage in short selling tend to possess significant market information. Consequently, these studies argue that short-interest data can be leveraged to predict the underperformance of stocks relative to the broader market. We have extended this approach by developing a set of signals that rely on securities lending data. Our analysis demonstrates that these signals are highly effective for selecting stocks across multiple markets. </p> <p>We have constructed two composite models that employ different techniques to weight the signals. The first model employs a simple, static schedule while the second model utilizes a dynamic machine learning algorithm. Both models generate consistent alpha, exhibit slow signal decay, and have low portfolio turnover across all markets.</p>"},{"location":"blocks/en/short_interests/#traditional-factors-and-cost-of-borrow","title":"Traditional factors and cost of borrow","text":"<p>Based on our analysis, we have discovered that a significant number of conventional investment factors experience reduced performance owing to the elevated borrowing expenses related to heavily shorted stocks. We have demonstrated that a straightforward modification on the long side can alleviate much of this shortfall. Additionally, we have exhibited how our collection of security lending indicators can be utilized to amplify traditional factors, leading to improved performance and decreased risk.</p>"},{"location":"blocks/en/short_interests/#ihs-markit-data","title":"IHS Markit data","text":"<p>Our research introduces the Markit Securities Finance (MSF) database, which offers a broad scope of the global securities lending market. The vendor collects data directly from prominent market participants such as custodians, prime brokers, asset managers (e.g., hedge funds), and other financial intermediaries.</p> <p>The MSF dataset encompasses over $16 trillion of global securities in lending programs across more than 20,000 institutional funds. It is worth highlighting that the securities lending market operates over-the-counter (OTC), hence the availability of information is limited and fragmented.</p>"},{"location":"blocks/en/short_interests/#data","title":"Data","text":"<p>Warning</p> <p>It is important to acknowledge that the data presented poses a challenge in that historical quantities are not adjusted for corporate actions such as stock splits. As a result, there may be inconsistencies around corporate actions due to the data being sourced from thousands of institutions.</p> <p>The MSF global equity data has a starting point of 2002, with data frequency being monthly during the early years and transitioning to daily from 2006 onwards. The coverage is comprehensive for most developed markets and moderate (with an improving trend) for emerging markets. In our research, the global all-cap research universe (Russell 3000 in the US, S&amp;P/TSX composite in Canada, and S&amp;P BMI in other regions) demonstrates close to 100% coverage in the US, Canada, UK, ANZ, and Japan, with more than 80% coverage in other regions. MSF data is collected daily and reports transactions two days after the settlement date, leading to a two-day lag in the data.</p> <p>The MSF database provides data on both the value and quantity basis (e.g., the actual dollar value of short interest and the actual number of shares shorted). We predominantly use quantity-based factors as they tend to be more consistent across countries and over time.</p>"},{"location":"blocks/en/short_interests/#factors","title":"Factors","text":"<p>We create a comprehensive set of factors for stock selection on a global scale, which cover different aspects of the securities lending market. </p> <ul> <li>Demand (e.g., Days to Cover): Conventionally, investors rely primarily on the demand side of the securities lending market, i.e., short interest. Because short sellers are more likely to be informed traders, stocks with high short interest (normalized by number of shares outstanding, float, or average daily trading volume) are expected to underperform the market</li> <li>Supply (e.g., Lendable Inventory): One of the drawbacks of using only the demand side is that it ignores the supply side of the securities lending market which can be captured by the available inventory of lendable stock. Not all outstanding shares are available for borrow. The lendable inventory is loosely related to institutional ownership, because institutional investors are more likely to lend securities for a fee as opposed to retail investors who tend to be averse to stock lending. If we assume institutional investors are more sophisticated and/or better informed, the supply side of the securities lending market may also have predictive power of future stock returns</li> <li>Supply-demand (e.g., Utilization): Obviously, we can combine the demand and supply sides to form a more complete picture of short sentiment. A natural measure for the supply side of the securities lending market is the utilization factor, which is simply the ratio of short interest and available inventory. Since the lendable inventory is collected from a large number of providers and consolidated by IHS Markit, the inventory data is not necessarily always up-to-date. IHS Markit makes adjustments for small or inactive positions to derive the so-called active inventory, which is further used to compute active utilization</li> <li>Cost (e.g., Cost-of-Borrow): Borrow cost (e.g., wholesale cost of borrow scores and value weighted average fees) reflects both demand and supply sides of the securities lending market. Stocks that are heavily shorted tend to have high borrow costs. Similarly, securities with limited supply (for lending) are also expected to be expensive to borrow. Although stocks with high short interest are more likely to underperform, it is expensive to short these firms which makes profiting from this information more complicated. This is the classic \u201climits to arbitrage\u201d argument in the academic finance literature. One of the benefits of the borrow cost data is that it can be used to more accurately quantify the alpha/cost trade-off associated with short interest strategies. We can also take advantage of our factors even without shorting, by removing them from the long side, albeit significantly limited our ability to explore the short side</li> <li>Other categories of data: IHS Markit also provides other data that can be used to quantify other themes such as sentiment or risk. For example, changes in utilization can reflect stock level sentiment across short sellers</li> </ul>"},{"location":"blocks/en/short_interests/#country-aggregation","title":"Country aggregation","text":""},{"location":"blocks/en/short_interests/#lendable-inventory-and-short-interest","title":"Lendable inventory and short interest","text":"<p>The Active Beneficiary Owner (BO) Inventory is a measure of available inventory or supply of lendable shares that are active (i.e., not stale). The level of short interest is higher in the US, Canada, and UK, compared to emerging markets like LATAM and merging EMEA, where short selling is more restricted. The availability of inventory is an important factor to consider when analyzing factor efficacy in emerging markets such as LATAM, EMEA, and AxJ, where the stock lending market is less developed and short selling regulations are often more restrictive. In such markets, short interest may not accurately reflect true demand, due to limited supply and regulatory constraints. In many emerging markets, shorting is too expensive and/or limited in scale for arbitrageurs, and regulators may clamp down on short selling at the first sign of a market downturn.</p>"},{"location":"blocks/en/short_interests/#cost-of-borrow","title":"Cost of borrow","text":"<p>The DCBS score provided by IHS Markit serves as a proxy for the cost and difficulty to borrow a stock. It ranges from 1 (easy and cheap to borrow) to 10 (nearly impossible and very expensive to borrow). In the US, almost 90% of stocks have a DCBS score of one, which implies that those stocks are cheap and easy to borrow. In emerging markets such as LATAM, EMEA, and Asia ex Japan, the cost of borrow can be particularly high for a significant number of stocks. However, the number of easy-to-borrow stocks is trending higher across almost all markets. It's important to note that since the exact cost of borrow may not always be available or precise, the DCBS score is used as a proxy.</p>"},{"location":"blocks/en/short_interests/#utilization","title":"Utilization","text":"<p>Utilization and active utilization are ratios that capture the demand for borrowing stocks. Utilization is the ratio of Short Interest and Beneficial Owner Inventory, while Active Utilization is based on Short Interest and Active Beneficial Owner Inventory. Utilization is a measure of the percentage of available inventory of a stock that is being borrowed by short sellers. Active utilization is similar, but it considers only active inventory, which is inventory that has been actively lent out or borrowed against.</p> <p>The level of utilization can be influenced by both supply and demand factors. In developed markets, utilization levels can be associated with general market sentiment. For example, utilization levels rose globally during the lead-up and peak of the 2008 financial crisis and descended back towards more moderate levels as financial markets normalized. The observable spikes in European utilization after the financial crisis are related to episodic crises driven by a series of problems with peripheral countries within the union.</p> <p>There are meaningful differences in utilization levels between developed and emerging markets. Data coverage for LATAM is poor, and utilization data does not become available until 2010 for AxJ and EMEA. In emerging markets, shorting is often too costly and/or too limited in scale for arbitrageurs, and regulators are more likely to restrict short selling at the first sign of market downturn. Thus, short interest may not represent true demand in these markets, and utilization may not be a reliable proxy for sentiment.</p>"},{"location":"blocks/en/short_interests/#sector-aggregation","title":"Sector aggregation","text":"<p>During the 2008 financial crisis in the US, there was a significant rise in short interest across all sectors classified under the Global Industry Classification Standard (GICS). Consumer Discretionary, Financials, and Real Estate sectors experienced the most substantial increases in short interest, which was followed by sharp reversals. Similarly, during Q3 2014, the Energy sector witnessed a significant surge in short interest, primarily due to a decline in crude oil prices. Short interest also remained relatively high in the health care sector. Although the information technology sector had historically high valuation levels, its active utilization remained subdued. This pattern of short interest increase during market selloffs and decrease during market rallies was observed across various developed markets at the GICS sector level. Does this indicate that short sellers exacerbate a market selloff by opportunistically participating in a market crash?</p>"},{"location":"blocks/en/short_interests/#market-aggregation","title":"Market aggregation","text":"<p>Info</p> <p>The market-level short interest signal is strong at predicting market return at longer horizons of semi-annual and annual returns. Together these results suggest that short sellers, as a group, can be used as a signal to anticipate for macroeconomic changes and emerging trends in market directions. Short interest based variables also seem to perform better than other popular market timing predictors (Rapach, et al, 2014).</p> <p>Given the ease of borrowing and shorting most stocks in the US, it is possible to use market-level short interest signals for market timing analysis. Contrary to patterns observed in the cross-sectional analysis, total short interest tends to move in a countercyclical manner. Although aggregated short selling is primarily a trend-following strategy, it has limited potential to stabilize the market. Aggregate short interest is positively associated with the prior month's declines, indicating that short sellers are trend-followers during market downturns. In contrast, during market rallies, short sellers tend to be less aggressive in selling. Our findings suggest that aggregated short interest (Active Utilization) at the market level can predict future market returns. The regression analysis shows that the T-statistics of the predictor variable (Active Utilization) are significant at -1.9, with a p-value of 5.7%. These statistics remain significant even after accounting for heteroscedasticity and autocorrelation using the Newey-West (1987) estimator.</p>"},{"location":"blocks/en/short_interests/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/state_space_models_and_the_kalman_filter/","title":"State Space: Kalman Filter","text":""},{"location":"blocks/en/state_space_models_and_the_kalman_filter/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/tail_risk_strategies/","title":"Tail Risk Strategies","text":""},{"location":"blocks/en/tail_risk_strategies/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/timing_factor_zoo/","title":"Timing Factor Zoo","text":""},{"location":"blocks/en/timing_factor_zoo/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/topological_data_analysis/","title":"Topological Data Analysis","text":"<p>Note<p>Utilizing Topological Data Analysis and Machine Learning for Discovering Temporal Insights from Complex Data</p> </p>"},{"location":"blocks/en/topological_data_analysis/#abstract","title":"Abstract","text":"<p>This research paper explores the integration of topological data analysis (TDA) and machine learning techniques as a novel approach for uncovering insights from complex data. TDA captures the inherent shape of data and generates visual summaries or compressed representations, facilitating the identification of clusters, progressions, anomalies, and cycles in complex or wide datasets. The combination of TDA with machine learning algorithms enhances the extraction of critical intelligence previously hidden or overlooked by conventional analytical approaches. The derived insights have significant implications for informing precise strategies related to nowcasting, regime shift, risk mitigation and asset allocation, market classification and forecasting. One of the key aspects of TDA is its ability to incorporate the temporal dimension and analyze the evolution of topological features through time. By considering the data as a sequence of observations or snapshots, TDA techniques can track the birth, persistence, and extinction of topological features over time, providing insights into the dynamic changes occurring within the data. This temporal perspective allows for a deeper understanding of the data's underlying patterns, relationships, and transitions. TDA's ability to analyze the evolution of topological features through time has found applications in various fields, such as neuroscience, finance, climate science, and social networks, where capturing the temporal dynamics is crucial for uncovering meaningful insights and making informed decisions. The integration of TDA with temporal considerations provides a valuable tool for studying complex data sets and offers a new dimension of analysis that enhances our understanding of dynamic systems.</p>"},{"location":"blocks/en/topological_data_analysis/#introduction","title":"Introduction","text":"<p>The growing complexity and abundance of data present challenges in extracting meaningful insights. This paper introduces the integration of topological data analysis and machine learning techniques as a promising solution for uncovering hidden patterns and extracting valuable information from complex datasets.</p>"},{"location":"blocks/en/topological_data_analysis/#methodology","title":"Methodology","text":"<p>Topological data analysis is employed to capture the underlying shape of data, generating visual summaries or compressed representations. These representations facilitate the identification of clusters, progressions, anomalies, and cycles within the datasets. Machine learning algorithms are then applied to extract actionable insights from the visual representations.</p>"},{"location":"blocks/en/topological_data_analysis/#results-and-discussion","title":"Results and Discussion","text":"<p>The integration of topological data analysis and machine learning has proven effective in uncovering insights that were previously undetected using conventional analytical approaches. The ability to identify patterns and understand the underlying reasons behind them enhances decision-making processes.</p>"},{"location":"blocks/en/topological_data_analysis/#applications","title":"Applications","text":"<p>The insights derived from the combined approach of topological data analysis and machine learning have significant implications for various business areas, including nowcasting, regime shift, risk mitigation and asset allocation, market classification and forecasting. Leveraging the identified patterns and understanding their underlying reasons can lead to the development of more precise strategies and improved overall performance.</p> <p>We aim to focus our research on the following analysis: \u2022   Nowcasting, which refers to the practice of estimating or predicting the current or near-term state of an economic or financial variable using the most recent available data. It aims to provide real-time insights into the current state of the economy or a specific market indicator, filling the gap between the release of historical data and the arrival of official statistics or indicators. \u2022   Regime shifts, which refer to the periods in which financial markets transition from one stable state to another, often characterized by different market behaviors, volatility levels, or macroeconomic conditions. \u2022   Clustering / Classification, which is a fundamental technique in data analysis and machine learning that involves grouping similar objects or data points together based on their intrinsic characteristics or properties. The goal of clustering is to discover inherent patterns, structures, or relationships in the data without prior knowledge of the groupings. \u2022   Forecasting, which is the process of predicting or estimating future outcomes or events based on historical data, patterns, and trends. It involves utilizing various quantitative and qualitative techniques to make informed projections about future values, behaviors, or conditions of a particular variable or system.</p>"},{"location":"blocks/en/topological_data_analysis/#conclusion","title":"Conclusion","text":"<p>The integration of topological data analysis and machine learning techniques provides a powerful approach for uncovering insights from complex data. By leveraging the inherent shape of the data and employing machine learning algorithms, organizations can enhance their decision-making processes and develop more effective strategies for nowcasting, regime shift, risk mitigation and asset allocation, market classification and forecasting.</p>"},{"location":"blocks/en/topological_data_analysis/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p> <ol> <li> <p>Perea, J. A., &amp; Harer, J. (2015). Sliding Windows And Persistence. In Proceedings of the 31<sup>st</sup> International Symposium on Computational Geometry (SoCG) (pp. 431\u2013444). ACM.\u00a0\u21a9</p> </li> <li> <p>Gidea, M., &amp; Katz, Y. (2017). Topological Data Analysis of Financial Time Series. Journal of Risk and Financial Management, 10(4), 20. doi:10.3390/jrfm10040020\u00a0\u21a9</p> </li> <li> <p>McInnes, L., Healy, J., &amp; Melville, J. (2018). UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction. arXiv preprint arXiv:1802.03426.\u00a0\u21a9</p> </li> <li> <p>Carlsson, G., Ishkhanov, T., De Silva, V., &amp; Zomorodian, A. (2008). Extracting insights from the shape of complex data using topology. Science, 321(5895), 767-770. doi:10.1126/science.1150352\u00a0\u21a9</p> </li> <li> <p>Carlsson, G., Memoli, F., &amp; Singh, G. (2010). Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition. In Handbook of Discrete and Computational Geometry (3<sup>rd</sup> ed.) (pp. 1395-1411). CRC Press.\u00a0\u21a9</p> </li> <li> <p>Elyasi, M., &amp; Moghadam, S. Z. (2019). An Introduction to a New Text Classification and Visualization for NLP Using TDA. In Proceedings of the International Conference on Advanced Research in Computer Science and Information Technology (ICARCSIT) (pp. 1-7). IEEE.\u00a0\u21a9</p> </li> <li> <p>Scikit-TDA. (n.d.). GitHub Repository. Retrieved from scikit-tda \u21a9</p> </li> <li> <p>Scikit-TDA. (n.d.). Documentation. Retrieved from scikit-tda \u21a9</p> </li> <li> <p>KeplerMapper. (2020). Documentation (Version 2.0.0). Retrieved from kepler-mapper \u21a9</p> </li> <li> <p>Smith, J. (2022). Leveraging Machine Intelligence for Data Analysis. Journal of Data Science, 15(2), 123-145.\u00a0\u21a9</p> </li> <li> <p>Brown, A., &amp; Johnson, M. (2023). Topological Data Analysis: Concepts and Applications. New York, NY: Springer.\u00a0\u21a9</p> </li> </ol>"},{"location":"blocks/en/trend_following_vs_mean_reversion/","title":"Trend Following","text":""},{"location":"blocks/en/trend_following_vs_mean_reversion/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/valuation_spread_methods/","title":"Valuation Spread Methods","text":""},{"location":"blocks/en/valuation_spread_methods/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/vector_auto_regression_and_impulse_response_models/","title":"VAR and Impulse-Responses","text":""},{"location":"blocks/en/vector_auto_regression_and_impulse_response_models/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/vix_yield_curve_cycle/","title":"VIX yield Curve Cycle","text":""},{"location":"blocks/en/vix_yield_curve_cycle/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/volatilite_cible/","title":"Volatility Targeting","text":"<p>Volatility targeting is a popular technique employed by portfolio managers to manage risk by adjusting the portfolio exposure to maintain a desired level of volatility. The objective is to keep the portfolio's volatility as close as possible to the target value while ensuring that the amount of risk in dollar amount remains the same. This technique involves increasing or decreasing the amount of leverage depending on the portfolio's predefined volatility level.</p> <p>Research conducted by Harvey et al.<sup>1</sup> (2019) has shown that volatility targeting can improve portfolio performance, especially for riskier assets like equities and credit.</p>"},{"location":"blocks/en/volatilite_cible/#methods-of-volatility-targeting-a-step-by-step-introduction","title":"Methods of Volatility Targeting: A Step-by-Step Introduction","text":"<p>To implement a volatility targeting strategy, the following steps are usually followed:</p>"},{"location":"blocks/en/volatilite_cible/#step-1-capture-volatility","title":"Step 1: Capture Volatility","text":""},{"location":"blocks/en/volatilite_cible/#simple-volatility","title":"Simple Volatility","text":"<p>Simple volatility weights periodic returns equally, which means that simple volatility is equal to the square root of the variance, which is equivalent to the arithmetic mean of returns squared. However, the most significant disadvantage of this approach is that recent returns have the same impact on the calculated volatility level as past returns.</p>"},{"location":"blocks/en/volatilite_cible/#ewma","title":"EWMA","text":"<p>The Equally-Weighted Moving Average (EWMA) method addresses the disadvantage of equal weighting of the Simple method by giving more weight to recent returns and less weight to past returns. The smoothing parameter \\(\\lambda\\) is introduced and adjust the decay in the series. Under this condition, instead of using equal weights, each squared return is weighted by a multiplier.</p>"},{"location":"blocks/en/volatilite_cible/#garch","title":"GARCH","text":"<p>The Generalized Autoregressive Conditional Heteroskedasticity (GARCH) method is autoregressive and assumes conditional heteroscedasticity. This is particularly useful as volatility tends to vary over time and depends on past variance, making a homoskedastic model suboptimal. The GARCH process depends on past squared returns and past variances to model the current variance. This model is recommended if there are repeated periods of unexplained high volatility.</p>"},{"location":"blocks/en/volatilite_cible/#step-2-dynamic-volatility-scaling","title":"Step 2: Dynamic Volatility Scaling","text":"<p>To enhance the accuracy of portfolio volatility targeting, portfolio managers can use long-term data and respective long-term correlation estimates to compute volatility. This technique mainly relies on the stablility of risk contribution of each asset over time, assuming that risk estimation derived from historical data is indicative of future risk. PMs can modify exposure to each instrument in the portfolio alongside changes in volatility. However, this technique suffers from delayed reaction, and premature response could result in downside losses and additional transaction costs.</p>"},{"location":"blocks/en/volatilite_cible/#step-3-volatility-switching","title":"Step 3: Volatility Switching","text":"<p>To address the issue in targeting portfolio volatility, volatility switching introduces a faster measure that is not implemented unless market conditions experience significant changes.</p>"},{"location":"blocks/en/volatilite_cible/#step-4-momentum-filtering","title":"Step 4: Momentum Filtering","text":"<p>Momentum Filter</p> <p>The momentum filter is employed to mitigate losses and reduce risk levels in unfavorable markets. This technique depends on additional observation of market return persistence, where decreasing (increasing) prices have been more likely to continue. Hence, this technique involves reducing the exposure of the portfolio in bear markets while increasing exposure in bull markets.</p> <p>After scaling volatility, the momentum filter is employed to further reduce exposure in declining markets to reach a risk level lower than the target, as long as prices continue to fall. This may result in a loss of some positive returns when markets switch from a negative trend to a positive trend or vice versa. However, research suggests that it is better to miss these turning points and reduce exposure to declining markets rather than endure turbulence.</p>"},{"location":"blocks/en/volatilite_cible/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p> <ol> <li> <p>Harvey, C. R., Hoyle, J. B., Korgaonkar, S., Rattray, S., Sargaison, M., &amp; Van Hemert, O. (2019). The impact of volatility targeting. Journal of Portfolio Management, 45(6), 71-88.\u00a0\u21a9</p> </li> </ol>"},{"location":"blocks/en/volatility_neutral_arbitrage/","title":"Volatility Neutral Arbitrage","text":""},{"location":"blocks/en/volatility_neutral_arbitrage/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"blocks/en/wavelet_transform_pairs_trading/","title":"Wavelet Transform","text":""},{"location":"blocks/en/wavelet_transform_pairs_trading/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"contacts/","title":"Contact","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/","title":"Documentation","text":"<p>This website is dedicated to providing documentation for a Python package that utilizes the \"blocks\" method for implementing financial strategies. This approach involves the sequential application of rule-based modular strategies that can be easily modified and added or removed from the sequence as needed. The method's effectiveness lies in its ability to generate a score for each block, enabling efficient testing and optimization, as well as facilitating research and machine learning integration. Furthermore, the method allows for the combination of various sources of alpha in a risk-efficient manner. </p> <p>The building blocks approach provides a comprehensive yet flexible means of financial strategy development and data analysis, making it an essential tool for professionals in the finance industry, whether experienced or novice.</p> <p></p> <p>Strategy is the main implementation of the library for portfolio construction, which initializes the execution of the sequence of blocks and allows to compute tilts/exposures, create and backtest target portfolios.</p> <p></p> <p>Signals constructs transfomed dataset, which are used to build strategies. Examples including High Frequency Indicators, Nowcasts/Forecasts or Alpha Signals.</p> <p></p> <p>Synthetic market or economic data refers to artificially generated datasets that mimic real-world economic or market conditions. Synthetic data can be used for various purposes, including research, testing economic models, simulating extreme events, evaluating strategies, or simply providing training data.</p> <p></p> <p>Workflow eliminates the need for manual resource creation and configuration, and also helps to manage the dependencies between resources. As a result, you can focus more on developing your applications that run on AWS, rather than spending time on resource management.</p>"},{"location":"documentation/api/signals/","title":"Signals","text":"<p>In the world of quantitative analysis and investment strategies, the ability to effectively interpret and utilize investment signals is of paramount importance. Investment signals refer to various indicators or patterns in financial data that can provide insights into the future performance of assets or markets. In recent years, advancements in machine learning and data analysis have significantly enhanced the way investment signals are analyzed and used in decision-making processes.</p> <p>This section delve deeper into the concepts of univariate, aggregation and multi-variate signals, and the principles behind multi-signals learning. We explore various approaches and techniques used in this field, highlighting their benefits and potential challenges. Through a better understanding of investment signals in quantitative analysis and the power of multi-signals learning, we can unlock new opportunities for generating alpha and achieving superior investment performance.</p>"},{"location":"documentation/api/signals/#univariate","title":"Univariate","text":"<p>One aspect of analyzing investment signals involves dealing with univariate signals, which focus on individual indicators or features of a dataset. These signals are typically analyzed in isolation, without considering the potential benefits of incorporating information from other sources.</p>"},{"location":"documentation/api/signals/#multivariate","title":"Multivariate","text":"<p>In quantitative analysis, the utilization of multivariate signals has gained significant attention due to its ability to capture a more comprehensive understanding of financial markets. Multivariate signals refer to datasets that consist of multiple indicators or features, each providing a distinct perspective on the underlying data. By incorporating multiple views or modalities, multivariate signals offer a more holistic representation of complex market dynamics, thereby enhancing the predictive power and accuracy of investment models.</p> <p>One of the primary advantages of multivariate signals is their ability to capture the interdependencies and interactions between different variables. Financial markets are complex systems with numerous factors influencing asset prices, including economic indicators, news sentiment, market sentiment, and technical analysis indicators, among others. By considering these diverse sources of information simultaneously, multivariate signals enable analysts to capture the interplay between various market drivers, providing a more nuanced understanding of price movements.</p> <p>The analysis of multivariate signals requires specialized techniques to handle the increased complexity and dimensionality of the data. One common approach is feature aggregation, which involves combining individual indicators or features into a single representation. Aggregation techniques can range from simple statistical measures such as mean, median, or sum, to more advanced methods like principal component analysis (PCA) or factor analysis. The goal of feature aggregation is to condense the information from multiple signals into a unified representation that preserves the most relevant information while reducing noise and redundancy.</p> <p>By aggregating features from different signals, analysts can extract high-level patterns and relationships that might not be evident when considering individual signals in isolation. This approach helps overcome the limitations of single-signal analysis, such as overlooking important interactions or failing to capture the full complexity of the underlying data. By leveraging the complementary information embedded in each signal, multivariate analysis enables more accurate modeling and prediction of market behavior.</p>"},{"location":"documentation/api/signals/#signal-learning","title":"Signal Learning","text":"<p>Machine learning algorithms excel at identifying complex patterns and relationships within large and diverse datasets, making them well-suited for handling different market signals. These algorithms can automatically learn and adapt to the inherent structure and dependencies present in the data, uncovering valuable insights that may not be apparent through traditional statistical methods.</p> <p>To overcome the limitations of univariate signals, researchers and practitioners have turned to the concept of multi-signals learning or multi-view learning. Multi-signals learning involves datasets that consist of multiple views, modalities, or representations of the same data object. Each signal represents a different perspective or feature set, providing complementary information that can be leveraged to improve the overall learning performance.</p> <p>The fundamental idea behind multi-signals learning is to extract a unified representation of the data by combining the information from multiple views. By integrating information from different views, this approach can overcome limitations of traditional single-signal learning, such as the curse of dimensionality, data sparsity, and feature redundancy. It enables the creation of more robust and accurate models that can better capture the complex relationships and patterns in financial data.</p> <p>Multi-signals learning can be categorized into two main types: </p> <ul> <li>Co-training and joint learning, which aim to learn a common representation for all views simultaneously, leveraging the interactions and relationships between different signals to enhance the learning process</li> <li>Co-training algorithms involve training separate models on each view and iteratively updating the models by transferring information from one view to another. This iterative process allows the models to refine their understanding of the data through the exchange of knowledge between views</li> </ul> <p>The application of multi-signals learning in quantitative analysis has shown promising results in improving investment strategies and decision-making. By considering multiple perspectives and leveraging the complementary information provided by different signals, practitioners can gain a more comprehensive understanding of the market dynamics and make more informed investment decisions.</p>"},{"location":"documentation/api/signals/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/signals/multi_signal_learning/","title":"Signal Learning","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/signals/multivariate_signals/","title":"Multivariate","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/signals/univariate_signals/","title":"Univariate","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/strategy/","title":"Strategy","text":""},{"location":"documentation/api/strategy/#strategy_1","title":"Strategy","text":"<pre><code>opendesk.Strategy(\nsteps: List[Tuple[str, Type, Dict, Dict]], \ntopdown: Optional[bool] = False, \nmapping_table: Optional[Dict[str, str]] = None, \nmapping_weights: Optional[Dict[int, Tuple]] = None\n)\n</code></pre> <p>The main implementation of the library is the <code>Strategy</code> class, which initializes the execution of the sequence and allows to compute tilts/exposures, create and backtest portfolios.</p> <p>Fluent interface</p> <p>In object-oriented programming, returning self from a method allows for the implementation of a fluent interface, where methods can be cascaded in a single statement.</p> <p>In object-oriented programming, returning <code>self</code> from a method can be useful for several reasons. One common use case is to create a fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This can make the code more readable and concise, as it eliminates the need to create intermediate variables to store the results of intermediate method calls. For example, with a fluent interface, this code could be written as <code>results = SomeClass().method1().method2().method3()</code>.</p>"},{"location":"documentation/api/strategy/#parameters","title":"Parameters","text":"steps<pre><code>List[Tuple[str, Type, Dict, Dict]]\n</code></pre> <p>Alpha blocks in order of execution. More information, please visit the Blocks. The parameter <code>steps</code> is required to enable the smooth integration of the strategy into the module with minimal disruption. It is a list of tuples containing:</p> <ul> <li>Custom name of the selected block (e.i. <code>myblock</code>)</li> <li>Class object of the selected block (e.i. <code>MyBlock</code>)</li> <li>Necessary parameters used at the block initialization (<code>__init__</code>) section (e.i. <code>dict(param_1 = \"param_1, param_2 = \"param_2\")</code>)</li> <li>Additional data, <code>**kwargs</code>, required to train the model from the method <code>processing()</code></li> </ul> <p>Example</p> <pre><code>from opendesk.blocks import Reversion\nsteps=[(\"Reversion\", Reversion)]\n</code></pre> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to <code>True</code>, a mapping table should be passed. Defaults to <code>False</code>.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> mapping_weights<pre><code>Optional[Dict[int, Tuple]] = None\n</code></pre> <p>Maps scores with range of weights. Defaults to <code>None</code>.</p>"},{"location":"documentation/api/strategy/#ancestors-in-mro","title":"Ancestors (in MRO)","text":"<ul> <li>opendesk.portfolio.PortfolioConstruction</li> </ul>"},{"location":"documentation/api/strategy/#attributes","title":"Attributes","text":""},{"location":"documentation/api/strategy/#breakdown","title":"breakdown","text":"breakdown<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Output (scores) of all provided blocks. Following the method <code>fit()</code>, the attribute <code>breakdown</code> can be accessed, which contains the output from various models in a single <code>pandas.DataFrame</code> object</p> <p>Breakdown</p> <p> <pre><code>$ strategy.breakdown\n&lt;span style=\"color: grey;\"&gt;           Model 1  Model 2  Model 3  Model 4\nsector 1        -1        0        2        1\nsector 2         2       -2        0        1\nsector 3         1        1       -2        2\nsector 4        -2        0       -1        0\nsector 5         0       -1       -1        2\nsector 6        -1        0        1       -2\nsector 7        -2        1       -2       -1\nsector 8         1        2        0       -1\nsector 9         0        0       -1        0\nsector 10        2        0        1        0\n&lt;/span&gt;\n</code></pre>"},{"location":"documentation/api/strategy/#exposures","title":"exposures","text":"exposures<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores.</p>"},{"location":"documentation/api/strategy/#model_data","title":"model_data","text":"model_data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Adjusted closing prices of the asset, each row is a date and each column is a ticker/id.</p>"},{"location":"documentation/api/strategy/#weights","title":"weights","text":"weights<pre><code>Dict[str, float]\n</code></pre> <p>Portfolio weights calculated through the discrete allocation <code>method</code>.</p> <p>Bounds</p> Group ConstraintsLower BoundMid BoundUpper Bound <p> <pre><code>$ strategy.group_constraints\n&lt;span style=\"color: grey;\"&gt;{'sector 1': (0.0, 0.0),\n'sector 2': (0.05, 0.15),\n'sector 3': (0.0, 0.0),\n'sector 4': (-0.15, -0.05),\n'sector 5': (0.0, 0.0),\n'sector 6': (0.0, 0.0),\n'sector 7': (0.0, 0.0),\n'sector 8': (0.05, 0.15),\n'sector 9': (0.0, 0.0),\n'sector 10': (0.0, 0.0)}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.lower_bound\n&lt;span style=\"color: grey;\"&gt;{'sector 1': 0.0,\n'sector 2': 0.05,\n'sector 3': 0.0,\n'sector 4': -0.15,\n'sector 5': 0.0,\n'sector 6': 0.0,\n'sector 7': 0.0,\n'sector 8': 0.05,\n'sector 9': 0.0,\n'sector 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.mid_bound\n&lt;span style=\"color: grey;\"&gt;{'sector 1': 0.0,\n'sector 2': 0.1,\n'sector 3': 0.0,\n'sector 4': -0.1,\n'sector 5': 0.0,\n'sector 6': 0.0,\n'sector 7': 0.0,\n'sector 8': 0.1,\n'sector 9': 0.0,\n'sector 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.upper_bound\n&lt;span style=\"color: grey;\"&gt;{'feature 1': 0.0,\n'feature 2': 0.15,\n'feature 3': 0.0,\n'feature 4': -0.05,\n'feature 5': 0.0,\n'feature 6': 0.0,\n'feature 7': 0.0,\n'feature 8': 0.15,\n'feature 9': 0.0,\n'feature 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"documentation/api/strategy/#instance-variables","title":"Instance variables","text":""},{"location":"documentation/api/strategy/#group_constraints","title":"group_constraints","text":"group_constraints<pre><code>Optional[Dict[str, Tuple(float, float)]]\n</code></pre> <p>Strategy constraints by group. Product of <code>exposures</code> and <code>mapping_weights</code>.</p>"},{"location":"documentation/api/strategy/#public-methods","title":"Public Methods","text":""},{"location":"documentation/api/strategy/#exposures_1","title":"Exposures","text":"<ul> <li><code>add_blocks()</code>: After initialization, additional blocks can be added to <code>steps</code></li> <li><code>check_group_constraints()</code>: Check group constraints after creating a portfolio</li> <li><code>estimate()</code>: Aggregate exposures by summing each units using a predetermined function</li> <li><code>fit()</code>: Executes each provided blocks with provided dataset</li> <li><code>run_block()</code>: Executes one block</li> </ul>"},{"location":"documentation/api/strategy/#portfolio-construction","title":"Portfolio Construction","text":"<ul> <li><code>portfolio()</code>: Find portfolio weights, at any levels</li> <li><code>discrete_allocation()</code>: Set portfolio weights following a discrete allocation weighting scheme</li> <li><code>optimize()</code>: Portfolio optimization, which aims to select the optimal mix of assets in a portfolio in order to satisfy the defined objectives and constraints</li> </ul>"},{"location":"documentation/api/strategy/#backtest","title":"Backtest","text":"<ul> <li><code>backtest()</code>: Backtest portfolio</li> </ul>"},{"location":"documentation/api/strategy/backtest/","title":"Backtest","text":""},{"location":"documentation/api/strategy/backtest/#backtest_1","title":"backtest","text":"<pre><code>@classmethod\nopendesk.Strategy.backtest(\nconfig: BacktestConfig, \n**kwargs\n) \u2011&gt; vectorbt.portfolio.base.Portfolio\n</code></pre> <p>The <code>backtest</code> instance of the <code>Strategy</code> class is a classmethod. It is used to run a simulation using a custom order function <code>from_order_func</code> from <code>vectorbt.Portfolio</code> class. </p> <p>The job of the Portfolio class is to create a series of positions allocated against a cash component, produce an equity curve, incorporate basic transaction costs and produce a set of statistics about its performance. In particular, it outputs position/profit metrics and drawdown information.</p>"},{"location":"documentation/api/strategy/backtest/#workflow","title":"Workflow","text":""},{"location":"documentation/api/strategy/backtest/#preparation","title":"Preparation","text":"<ul> <li>Receives a set of inputs, such as signal arrays and other parameters</li> <li>Resolves parameter defaults by searching for them in the global settings</li> <li>Brings input arrays to a single shape</li> <li>Does some basic validation of inputs and converts Pandas objects to NumPy arrays</li> <li>Passes everything to a Numba-compiled simulation function</li> </ul>"},{"location":"documentation/api/strategy/backtest/#simulation","title":"Simulation","text":"<ul> <li>The simulation function traverses the broadcasted shape element by element, row by row (time dimension), column by column (asset dimension)</li> <li>For each asset and timestamp (= element):<ul> <li>Gets all available information related to this element and executes the logic</li> <li>Generates an order or skips the element altogether</li> <li>If an order has been issued, processes the order and fills/ignores/rejects it</li> <li>If the order has been filled, registers the result by appending it to the order records</li> <li>Updates the current state such as the cash and asset balances</li> </ul> </li> </ul>"},{"location":"documentation/api/strategy/backtest/#construction","title":"Construction","text":"<ul> <li>Receives the returned order records and initializes a new Portfolio object</li> </ul>"},{"location":"documentation/api/strategy/backtest/#analysis","title":"Analysis","text":"<ul> <li>Offers a broad range of risk &amp; performance metrics based on order records</li> </ul> <p>It requires <code>BacktestConfig</code> dataclass, a configuration pipeline which simplifies model configurations.</p> <p><code>@classmethod</code></p> <p>A class method is a method that is bound to the class and not the instance of the class. It can be called on the class itself, as well as on any instance of the class. In Python, a class method is defined using the <code>@classmethod</code> decorator.</p> <p>The <code>backtest</code> classmethod initially initializes and fits the strategy using the <code>fit()</code> method, and estimates its exposures using the <code>estimate()</code> method. Afterwards, it constructs a portfolio based on specified orchestration and methodologies at each point in time. The final result is a vectorbt object, which provides access to the full range of functionality offered by the vectorbt ecosystem.</p>"},{"location":"documentation/api/strategy/backtest/#parameters","title":"Parameters","text":"config<pre><code>opendesk.backtest.config.BacktestConfig\n</code></pre> <p>Backtesting configuration pipeline <code>dataclass</code>, which includes the necessary parameters to activate the internal strategy builder, is required for this process.</p> kwargs<pre><code>Dict\n</code></pre> <p>Additional parameters for <code>vbt.portfolio.base.Portfolio.from_order_func</code> function. It accepts:</p> <ul> <li>wrapper: ArrayWrapper,</li> <li>close: tp.ArrayLike,</li> <li>order_records: tp.RecordArray,</li> <li>log_records: tp.RecordArray,</li> <li>init_cash: tp.ArrayLike,</li> <li>cash_sharing: bool,</li> <li>call_seq: tp.Optional[tp.Array2d] = None,</li> <li>fillna_close: tp.Optional[bool] = None,</li> <li>trades_type: tp.Optional[tp.Union[int, str]] = None) -&gt; None:</li> </ul> <p>When Vectorbt Default to None<p>If you look at the arguments of each class method, you will notice that most of them default to None. None has a special meaning in <code>vectorbt</code>: it's a command to pull the default value from the global settings config - <code>settings</code>. The branch for the Portfolio can be found` under the key 'portfolio'. For example, the default size is:</p> <pre><code>vbt.settings.portfolio['size']\n</code></pre> </p>"},{"location":"documentation/api/strategy/backtest/#returns","title":"Returns","text":"<p><code>vectorbt.Portfolio.from_order_func</code>: vectorbt <code>Portfolio</code> ecosystem.</p> <p>Example Backtest<p>The <code>backtest</code> classmethod is initalized through the <code>BacktestConfig</code> dataclass, which facilitates feature integration. Mandatory variables <code>universe</code>, <code>model_data</code> and <code>steps</code> are set, as follow:</p> <p> <pre><code>$ from opendesk import BacktestConfig\n$ from opendesk import Strategy\n$ config = BacktestConfig(\n$     universe=stock_prices, $     model_data=model_data, $     steps=steps\n$ )\n$ backtest = Strategy.backtest(config)\n$ backtest.stats()\n&lt;span style=\"color: grey;\"&gt;Start                                 2019-01-02 00:00:00\nEnd                                   2022-12-30 00:00:00\nPeriod                                 1008 days 00:00:00\nStart Value                                         100.0\nEnd Value                                      152.092993\nTotal Return [%]                                52.092993\nBenchmark Return [%]                            62.173178\nMax Gross Exposure [%]                          31.819902\nTotal Fees Paid                                       0.0\nMax Drawdown [%]                                13.822639\nMax Drawdown Duration                   320 days 00:00:00\nTotal Trades                                          391\nTotal Closed Trades                                   372\nTotal Open Trades                                      19\nOpen Trade PnL                                  14.614093\nWin Rate [%]                                    54.301075\nBest Trade [%]                                 122.866679\nWorst Trade [%]                               -164.309231\nAvg Winning Trade [%]                           22.115064\nAvg Losing Trade [%]                            -19.44764\nAvg Winning Trade Duration    158 days 21:51:40.990099010\nAvg Losing Trade Duration     143 days 02:49:24.705882354\nProfit Factor                                    1.417933\nExpectancy                                        0.10075\nSharpe Ratio                                     0.942305\nCalmar Ratio                                     0.799575\nOmega Ratio                                      1.179846\nSortino Ratio                                    1.431216\nName: group, dtype: object\n&lt;/span&gt;\n</code></pre> </p> </p>"},{"location":"documentation/api/strategy/backtest/#backtestconfig","title":"BacktestConfig","text":"<pre><code>@dataclass\nbacktest.config.BacktestConfig(\nsteps:\u00a0List, \nuniverse:\u00a0pandas.core.frame.DataFrame, \nmodel_data:\u00a0Optional[pandas.core.frame.DataFrame]\u00a0=\u00a0None, \ntopdown:\u00a0Optional[bool]\u00a0=\u00a0False, \nmapping_table:\u00a0Optional[Dict]\u00a0=\u00a0None, \nmapping_weights:\u00a0Optional[Dict]\u00a0=\u00a0None, \nfreq:\u00a0Optional[int]\u00a0=\u00a0252, \nverbose:\u00a0Optional[bool]\u00a0=\u00a0False, \nfit_backend:\u00a0Optional[str]\u00a0=\u00a0'joblib', \nestimate:\u00a0Optional[Type]\u00a0=\u00a0&lt;built-in function sum&gt;, \nportfolio:\u00a0Optional[str]\u00a0=\u00a0'optimize', \noptimize_model:\u00a0Optional[str]\u00a0=\u00a0'mvo', \noptimize_cov_matrix_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \noptimize_expected_returns_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \noptimize_black_litterman:\u00a0Optional[bool]\u00a0=\u00a0False,\noptimize_black_litterman_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0None, \noptimize_weight_bounds:\u00a0Optional[Tuple[int,\u00a0int]]\u00a0=\u00a0(-1, 1), \noptimize_solver:\u00a0Optional[str]\u00a0=\u00a0None, \noptimize_verbose:\u00a0Optional[bool]\u00a0=\u00a0False, \noptimize_solver_options:\u00a0Optional[Dict]\u00a0=\u00a0None, \nadd_alpha_block_constraints:\u00a0Optional[bool]\u00a0=\u00a0True, \nadd_n_asset_constraints:\u00a0Optional[int]\u00a0=\u00a0None, \nadd_l2_regularization:\u00a0Optional[bool]\u00a0=\u00a0True, \nadd_gamma:\u00a0Optional[int]\u00a0=\u00a02, \nadd_custom_objectives:\u00a0Optional[List[Tuple[Type,\u00a0Dict[str,\u00a0Any]]]]\u00a0=\u00a0None, \nadd_custom_constraints:\u00a0Optional[List[Type]]\u00a0=\u00a0None, \noptimize_method:\u00a0Optional[str]\u00a0=\u00a0'efficient_risk', \noptimize_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \ndiscrete_allocation_model:\u00a0Optional[str]\u00a0=\u00a0'equal_weighted',\ndiscrete_allocation_model_params:\u00a0Dict[str,\u00a0Any]\u00a0=\u00a0None, \ndiscrete_allocation_range_bound:\u00a0Optional[str]\u00a0=\u00a0'mid', \nbacktest_every_nth:\u00a0Optional[int]\u00a0=\u00a030, \nbacktest_history_len:\u00a0Optional[int]\u00a0=\u00a0-1, \nbacktest_direction:\u00a0Optional[str]\u00a0=\u00a0'long_short', \nbacktest_backup:\u00a0Optional[str]\u00a0=\u00a0'discrete_allocation', \nbacktest_backup_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;\n)\n</code></pre> <p><code>@dataclass</code></p> <p>A <code>dataclass</code> is a decorator that is used to define a class that will be used to store data. It automatically generates special methods, such as <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code>, based on the class's attributes. A dataclass is defined using the <code>@dataclass</code> decorator.</p>"},{"location":"documentation/api/strategy/backtest/#parameters_1","title":"Parameters","text":"steps<pre><code>List[Tuple[str, Type, Dict, Dict]]\n</code></pre> <p>Alpha blocks in order of execution. More information, please visit the Blocks. The parameter <code>steps</code> is required to enable the smooth integration of the strategy into the module with minimal disruption. It is a list of tuples containing:</p> <ul> <li>Custom name of the selected block (e.i. <code>myblock</code>)</li> <li>Class object of the selected block (e.i. <code>MyBlock</code>)</li> <li>Necessary parameters used at the block initialization (<code>__init__</code>) section (e.i. <code>dict(param_1=\"param_1, param_2=\"param_2\")</code>)</li> <li>Additional data, <code>**kwargs</code>, required to train the model from the method <code>processing()</code></li> </ul> universe<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Market price time-series universe, each row is a date and each column is a ticker/id.</p> model_data<pre><code>Optional[pandas.core.frame.DataFrame] = None\n</code></pre> <p>Market returns time-series used to train the model.</p> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to <code>True</code>, a mapping table should be passed. Defaults to <code>False</code>.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> mapping_weights<pre><code>Optional[Dict[int, Tuple]] = None\n</code></pre> <p>Maps scores with range of weights. Defaults to <code>None</code>.</p> fit_backend<pre><code>Optional[str] = \"joblib\"\n</code></pre> <p>Run parallel multiprocessing or iterative process.</p> verbose<pre><code>Optional[bool] = None\n</code></pre> <p>Progress messages are printed. Applied to <code>fit()</code> and <code>optimize()</code> functions.</p> estimate<pre><code>Optional[Type] = sum\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores. The <code>func</code> parameter can be any object that is compatible with the <code>.apply</code> function in the pandas library.</p> portfolio<pre><code>Optional[str] = \"optimize\"\n</code></pre> <p>Portfolio construction procedure to allocate weights. It could be <code>optimize</code> or <code>discrete allocation</code>. Defauts to <code>optimize</code></p> optimize_model<pre><code>Optional[str] = \"mvo\"\n</code></pre> <p>Type of optimizer to be used. Type of optimization:</p> <ul> <li><code>mvo</code>: Mean-variance optimization</li> <li><code>hrp</code>: Hierarchical Risk Parity</li> </ul> optimize_expected_returns_params<pre><code>Optional[Dict[str, Any]]\n</code></pre> <p>Parameters to compute an estimate of future returns:</p> <ul> <li><code>method</code> (str): the return model to use. Should be one of:<ul> <li><code>mean_historical_return</code></li> <li><code>ema_historical_return</code></li> <li><code>capm_return</code></li> </ul> </li> <li><code>**kwargs</code>: Method specificities Defaults to:</li> </ul> <pre><code>dict(\nmethod=\"ema_historical_return\", \ncompounding=True, \nspan=500, \nfrequency=252, \nlog_returns=False\n)\n</code></pre> optimize_cov_matrix_params<pre><code>Optional[Dict[str, Any]]\n</code></pre> <p>Parameters to compute a covariance matrix:</p> <ul> <li><code>method</code> (str): the risk model to use. Should be one of:<ul> <li><code>sample_cov</code></li> <li><code>semicovariance</code></li> <li><code>exp_cov</code></li> <li><code>ledoit_wolf</code></li> <li><code>ledoit_wolf_constant_variance</code></li> <li><code>ledoit_wolf_single_factor</code></li> <li><code>ledoit_wolf_constant_correlation</code></li> <li><code>oracle_approximating</code></li> </ul> </li> <li><code>**kwargs</code>: Method specificities Defautls to:</li> </ul> <pre><code>dict(\nmethod=\"ledoit_wolf\", \nfrequency=252, \nlog_returns=False\n)\n</code></pre> optimize_weight_bounds<pre><code>Optional[Tuple[int, int]] = (-1, 1)\n</code></pre> <p>Minimum and maximum weight of each asset or single min/max pair if all identical, defaults to (-1, 1). If <code>weight_bounds=(-1, 1)</code>, allows short positions. Defaults to <code>(-1, 1)</code>.</p> optimize_solver<pre><code>Optional[str] = None\n</code></pre> <p>name of solver. list available solvers with: <code>cvxpy.installed_solvers()</code>.</p> optimize_solver_options<pre><code>Optional[Dict] = None\n</code></pre> <p>Parameters for the given solver.</p> add_alpha_block_constraints<pre><code>Optional[bool] = True\n</code></pre> <p>Alpha blocks core constraints. It adds constraints on the sum of weights of different groups of assets. Most commonly, these will be sector constraints. These constraints a particularly relevant when working with alpha blocks (top-down or bottom-up), as we aim to limit our exposure to paricular group of assets. Defaults to <code>True</code>.</p> add_n_asset_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>Number of assets in the portfolio constraints. Cardinality constraints are not convex, making them difficult to implement. However, we can treat it as a mixed-integer program and solve (provided you have access to a solver). for small problems with less than 1000 variables and constraints, you can use the community version of CPLEX available in python <code>pip install cplex</code>.</p> <p><code>n_asset_constraints</code></p> <p>This functionnality is still work in progress, as it requires external capabilities (<code>cplex</code>).</p> add_l2_regularization<pre><code>Optional[bool] = True\n</code></pre> <p>L2 regularisation, i.e \\(\\gamma ||w||^2\\), to increase the number of nonzero weights.</p> <p>Mean-variance optimization often results in many weights being negligible, i.e the efficient portfolio does not end up including most of the assets. This is expected behaviour, but it may be undesirable if you need a certain number of assets in your portfolio. </p> <p>In order to coerce the mean-variance optimizer to produce more non-negligible weights, we add what can be thought of as a \u201csmall weights penalty\u201d to all of the objective functions, parameterised by \\(\\gamma\\) (gamma). This term reduces the number of negligible weights, because it has a minimum value when all weights are equally distributed, and maximum value in the limiting case where the entire portfolio is allocated to one asset. We refer to it as L2 regularisation because it has exactly the same form as the L2 regularisation term in machine learning, though a slightly different purpose (in ML it is used to keep weights small while here it is used to make them larger).</p> <p>Gamma</p> <p>In practice, \\(\\gamma\\) must be tuned to achieve the level of regularisation that you want. However, if the universe of assets is small (less than 20 assets), then gamma=1 is a good starting point. For larger universes, or if you want more non-negligible weights in the final portfolio, increase gamma.</p> add_gamma<pre><code>Optional[int] = 2\n</code></pre> <p>L2 regularisation parameter. Defaults to 2. Increase if you want more non-negligible weights</p> add_custom_objectives<pre><code>Optional[List[Tuple[Type, Dict[str, Any]]]] = None\n</code></pre> <p>List of lambda functions to add new term into the based objective function. This term must be convex, and built from cvxpy atomic functions.</p> add_custom_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>List of lambda function (e.i. all assets &lt;= 3% of the total portfolio = [lambda w: w &lt;= .03]. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</p> optimize_method<pre><code>Optional[str] = \"efficient_risk\"\n</code></pre> <p>Optimization method that can be called (corresponding to different objective functions).</p> <p>Object Instantiation</p> <p>A new EfficientFrontier object should be instantiated if you want to make any change to objectives/constraints/bounds/parameters.  The backtesting framework re-instantiate the optimization process at every rebalancing periods.</p> optimize_params<pre><code>Optional[Dict[str, Any]] = dict(target_volatility=0.08, market_neutral=True)\n</code></pre> <p>Optimization method parameters. Defaults to:</p> <pre><code>dict(\ntarget_volatility=0.08, \nmarket_neutral=True\n)\n</code></pre> discrete_allocation_method<pre><code>Optional[str] = \"uniform\"\n</code></pre> <p>Method used to allocate weights.</p> discrete_allocation_model_params<pre><code>Dict[str, Any] = None\n</code></pre> <p>Model specific parameters.</p> discrete_allocation_range_bound<pre><code>Optional[str] = \"mid\"\n</code></pre> <p>Bound from <code>mapping_weights</code>. Total budget (in %) to apply.</p> backtest_every_nth<pre><code>Optional[int] = 30\n</code></pre> <p>Backtest rebalancing period in days. Defaults to 30. </p> backtest_history_len<pre><code>Optional[int] = -1\n</code></pre> <p>Backtest model training period. If -1, the model trains the entire history. Defaults to -1.</p> backtest_direction<pre><code>Optional[str] = \"long_short\"\n</code></pre> <p>Backtest Direction. It can be <code>long_only</code>, <code>short_only' or</code>long_short<code>. Defaults to</code>long_short`.</p> backtest_backup<pre><code> Optional[str] = \"discrete_allocation\"\n</code></pre> <p>Backtest \"back-up\" used as a fallback in the event that the optimizer is unable to deliver feasible weights. It can be another portfolio construction procedure to allocate weights: <code>optimize</code> or <code>discrete allocation</code>. Defauts to <code>discrete_allocation</code>.</p> backtest_backup_params<pre><code>Optional[Dict[str, Any]] = dict(range_bound=\"mid\")\n</code></pre> <p>Backtest \"back-up\" configuration. Defauts to <code>dict(range_bound=\"mid\")</code> as <code>discrete_allocation</code> is set in <code>backtest_backup</code>.</p>"},{"location":"documentation/api/strategy/backtest/#example-backtest","title":"Example Backtest","text":"<p>Example Backtest<p>For this example, we are using <code>yfinance</code>, a python library to fetch yahoo market data. First, we import the module and query some tickers from 2019 to 2022: <pre><code>import yfinance as yf\n# Date range\nstart = \"2019-01-01\"\nend = \"2022-12-31\"\n# Tickers of assets\nassets = [\n\"JCI\", \"TGT\", \"CMCSA\", \"CPB\", \n\"MO\", \"APA\", \"MMC\", \"JPM\",\n\"ZION\", \"PSA\", \"BAX\", \"BMY\", \n\"LUV\", \"PCAR\", \"TXT\", \"TMO\",\n\"DE\", \"MSFT\", \"HPQ\", \"SEE\",\n]\n# Downloading data\ndata = yf.download(assets, start=start, end=end)\nstock_prices = data.loc[:, (\"Close\", slice(None))]\nstock_prices.columns = assets\n</code></pre></p> <pre><code>from opendesk.backtest import BacktestConfig\nfrom opendesk.blocks import Reversion, SignalBased, TrendFollowing\nfrom opendesk.strategy import Strategy\n# Config\nconfig = BacktestConfig(\nuniverse=close_prices,\nsteps=[\n(\"Reversion\", Reversion),\n(\"TrendFollowing\", TrendFollowing),\n],\nportfolio_construction=\"optimize\",\nportfolio_expected_returns_params=dict(\nmethod=\"capm_return\"\n),\nportfolio_cov_matrix_params=dict(\nmethod=\"sample_cov\"\n),\nsolver_method=\"efficient_risk\",\nsolver_params=dict(\ntarget_volatility=0.2, \nmarket_neutral=True\n),\nadd_constraints=[\nlambda w: w &lt;= 0.1, \nlambda w: w &gt;= -0.1\n],\nbacktest_every_nth=30,\n)\n</code></pre> <p> <pre><code>$ backtest = Strategy.backtest(config)\n$ backtest.total_profit(group_by=False)\n&lt;span style=\"color: grey;\"&gt;APA      20.123646\nBAX       6.338269\nBMY      -4.664290\nCMCSA     1.361760\nCPB       1.361636\nDE        8.268657\nHPQ       2.813247\nJCI      11.984073\nJPM       1.150170\nLUV      12.407130\nMMC      -4.770791\nMO       -4.857592\nMSFT      6.389380\nPCAR      5.609880\nPSA       4.577316\nSEE      -2.475789\nTGT      -7.691861\nTMO      -5.878451\nTXT       7.768601\nZION     -7.347374\nName: total_profit, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p>More information about the vectorbt ecosystem can be found in the official documentation.</p> </p>"},{"location":"documentation/api/strategy/exposures/","title":"Exposures","text":""},{"location":"documentation/api/strategy/exposures/#add_blocks","title":"add_blocks","text":"<pre><code>Strategy.add_blocks(\n*args: Tuple[str, Type, Dict, Dict]\n) -&gt; opendesk.strategy.Strategy:\n</code></pre> <p>After initialization, additional strategies (blocks) can be added to the instance <code>steps</code>. We included in the below snippet two additionnal examples: A reversion factor and a Markov regime switching model.</p>"},{"location":"documentation/api/strategy/exposures/#parameters","title":"Parameters","text":"*args<pre><code>Tuple[str, Type, Dict, Dict]\n</code></pre> <p>Additional blocks to be added.</p>"},{"location":"documentation/api/strategy/exposures/#returns","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Add Blocks</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, MarkovRegression\nstrategy = Strategy(steps)\nother_blocks = (\n(\n\"reversion\",\nReversion, # (1)\n{'short_term_rule': None, 'long_term_rule': None}\n),\n(\n\"markov_regression\",\nMarkovRegression, # (2)\n{\"n_phase\": 3},\n{\"cli\": cli}\n)\n)\n</code></pre> <ol> <li>Calculate sentiment using Reversion Ranking Method.     More information provided in the Model Glossary.</li> <li>Calculate sentiment using Trend Following Ranking Method.     More information provided in the Model Glossary.</li> </ol> <p> <pre><code>$ strategy = Strategy(steps)\n &lt;span style=\"color: grey;\"&gt;Blocks -&gt; signal + gaussian_mixture&lt;/span&gt;\n$ strategy.add_blocks(other_blocks)\n&lt;span style=\"color: grey;\"&gt;Blocks -&gt; signal + gaussian_mixture + reversion + markov_regression&lt;/span&gt;\n</code></pre> </p>"},{"location":"documentation/api/strategy/exposures/#check_goup_constraints","title":"check_goup_constraints","text":"<pre><code>Strategy.check_group_constraints(\nweights: pandas.core.series.Series\n) \u2011&gt; pandas.core.frame.DataFrame\n</code></pre> <p>The performance of the model can be assessed by examining the satisfaction of predetermined constraints. By comparing the output of the model to the set of exposures produced by the actual allocation, it is possible to determine if it is operating as intended. To do this, we use the <code>check_group_constraints()</code> method, which groups assets and sum their weights.</p>"},{"location":"documentation/api/strategy/exposures/#parameters_1","title":"Parameters","text":"weights<pre><code>pandas.core.series.Series\n</code></pre> <p>Portfolio weights calculated either with the <code>optimize</code> or the <code>discrete_allocation</code> methods.</p>"},{"location":"documentation/api/strategy/exposures/#returns_1","title":"Returns","text":"<p><code>pandas.core.frame.DataFrame</code> object with calculated aggregated weights and target weights.</p> <p>Example Group Constraints</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, TrendFollowing\nstrategy = Strategy(steps, topdown, mapping_table)\nstrategy.fit(df).estimate(sum).portfolio(stock_prices)    \nweights = strategy.discrete_allocation(\"equal_weighted\")\nseries_weights = pd.Series(weights, name=\"weights\")\n</code></pre> <p> <pre><code>$ strategy.check_group_constraints(series_weights))\n&lt;span style=\"color: grey;\"&gt;            weights  target weights\nsector 1      0.15    (0.05, 0.15)\nsector 10     0.10    (0.05, 0.15)\nsector 2      0.00      (0.0, 0.0)\nsector 3      0.00      (0.0, 0.0)\nsector 4     -0.05  (-0.15, -0.05)\nsector 5     -0.15  (-0.15, -0.05)\nsector 6      0.00      (0.0, 0.0)\nsector 7      0.00      (0.0, 0.0)\nsector 8     -0.05  (-0.15, -0.05)\nsector 9      0.00      (0.0, 0.0)\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"documentation/api/strategy/exposures/#estimate","title":"estimate","text":"<pre><code>Strategy.estimate(\nfunc: Type, \ninplace: Optional[bool] = False\n) \u2011&gt; Union[pandas.core.series.Series, opendesk.strategy.Strategy]\n</code></pre> <p>Aggregate exposures by aggregating each units using a predetermined function <code>func</code>.</p>"},{"location":"documentation/api/strategy/exposures/#parameters_2","title":"Parameters","text":"func<pre><code>Type\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores. The <code>func</code> parameter can be any object that is compatible with the <code>.apply</code> function in the pandas library.</p> inplace<pre><code>Optional[bool] = False\n</code></pre> <p>Returns a copy of <code>exposures</code>. Defaults to <code>False</code>.</p>"},{"location":"documentation/api/strategy/exposures/#returns_2","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Estimate</p> <p>Some examples of different <code>func</code> parameter are provided below:</p> MeanMedianMaxMin <p> <pre><code>$ strategy.estimate(mean, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     0.50\nsector 2     0.25\nsector 3     0.50\nsector 4    -0.75\nsector 5     0.00\nsector 6    -0.50\nsector 7    -1.00\nsector 8     0.50\nsector 9    -0.25\nsector 10    0.75\nName: mean, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     0.5\nsector 2     0.5\nsector 3     1.0\nsector 4    -0.5\nsector 5    -0.5\nsector 6    -0.5\nsector 7    -1.5\nsector 8     0.5\nsector 9     0.0\nsector 10    0.5\nName: median, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     2\nsector 2     2\nsector 3     2\nsector 4     0\nsector 5     2\nsector 6     1\nsector 7     1\nsector 8     2\nsector 9     0\nsector 10    2\nName: max, dtype: int64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1    -1\nsector 2    -2\nsector 3    -2\nsector 4    -2\nsector 5    -1\nsector 6    -2\nsector 7    -2\nsector 8    -1\nsector 9    -1\nsector 10    0\nName: min, dtype: int64\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"documentation/api/strategy/exposures/#fit","title":"fit","text":"<pre><code>Strategy.fit(\nmodel_data: pandas.core.frame.DataFrame, \nbackend: Optional[str] = 'joblib', \nverbose: Optional[bool] = True\n) \u2011&gt; opendesk.strategy.Strategy\n</code></pre> <p>Executes each provided blocks with common dataset.</p>"},{"location":"documentation/api/strategy/exposures/#parameters_3","title":"Parameters","text":"model_data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Market returns time-series used to train the model.</p> backend<pre><code>Optional[str] = \"joblib\"\n</code></pre> <p>Run parallel multiprocessing or iterative process. Defaults to <code>\"joblib\"</code>.</p> verbose<pre><code>Optional[bool] = True\n</code></pre> <p>Joblib progress messages are printed. Defaults to <code>True</code>.</p>"},{"location":"documentation/api/strategy/exposures/#returns_3","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Fit</p> <p>After initializing the class, the market returns can be fit using the <code>fit()</code> method, as in the following example:</p> <pre><code>strategy = Strategy(steps).fit(df)\n</code></pre> <p>This returns the instance object <code>self</code> on which the method was called. </p>"},{"location":"documentation/api/strategy/portfolio_construction/","title":"Porfolio","text":"<p>Portfolio construction is the process of creating a balanced collection of investments that aligns with an investor's financial goals, risk tolerance, and investment horizon. The goal of portfolio construction is to maximize returns while minimizing risk.</p> <p>Portfolio construction involves translating scores into weights, can be a complex and nuanced process. We have developed two methods that allows for greater flexibility and experimentation: </p> <ul> <li>Optimization</li> <li>Discrete Allocation</li> </ul> <p>These approach enables the exploration of a wide range of potential top-down and bottom-up portfolio compositions.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#portfolio","title":"portfolio","text":"<pre><code>Strategy.portfolio(\ndata: Optional[pandas.core.frame.DataFrame] = None, \n) \u2011&gt; opendesk.strategy.Strategy\n</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/#parameters","title":"Parameters","text":"data<pre><code>Optional[pandas.core.frame.DataFrame] = None\n</code></pre> <p>Market price time-series, each row is a date and each column is a ticker/id. If <code>None</code>, it takes <code>model_data</code>, the dataset used in the <code>fit()</code> method. Defaults to <code>None</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#returns","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#portfolioconstruction","title":"PortfolioConstruction","text":"<pre><code>portfolio.PortfolioConstruction(\ndata: pandas.core.frame.DataFrame,\ngroup_constraints: Optional[Dict[str, Tuple[float, float]]] = None,\nexposures: Optional[pandas.core.series.Series] = None,\ntopdown: Optional[bool] = False,\nmapping_table: Optional[Dict[str, str]] = None,\nfreq: Optional[int] = 252\n)\n</code></pre> <p>Portfolio construction is the process of creating a balanced collection of investments that aligns with an investor's financial goals, risk tolerance, and investment horizon. The goal of portfolio construction is to maximize returns while minimizing risk.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#parameters_1","title":"Parameters","text":"data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Adjusted closing prices of the asset, each row is a date and each column is a ticker/id.</p> group_constraints<pre><code>Optional[Dict[str, Tuple[float, float]]] = None\n</code></pre> <p>Strategy constraints by group. Product of <code>mapping_weights</code> inputs and <code>exposures</code> outputs from the <code>Strategy</code> class.</p> exposures<pre><code>Optional[pandas.core.series.Series] = None\n</code></pre> <p>Strategy exposures attribute estimated from the <code>estimate()</code> function. </p> <p>In the portfolio optimization section, it also core to the <code>asset_views</code> property, to limit the overall average score (exposure) as a custom constraint. E.i. suppose that for each asset you have some \u201cscore\u201d \u2013 it could be an ESG metric, or some custom risk/return metric. It is simple to specify linear constraints, like \u201cportfolio ESG score must be greater than x\u201d: you simply create a vector of scores, add a constraint on the dot product of those scores with the portfolio weights, then optimize your objective:</p> <p>Example</p> <pre><code>from opendesk.blocks import ESGModel\n# portolfio mininum score to find\nportfolio_min_score = 0.5\n# start strategy\nesg_strategy = Strategy(steps=[(\"ESG\", ESGModel)], topdown=True, mapping_table=mapping_table)\nesg_strategy.fit(df).estimate(sum) # (1)\nesg_strategy.optimize(stock_prices)\nesg_strategy.portfolio(**portfolio_params) # (2)\nesg_strategy.add(\ncustom_constraints=[\nlambda w: strategy.asset_scores @ w &gt;= portfolio_min_score\n]\n)\n</code></pre> <ol> <li>Here, ESG scores are produced by the strategy <code>estimate()</code> function.</li> <li>Portfolio parameters are not explained here, as the goal of this snippet is to showcase the <code>custom_constraints</code> parameter with the optimizer <code>asset_scores</code> proprety.</li> </ol> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to True, a mapping table should be passed. Defaults to False.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> freq<pre><code>Optional[int] = 252\n</code></pre> <p>Number of time periods in a year, Defaults to 252 (the number of trading days in a year).</p>"},{"location":"documentation/api/strategy/portfolio_construction/#descendants","title":"Descendants","text":"<ul> <li>opendesk.strategy.Strategy</li> </ul>"},{"location":"documentation/api/strategy/portfolio_construction/#instance-variables","title":"Instance variables","text":""},{"location":"documentation/api/strategy/portfolio_construction/#asset_scores","title":"asset_scores","text":"asset_scores<pre><code>Dict[str, float]\n</code></pre> <p>Transform exposures to score at any level. If <code>topdown</code> is set to <code>True</code>, it transforms exposures at the lower level. Otherwise, It returns <code>exposures</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#asset_views","title":"asset_views","text":"asset_views<pre><code>Dict[str, float]\n</code></pre> <p>The alpha blocks implementation works with the Black-Litterman <code>asset_views</code>, where views direction is extracted from the median of weight range and the magnitude is \\(1.96 \\sigma\\), where \\(\\sigma\\) is the annualized volatility calculated from log returns.</p> <p>1.96 is Hardcoded</p> <p>In probability and statistics, the 97.5<sup>th</sup> percentile point of the standard normal distribution is a number commonly used for statistical calculations. The approximate value of this number is 1.96, meaning that 95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#lower_bound","title":"lower_bound","text":"lower_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Lower weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#mid_bound","title":"mid_bound","text":"mid_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Mid weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#upper_bound","title":"upper_bound","text":"upper_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Upper weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/#public-methods","title":"Public Methods","text":"<ul> <li><code>discrete_allocation()</code>: Set portfolio weights following a discrete allocation weighting scheme</li> <li><code>optimize()</code>: Portfolio optimization, which aims to select the optimal mix of assets in a portfolio in order to satisfy the defined objectives and constraints</li> </ul>"},{"location":"documentation/api/strategy/portfolio_construction/discrete_allocation/","title":"Discrete Allocation","text":"<p>The <code>portfolio()</code> calls the <code>PortfolioConstruction</code> class, which includes the implementation of the <code>discrete_allocation()</code> built-in public methods for discrete allocation procedures, which allows multiple pre-determined rule-based allocation strategies</p>"},{"location":"documentation/api/strategy/portfolio_construction/discrete_allocation/#discrete_allocation","title":"discrete_allocation","text":"<pre><code>Portfolio.discrete_allocation(\nmodel: str,\nmodel_params: Dict[str, Any] = None,\nrange_bound: Optional[str] = 'mid'\n) \u2011&gt; OrderedDict\n</code></pre> <p>Discrete allocation is a method for implementing predefined, rule-based allocation strategies for building optimal, diversified portfolios at scale. The use of the <code>topdown</code> parameter, when set to <code>True</code>, can introduce additional complexity in the portfolio construction process, as it involves diversifying allocation across a wide range of assets, using techniques such as uniform or market cap-based allocation. The <code>PortfolioConstruction.discrete_allocation()</code> function calls the <code>DiscreteAllocation</code> class (only when <code>topdown=True</code>), which offers a variety of rule-based weighting schemes. These weighting schemes, which are commonly used to construct factor portfolios, are designed to achieve a range of portfolio objectives.</p>"},{"location":"documentation/api/strategy/portfolio_construction/discrete_allocation/#parameters","title":"Parameters","text":"model<pre><code>Optional[str] = \"equal_weighted\"\n</code></pre> <p>Model used to allocate weights. Possible methods are:</p> <ul> <li><code>equal_weighted</code>: Asset equally weighted</li> <li><code>market_cap_weighted</code>: Asset weighted in proportion to their free-float market cap</li> <li><code>score_weighted</code>: Asset weighted in proportion to their target-factor scores</li> <li><code>score_tilt_weighted</code>: Asset weighted in proportion to the product of their market cap and factor score</li> <li><code>inv_volatility_weighted</code>: Asset weighted in proportion to the inverse of their historical volatility</li> <li><code>min_correlation_weighted</code>: Optimized weighting scheme to obtain a portfolio with minimum volatility under the assumption that all asset have identical volatilities</li> </ul> <p>Defaults to <code>equal_weighted</code>.</p> model_params<pre><code>Dict[str, Any] = None\n</code></pre> <p>Model specific parameters.</p> range_bound<pre><code>Optional[str] = \"mid\"\n</code></pre> <p>Weight bound (from <code>mapping_weights</code>). Total budget (in %) to apply. Possible values are:</p> <ul> <li><code>lower</code>: Lower weight bound</li> <li><code>mid</code>: Median weight</li> <li><code>upper</code>: Upper weight bound</li> </ul> <p>Defaults to <code>mid</code>.</p>"},{"location":"documentation/api/strategy/portfolio_construction/discrete_allocation/#returns","title":"Returns","text":"<p><code>OrderedDict</code>, discrete weights.</p>"},{"location":"documentation/api/strategy/portfolio_construction/discrete_allocation/#example-discrete-allocation","title":"Example Discrete Allocation","text":"<p>Example Discrete Allocation</p> <pre><code>from opendesk import Strategy\nstrategy = Strategy(steps=steps, topdown=True, mapping_table=mapping_table)\nstrategy.fit(df).estimate(sum)\nweights = strategy.portfolio(stock_prices).discrete_allocation(model=\"equal_weighted\")\n</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/","title":"Optimization","text":"<p>Portfolio optimization capabilities is the process of selecting the optimal mix of assets in a portfolio, with respect to the alpha scores, in order to maximize returns while minimizing risk. The <code>portfolio()</code> method has been implemented to streamline the process of optimization and facilitate the integration of backtesting.</p> <p>The <code>portfolio()</code> calls the <code>PortfolioConstruction</code> class, which includes the implementation of the <code>optimize()</code> built-in public methods for optimization procedures, which integrates vectorbt pro PyPortfolioOpt wrapper.</p>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#integration","title":"Integration","text":"<p>PyPortfolioOpt is a library that implements portfolio optimization methods, including classical efficient frontier techniques and Black-Litterman allocation, as well as more recent developments in the field like shrinkage and Hierarchical Risk Parity, along with some novel experimental features, like exponentially-weighted covariance matrices. </p> <p>PyPortfolioOpt implements a range of optimization methods that are very easy to use. The optimization procedure consists of several distinct steps (some of them may be skipped depending on the optimizer):</p> <ul> <li>Calculate the expected returns (mostly located in <code>pypfopt.expected_returns</code>)</li> <li>Calculate the covariance matrix (mostly located in <code>pypfopt.risk_models</code>)</li> <li>Initialize and set up an optimizer (mostly located in <code>pypfopt.efficient_frontier</code>, with the base class located in `pypfopt.base_optimizer) including objectives, constraints, and target</li> <li>Run the optimizer to get the weights</li> <li>Convert the weights into a discrete allocation (optional)</li> </ul> <p>For example, let's perform the mean-variance optimization (MVO) for maximum Sharpe:</p> <pre><code>from pypfopt.expected_returns import mean_historical_return\nfrom pypfopt.risk_models import CovarianceShrinkage\nfrom pypfopt.efficient_frontier import EfficientFrontier\nexpected_returns = mean_historical_return(data)\ncov_matrix = CovarianceShrinkage(data).ledoit_wolf()\noptimizer = EfficientFrontier(expected_returns, cov_matrix)\nweights = optimizer.max_sharpe()\n</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#parsing","title":"Parsing","text":"<p>The entire codebase of PyPortfolioOpt (with a few exceptions) has consistent argument and function namings, such that we can build a semantic web of functions acting as inputs to other functions. Therefore, the user just needs to provide the target function (e.g. <code>EfficientFrontier.max_sharpe</code>), and we can programmatically figure out the entire call stack having the pricing data alonw. If the user passes any additional keyword arguments, we can check which functions from the stack accept those arguments and automatically pass them.</p> <p>For the example above, the web would be:</p> <pre><code>flowchart TD\n    U[User] -- prices --&gt; m[mean_historical_return]\n    U -- prices --&gt; c[CovarianceShrinkage.ledoit_wolf]\n    m -- expected_returns --&gt; e[EfficientFrontier]\n    c -- cov_matrix --&gt; e</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#auto-optimization","title":"Auto-optimization","text":"<p>Knowing how to parse and resolve function arguments, vectorbt implements a function <code>pypfopt_optimize</code>, which takes user requirements and translates them into function calls:</p> <pre><code>vbt.pypfopt_optimize(prices=data.get(\"Close\"))\n</code></pre> <p>In short, <code>pypfopt_optimize</code> triggers a waterfall of argument resolutions by parsing arguments, including the calculation of the expected returns and the risk model quantifying asset risk. Then, it adds objectives and constraints to the optimizer instance. Finally, it calls the target metric method (such as <code>max_sharpe</code>) or custom convex/non-convex objective using the same parsing procedure as we did above. If wanted, it can also translate continuous weights into discrete ones using <code>pypfopt.DiscreteAllocation</code>.</p> <p>Since multiple PyPortfolioOpt functions can require the same argument that has to be pre-computed yet, pypfopt_optimize deploys a built-in caching mechanism. Below, we will demonstrate various optimizations done both using PyPortfolioOpt and vectorbt:</p> <p>Optimizing a long/short portfolio to minimise total variance:</p> pypfoptopendesk <pre><code>S = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(None, S, weight_bounds=(-1, 1))\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=None,\nweight_bounds=(-1, 1),\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Optimizing a portfolio to maximise the Sharpe ratio, subject to sector constraints. In opendesk, while the sector constraints is also an option, we can set the <code>alpha_block_constraints</code> to True, which constraints the portfolio depending on pre-modeled alpha blocks exposures:</p> pypfoptopendesk <pre><code>from pypfopt.expected_returns import capm_return\nmu = capm_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\nef.max_sharpe()\nweights = ef.clean_weights()\nweights\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\nalpha_block_constraints=True\n)\n</code></pre> <p>Optimizing a portfolio to maximise return for a given risk, subject to sector constraints, with an L2 regularisation objective:</p> pypfoptopendesk <pre><code>from pypfopt.objective_functions import L2_reg\nmu = capm_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\nef.add_objective(L2_reg, gamma=0.1)\nef.efficient_risk(0.15)\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\nsector_mapper=sector_mapper,\nsector_lower=sector_lower,\nsector_upper=sector_upper,\nobjectives=[\"L2_reg\"],  \ngamma=0.1,  \ntarget=\"efficient_risk\",\ntarget_volatility=0.15  \n)\n</code></pre> <p>Optimizing along the mean-semivariance frontier:</p> pypfoptopendesk <pre><code>from pypfopt import EfficientSemivariance\nfrom pypfopt.expected_returns import returns_from_prices\nmu = capm_return(data.get(\"Close\"))\nreturns = returns_from_prices(data.get(\"Close\"))\nreturns = returns.dropna()\nes = EfficientSemivariance(mu, returns)\nes.efficient_return(0.01)\nweights = es.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\noptimizer=\"efficient_semivariance\",  \ntarget=\"efficient_return\",\ntarget_return=0.01\n)\n</code></pre> <p>Minimizing transaction costs:</p> <pre><code>initial_weights = np.array([1 / len(data.symbols)] * len(data.symbols))\n</code></pre> pypfoptopendesk <pre><code>from pypfopt.objective_functions import transaction_cost\nmu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_objective(transaction_cost, w_prev=initial_weights, k=0.001)\nef.add_objective(L2_reg, gamma=0.05)\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nobjectives=[\"transaction_cost\", \"L2_reg\"],\nw_prev=initial_weights, \nk=0.001,\ngamma=0.05,\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Custom convex objective:</p> <pre><code>import cvxpy as cp\ndef logarithmic_barrier_objective(w, cov_matrix, k=0.1):\nlog_sum = cp.sum(cp.log(w))\nvar = cp.quad_form(w, cov_matrix)\nreturn var - k * log_sum\n</code></pre> pypfoptopendesk <pre><code>mu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S, weight_bounds=(0.01, 0.3))\nef.convex_objective(logarithmic_barrier_objective, cov_matrix=S, k=0.001)\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nweight_bounds=(0.01, 0.3),\nk=0.001,\ntarget=logarithmic_barrier_objective  \n)\n</code></pre> <p>Custom non-convex objective:</p> <pre><code>def deviation_risk_parity(w, cov_matrix):\ncov_matrix = np.asarray(cov_matrix)\nn = cov_matrix.shape[0]\nrp = (w * (cov_matrix @ w)) / cp.quad_form(w, cov_matrix)\nreturn cp.sum_squares(rp - 1 / n).value\n</code></pre> pypfoptopendesk <pre><code>mu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.nonconvex_objective(deviation_risk_parity, ef.cov_matrix)\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \ntarget=deviation_risk_parity,  \ntarget_is_convex=False\n)\n</code></pre> <p>Black-Litterman Allocation, where <code>viewdict</code> is integrated within alpha blocks methods, where views are extracted from the median of weight range and magnitude calculated by multiplying the annualized volatility times \\(1.96\\) (95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean):</p> <pre><code>sp500_data = vbt.YFData.fetch(\n\"^GSPC\", \nstart=data.wrapper.index[0], \nend=data.wrapper.index[-1]\n)\n# example given in vectorbt\nmarket_caps = data.get(\"Close\") * data.get(\"Volume\")\nviewdict = {\n\"ADAUSDT\": 0.20, \n\"BNBUSDT\": -0.30, \n\"BTCUSDT\": 0, \n\"ETHUSDT\": -0.2, \n\"XRPUSDT\": 0.15\n}\n</code></pre> pypfoptopendesk <pre><code>from pypfopt.black_litterman import (\nmarket_implied_risk_aversion,\nmarket_implied_prior_returns,\nBlackLittermanModel\n)\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\ndelta = market_implied_risk_aversion(sp500_data.get(\"Close\"))\nprior = market_implied_prior_returns(market_caps.iloc[-1], delta, S)\nbl = BlackLittermanModel(S, pi=prior, absolute_views=viewdict)\nrets = bl.bl_returns()\nef = EfficientFrontier(rets, S)\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"bl_returns\",  \nmarket_prices=sp500_data.get(\"Close\"),\nmarket_caps=market_caps.iloc[-1],\nabsolute_views='alpha_blocks',\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Hierarchical Risk Parity:</p> pypfoptopendesk <pre><code>from pypfopt import HRPOpt\nrets = returns_from_prices(data.get(\"Close\"))\nhrp = HRPOpt(rets)\nhrp.optimize()\nweights = hrp.clean_weights()\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \noptimizer=\"hrp\",\ntarget=\"optimize\"\n)\n</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#argument-groups","title":"Argument groups","text":"<p>In cases where two functions require an argument with the same name but you want to pass different values to them, pass the argument as an instance of <code>pfopt_func_dict</code> where keys should be functions or their names, and values should be different argument values:</p> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=\"bl_returns\",  \nmarket_prices=sp500_data.get(\"Close\"),\nmarket_caps=market_caps.iloc[-1],\nabsolute_views=viewdict,\ntarget=\"min_volatility\",\ncov_matrix=vbt.pfopt_func_dict({\n\"EfficientFrontier\": \"sample_cov\",  \n\"_def\": \"ledoit_wolf\"  \n})\n)\n</code></pre>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#optimize","title":"optimize","text":"<pre><code>Portfolio.optimize(\ntarget: Optional[Union[Callable, str]] = None,\ntarget_is_convex: Optional[bool] = None,\nweights_sum_to_one: Optional[bool] = None,\ntarget_constraints: Optional[List[Any]] = None,\ntarget_solver: Optional[str] = None,\ntarget_initial_guess: Optional[np.array] = None,\nobjectives: Optional[List[Union[Callable, str]]] = None,\nconstraints: Optional[List[Callable]] = None,\nalpha_block_constraints: Optional[bool] = True,\nsector_mapper: Optional[dict] = None,\nsector_lower: Optional[dict] = None,\nsector_upper: Optional[dict] = None,\ndiscrete_allocation: Optional[bool] = None,\nallocation_method: Optional[str] = None,\nsilence_warnings: Optional[bool] = None,\nignore_opt_errors: Optional[bool] = None,\n**kwargs\n) -&gt; OrderedDict:\n</code></pre> <p>Base optimizer model, allowing for the efficient computation of optimized asset weights. The portfolio method houses different optimization methods from PyPortfolioOpt, which generate optimal portfolios for various possible objective functions and parameters.</p> <p>New Object Instantiation</p> <p>A new object should be instantiated if you want to make any change to objectives/constraints/bounds/parameters.</p>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#parameters","title":"Parameters","text":"target<pre><code>Optional[Union[Callable, str]] = \"max_sharpe\"\n</code></pre> <ul> <li><code>min_volatility</code>: Optimizes for minimum volatility</li> <li><code>min_semivariance</code>: Minimises the portfolio semi-variance (downside deviation)</li> <li><code>max_sharpe()</code>: Optimizes for maximal Sharpe ratio (a.k.a the tangency portfolio)</li> <li><code>max_quadratic_utility</code>: Maximises the quadratic utility, given some risk aversion</li> <li><code>efficient_risk</code>: Maximises return for a given target risk</li> <li><code>efficient_return</code>: Minimises risk for a given target return</li> <li><code>min_cvar</code>: Minimises the CVaR</li> <li><code>min_cdar</code>: Minimises the CDaR</li> </ul> <p>Custom Objectives: The Kelly Criterion</p> <pre><code>def kelly_objective(w, e_returns, cov_matrix, k=3): # (1)\nvariance = np.dot(w.T, np.dot(cov_matrix, w))\nobjective = variance * 0.5 * k - np.dot(w, e_returns)\nreturn objective\nweights = strategy.portfolio(prices).optimize(target=kelly_objective)\n</code></pre> <ol> <li>In probability theory, the Kelly criterion is a formula that determines the optimal theoretical size for a bet. It is valid when the expected returns are known. The Kelly bet size is found by maximizing the expected value of the logarithm of wealth, which is equivalent to maximizing the expected geometric growth rate, J. L. Kelly Jr [1956]. The criterion is also known as the scientific gambling method, as it leads to higher wealth compared to any other strategy in the long run (i.e. the theoretical maximum return as the number of bets goes to infinity).</li> </ol> target_is_convex<pre><code>Optional[bool] = None\n</code></pre> <p>If <code>target_is_convex</code> is True, the function is added as a convex function. Otherwise, the function is added as a non-convex function.</p> weights_sum_to_one<pre><code>Optional[bool] = None\n</code></pre> <p>Convex objective. Forces the sum of weight equals 1.</p> target_constraints<pre><code>Optional[List[Any]] = None\n</code></pre> target_solver<pre><code> Optional[str] = None\n</code></pre> target_initial_guess<pre><code>Optional[np.array] = None\n</code></pre> objectives<pre><code>Optional[List[Union[Callable, str]]] = None\n</code></pre> <p>List of lambda functions to add new term into the based objective function. This term must be convex, and built from cvxpy atomic functions.</p> <p>Built-in objective functions wrapper includes:</p> <ul> <li> <p><code>L2_reg</code>: L2 regularisation, i.e \\(\\gamma ||w||^2\\), to increase the number of nonzero weights. Mean-variance optimization often results in many weights being negligible, i.e the efficient portfolio does not end up including most of the assets. This is expected behaviour, but it may be undesirable if you need a certain number of assets in your portfolio. In order to coerce the mean-variance optimizer to produce more non-negligible weights, we add what can be thought of as a \u201csmall weights penalty\u201d to all of the objective functions, parameterised by \\(\\gamma\\) (gamma). This term reduces the number of negligible weights, because it has a minimum value when all weights are equally distributed, and maximum value in the limiting case where the entire portfolio is allocated to one asset. We refer to it as L2 regularisation because it has exactly the same form as the L2 regularisation term in machine learning, though a slightly different purpose (in ML it is used to keep weights small while here it is used to make them larger).</p> </li> <li> <p><code>ex_ante_tracking_error</code>: Calculate the (square of) the ex-ante Tracking Error, i.e \\((w - w_b)^T \\Sigma (w-w_b)\\)</p> </li> <li><code>ex_post_tracking_error</code>: Calculate the (square of) the ex-post Tracking Error, i.e \\(Var(r - r_b)\\)</li> <li><code>portfolio_return</code>: Calculate the (negative) mean return of a portfolio</li> <li><code>portfolio_variance</code>: Calculate the total portfolio variance (i.e square volatility)</li> <li><code>quadratic_utility</code>: Quadratic utility function, i.e \\(\\mu - \\frac 1 2 \\delta  w^T \\Sigma w\\)</li> <li><code>sharpe_ratio</code>: Calculate the (negative) Sharpe ratio of a portfolio</li> <li><code>transaction_cost</code>: A very simple transaction cost model: sum all the weight changes and multiply by a given fraction (default to 10bps). This simulates a fixed percentage commission from your broker.</li> </ul> gamma<pre><code>Optional[float] = 1\n</code></pre> <p>L2 regularisation parameter, defaults to 1. Increase if you want more non-negligible weights. In practice, \\(\\gamma\\) must be tuned to achieve the level of regularisation that you want. However, if the universe of assets is small (less than 20 assets), then gamma=1 is a good starting point. For larger universes, or if you want more non-negligible weights in the final portfolio, increase gamma.</p> k<pre><code>Optional[float] = 0.001\n</code></pre> <p>When transaction cost objective is set, fractional cost per unit weight exchanged.</p> constraints<pre><code>Optional[List[Callable]] = None\n</code></pre> <p>List of lambda function (e.i. all assets &lt;= 3% of the total portfolio = [lambda w: w &lt;= .03]. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</p> alpha_block_constraints<pre><code>Optional[bool] = True\n</code></pre> <p>Alpha blocks core constraints. It adds constraints on the sum of weights of different groups of assets. Most commonly, these will be sector constraints. These constraints a particularly relevant when working with alpha blocks (top-down or bottom-up), as we aim to limit our exposure to paricular group of assets. Defaults to <code>True</code>.</p> sector_mapper<pre><code>Optional[dict] = None\n</code></pre> <p>Maps tickers to sectors. Equivalent to <code>Strategy.mapping_table</code>.</p> sector_lower<pre><code>Optional[dict] = None\n</code></pre> <p>Lower bounds for each sector. Equivalent to <code>Portfolio.lower_bound</code>.</p> sector_upper<pre><code>Optional[dict] = None\n</code></pre> <p>Upper bounds for each sector. Equivalent to <code>Portfolio.upper_bound</code>.</p> discrete_allocation<pre><code>Optional[bool] = None\n</code></pre> <p>The discrete_allocation module contains the <code>DiscreteAllocation</code> class, which offers multiple methods to generate a discrete portfolio allocation from continuous weights. If <code>discrete_allocation</code> is True, it calls <code>allocation_method</code> as an attribute of the allocation object.</p> allocation_method<pre><code>Optional[str] = \"lp_portfolio\"\n</code></pre> <p>Generate a discrete portfolio allocation from continuous weights.  Method are:</p> <ul> <li><code>greedy_portfolio</code>: Uses a greedy algorithm</li> <li><code>lp_portfolio</code>: Uses linear programming</li> </ul> silence_warnings<pre><code>Optional[bool] = None\n</code></pre> <p>Disable warnings. If set to False, it throws a warning stating that either the optimization fails or that an argument wasn't required by any function in the call stack. </p> ignore_opt_errors<pre><code>Optional[bool] = None\n</code></pre> <p>Ignore any target optimization errors.</p> optimizer<pre><code>Optional[str] = None\n</code></pre> <p>Specify the optimizer. It can be an instance of <code>pypfopt.base_optimizer.BaseOptimizer</code>, an attribute of <code>pypfopt</code>, a subclass of <code>pypfopt.base_optimizer.BaseOptimizer</code>, or one of the following options:</p> <ul> <li><code>efficient_frontier</code>: <code>pypfopt.efficient_frontier.EfficientFrontier</code></li> <li><code>efficient_cdar</code>: <code>pypfopt.efficient_frontier.EfficientCDaR</code></li> <li><code>efficient_cvar</code>: <code>pypfopt.efficient_frontier.EfficientCVaR</code></li> <li><code>efficient_semivariance</code>: <code>pypfopt.efficient_frontier.EfficientSemivariance</code></li> <li><code>black_litterman</code> or <code>bl</code>: <code>pypfopt.black_litterman.BlackLittermanModel</code></li> <li><code>hrp</code>: <code>pypfopt.hierarchical_portfolio.HRPOpt</code></li> <li><code>cla</code>: <code>pypfopt.cla.CLA</code></li> </ul> <p>Black-litterman</p> <p>Black-Litterman model takes a Bayesian approach to asset allocation. Specifically, it combines a prior estimate of returns (for example, the market-implied returns) with views on certain assets, to produce a posterior estimate of expected returns. It will then meaningfully propagate views, taking into account the covariance with other assets. </p> absolute_views<pre><code>Optional[pandas.core.series.Series | Dict[str, float] | str] = None\n</code></pre> <p>A colleciton of K absolute views on a subset of assets, defaults to None. If set to <code>alpha_blocks</code>, it computes the \"alpha blocks views\". The alpha blocks implementation works with the Black-Litterman absolute views, where views are extracted from the median of weight range and magnitude calculated by multiplying the annualized volatility times \\(1.96\\). In probability and statistics, the 97.5<sup>th</sup> percentile point of the standard normal distribution is a number commonly used for statistical calculations. The approximate value of this number is 1.96, meaning that 95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean.</p> pi<pre><code>Optional[pandas.core.series.Series | numpy.ndarray] = None\n</code></pre> <p>Nx1 prior estimate of returns, defaults to None. If pi=\u201dmarket\u201d, calculate a market-implied prior (requires market_caps to be passed). If pi=\u201dequal\u201d, use an equal-weighted prior.</p> omega<pre><code>Optional[pandas.core.frame.DataFrame | numpy.ndarray | string] = None\n</code></pre> <p>KxK view uncertainty matrix (diagonal), defaults to None Can instead pass \u201cidzorek\u201d to use Idzorek\u2019s method (requires you to pass view_confidences). If omega=\u201ddefault\u201d or None, we set the uncertainty proportional to the variance.</p> view_confidences<pre><code>Optional[pandas.core.series.Series | numpy.ndarray | List] = None\n</code></pre> <p>Kx1 vector of percentage view confidences (between 0 and 1), required to compute omega via Idzorek\u2019s method.</p> tau<pre><code>Optional[float] = 0.05\n</code></pre> <p>the weight-on-views scalar (default is 0.05) risk_aversion (positive float, optional) \u2013 risk aversion parameter, defaults to 1.</p> market_caps<pre><code>Optional[pandas.core.series.Series | numpy.ndarray] = None\n</code></pre> <p>Market caps for the assets, required if pi=\u201dmarket\u201d</p> risk_free_rate<pre><code>Optional[float] = 0.02\n</code></pre> <p>Risk-free rate of borrowing/lending, defaults to 0.02. The period of the risk-free rate should correspond to the frequency of expected returns.</p> expected_returns<pre><code>Optional[str] = \"mean_historical_return\"\n</code></pre> <p>Specify the expected returns. <code>expected_returns</code> can be an array, an attribute of <code>pypfopt.expected_returns</code>, a function, or one of the following options:</p> <ul> <li><code>mean_historical_return</code>: <code>pypfopt.expected_returns.mean_historical_return</code></li> <li><code>ema_historical_return</code>: <code>pypfopt.expected_returns.ema_historical_return</code></li> <li><code>capm_return</code>: <code>pypfopt.expected_returns.capm_return</code></li> <li><code>bl_returns</code>: <code>pypfopt.black_litterman.BlackLittermanModel.bl_returns</code></li> </ul> cov_matrix<pre><code>Optional[str] = \"ledoit_wolf\"\n</code></pre> <p>Specify the covariance matrix. <code>cov_matrix</code> can be an array, an attribute of <code>pypfopt.risk_models</code>, a function, or one of the following options:</p> <ul> <li><code>sample_cov</code>: pypfopt.risk_models.sample_cov</li> <li><code>semicovariance</code> or <code>semivariance</code>: pypfopt.risk_models.semicovariance</li> <li><code>exp_cov</code>: pypfopt.risk_models.exp_cov</li> <li><code>ledoit_wolf</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>constant_variance</code> as shrinkage factor</li> <li><code>ledoit_wolf_single_factor</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>single_factor</code> as shrinkage factor</li> <li><code>ledoit_wolf_constant_correlation</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>constant_correlation</code> as shrinkage factor</li> <li><code>oracle_approximating</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>oracle_approximating</code> as shrinkage factor</li> </ul> weight_bounds<pre><code>Optional[Tuple[float, float]] = (-1, 1)\n</code></pre> <p>Minimum and maximum weight of each asset or single min/max pair if all identical, defaults to (-1, 1). If <code>weight_bounds=(-1, 1)</code>, allows short positions.</p> target_return<pre><code>float\n</code></pre> <p>The desired return of the resulting portfolio.</p> market_neutral<pre><code>Optional[bool] = False\n</code></pre> <p>whether the portfolio should be market neutral (weights sum to zero), defaults to False. Requires negative lower weight bound.</p> target_volatility<pre><code>float\n</code></pre> <p>The desired maximum volatility of the resulting portfolio.</p> risk_aversion<pre><code>Optional[int] = 1\n</code></pre> <p>Risk aversion parameter (must be greater than 0), defaults to 1.</p> n_asset_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>Number of assets in the portfolio constraints. Cardinality constraints are not convex, making them difficult to implement. However, we can treat it as a mixed-integer program and solve (provided you have access to a solver). for small problems with less than 1000 variables and constraints, you can use the community version of CPLEX available in python <code>pip install cplex</code>.</p> <p><code>n_asset_constraints</code></p> <p>This functionnality is still work in progress, as it requires external capabilities (<code>cplex</code>).</p>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#returns","title":"Returns","text":"<p><code>OrderedDict</code>, optimized weights.</p>"},{"location":"documentation/api/strategy/portfolio_construction/optimization/#example-optimizer","title":"Example Optimizer","text":"<p>Example Optimizer<p>Portfolio construction, which involves optimizing the allocation of assets within a portfolio, can be a complex and nuanced process. We have developed a method that allows for greater flexibility and experimentation in the portfolio optimization process. This approach enables the exploration of a wide range of potential portfolio compositions, and the example provided illustrates this method applied from the initial stages of portfolio construction:</p> <ul> <li>A mapping table, <code>mapping_table</code>, has been defined to specify the group membership of each investable stocks</li> <li>The base model is set <code>mvo</code>, the Mean-Variance Optimization from the pypfopt library, with the appropriate return and risk models</li> <li>The weight bounds parameter, <code>weight_bounds</code> is set to <code>(-1, 1)</code>, which serves as the first constraint by limiting the minimum and maximum weight of each asset in portfolios that allow short positions</li> <li>Additionally, new constraints are introduced to the optimization problem in the form of convex inequalities, which ensure that long positions do not exceed 10% and short positions do not fall below -10%</li> </ul> <pre><code>from opendesk import Strategy\nstrategy = Strategy(\nsteps=steps, \ntopdown=True,\nmapping_table=mapping_table # (3)\n)\nstrategy.fit(sector_prices).estimate(sum) # (1)\nweights = strategy.portfolio(stock_prices).optimize( # (2)\ntarget=\"min_volatility\"\nweight_bounds=(-1, 1) # (4)\nconstraints=[\nlambda w: w &lt;=  .1, \nlambda w: w &gt;= -.1\n] # (5)\n)\n</code></pre> <ol> <li>pandas.DataFrame object, with specifiy the variation of sector returns over time.</li> <li>pandas.DataFrame object, with specifiy the variation of stock prices over time.</li> <li>Mapping table where stock ticker/id are keys and sector name are values.</li> <li><code>weight_bounds</code> parameter serves as a constraint by limiting the minimum and maximum weight of each asset in portfolios. Because it ranges from <code>-1</code> to <code>1</code>, it allows Long and Shorts.</li> <li>Users can add new constraints in a form of lambda function as the user need to the optimization problem. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</li> </ol> </p>"},{"location":"documentation/api/synthetic/","title":"Synthetic","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/synthetic/continuous_time_processes/","title":"Continuous Processes","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/synthetic/diffusion_models/","title":"Diffusion Models","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/synthetic/discrete_time_processes/","title":"Discrete Processes","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/synthetic/noise_processes/","title":"Noise Processes","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/api/synthetic/random_number_generation/","title":"Random Generator","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/contributing/","title":"Developement Guildeline","text":""},{"location":"documentation/contributing/#philosophy","title":"Philosophy","text":"<p>The engineering and research process in the context of investment strategies involves several key stages.</p> <ol> <li>Research phase: In this phase, the focus is on gathering and analyzing data to inform the development of a quantitative model or algorithm. This involves collecting data from various sources, such as financial statements, market trends, and economic indicators, and using statistical and mathematical techniques to analyze and interpret the data.</li> <li>Strategy implementation: Once the research phase is complete, the next step is to implement the quantitative model or algorithm. </li> <li>Model Optimization: After the initial implementation of the quantitative model, it is important to continuously review and optimize it to ensure that it is accurately predicting outcomes and providing valuable insights. This may involve  adjusting the model's parameters or adding additional data sources to improve its performance.</li> <li>Backtesting: Before implementing a quantitative model in a live trading environment, it is often useful to backtest it to see how it would have performed in the past. This can help to identify any potential weaknesses or areas for improvement in the model.</li> <li>Target and invested portfolio: In the context of quantitative analysis, the target portfolio refers to the desired mix of investments that the model is intended to identify and select, while the invested portfolio refers to the actual mix of investments that has been chosen based on the model's predictions.</li> <li>Risk monitoring: It is important to continuously monitor the portfolio for any potential risks and to take appropriate action to mitigate those risks. This involves regularly reviewing the portfolio and making adjustments as needed, as well as staying informed about market trends and economic conditions.</li> </ol> <pre><code>sequenceDiagram\n  Database-&gt;&gt;Blocks: Feed Strategy\n  Blocks-&gt;&gt;Portfolio: Compute Exposures\n  Portfolio-&gt;&gt;Backtest: Weights\n  loop Objectives &amp; Constraints\n        Backtest-&gt;&gt;Portfolio: Performance\n    end</code></pre> <p>Overall, the engineering and research process in the context of quantitative analysis involves a systematic approach to collecting and analyzing data, developing and implementing a quantitative model or algorithm, and continuously reviewing and optimizing it to ensure that it is providing accurate and valuable insights.</p>"},{"location":"documentation/contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"documentation/contributing/#abstract-classes-abcs","title":"Abstract Classes (ABCs)","text":"<p>An abstract class can be considered as a blueprint for other classes. It allows you to create a set of methods that must be created within any child classes built from the abstract class. </p> <ul> <li>A class which contains one or more abstract methods is called an abstract class. </li> <li>An abstract method is a method that has a declaration but does not have an implementation. </li> <li>While we are designing large functional units we use an abstract class.</li> <li>When we want to provide a common interface for different implementations of a component, we use an abstract class.\u00a0</li> </ul>"},{"location":"documentation/contributing/#why-are-abcs-useful","title":"Why are ABCs useful?","text":"<p>By defining an abstract base class, you can define a common Application Program Interface(API) for a set of subclasses. This capability is especially useful in situations where a third-party is going to provide implementations, such as with plugins, but can also help you when working in a large team or with a large code-base where keeping all classes in your mind is difficult or not possible.</p>"},{"location":"documentation/contributing/#how-abcs-work","title":"How ABCs work?","text":"<p>ABC</p> <p>Find more information about abc \u2014 Abstract Base Classes.</p> <p>By default, Python does not provide abstract classes. Python comes with a module that provides the base for defining Abstract Base classes (ABC) and that module name is <code>ABC</code>. <code>ABC</code> works by decorating methods of the base class as abstract and then registering concrete classes as implementations of the abstract base. A method becomes abstract when decorated with the keyword <code>@abstractmethod</code>. </p>"},{"location":"documentation/contributing/#base-score-model","title":"Base Score Model","text":"<p>In order to ensure consistency and ease of use, all blocks must adhere to the <code>BaseScoreModel</code> abstract base class. </p> <pre><code>from typing import Dict\nimport pandas as pd\nfrom abc import ABC, abstractmethod\nclass BaseScoreModel(ABC):\n@abstractmethod\ndef processing(\nself, X: pd.DataFrame, **kwargs) -&gt; \"BaseScoreModel\":\nself.processing_output: pd.DataFrame\nreturn self\n@abstractmethod\ndef ranking(self) -&gt; \"BaseScoreModel\":\nself.ranking_output: pd.DataFrame\nreturn self\n@abstractmethod\ndef scoring(self) -&gt; \"BaseScoreModel\":\nself.scoring_output: pd.DataFrame\nself.strategy_output: Dict\nreturn self\n</code></pre> <p>This set of rules allows us to seamlessly integrate blocks and reduces friction. As a result, the code becomes more readable, easier to debug, and simpler to contribute to. It also significantly decreases the amount of code required to access information.</p>"},{"location":"documentation/contributing/#step-1-inheritance","title":"Step 1: Inheritance","text":"<p>Blocks must inherite from <code>BaseScoreModel</code>:</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\n...\n</code></pre>"},{"location":"documentation/contributing/#step-2-initialize-parameters","title":"Step 2: Initialize Parameters","text":"<p>Parameters should be initialized at the <code>__init__</code> section of the class. It also means that model data should NOT be passed at this stage (see step 5 below):</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\n</code></pre>"},{"location":"documentation/contributing/#step-3-prs","title":"Step 3: PRS","text":"<p>Blocks must follow the \"PRS\" schema, which is \"Processing\" data, \"Ranking\" results, \"Scoring\" output. All classes must have the mandatory three functions:</p> <ul> <li><code>processing()</code></li> <li><code>ranking()</code></li> <li><code>scoring()</code></li> </ul> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self)\n...\ndef ranking(self)\n...\ndef scoring(self)\n...\n</code></pre>"},{"location":"documentation/contributing/#step-4-fluent-interface","title":"Step 4: Fluent Interface","text":"<p>Each of the above three functions must return the instance object <code>self</code> on which the method was called:</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self) -&gt; \"MyBlock\":\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nreturn self\n</code></pre> <p>Fluent interface</p> <p>In object-oriented programming, returning self from a method allows for the implementation of a fluent interface, where methods can be cascaded i a single statement.</p> <p>In object-oriented programming, returning <code>self</code> from a method can be useful for several reasons. One common use case is to create a fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This can make the code more readable and concise, as it eliminates the need to create intermediate variables to store the results of intermediate method calls. For example, with a fluent interface, this code could be written as <code>results = SomeClass().method1().method2().method3()</code>. </p>"},{"location":"documentation/contributing/#step-5-processing-parameters","title":"Step 5: Processing Parameters","text":"<p>The function <code>processing()</code> must accept a market price <code>pandas.DataFrame</code> time-series object and can accept additional dataset (within <code>**kwargs</code>):</p> <pre><code>import pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nreturn self\n</code></pre>"},{"location":"documentation/contributing/#step-6-set-attributes","title":"Step 6: Set Attributes","text":"<p>Functions <code>processing()</code>, <code>ranking()</code> and <code>scoring()</code> must set attributes <code>processing_output</code>, <code>ranking_output</code> and <code>scoring_output</code> as <code>pandas.DataFrame</code> object, respectively:</p> <pre><code>import pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nself.processing_output: pd.DataFrame\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output: pd.DataFrame\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nself.scoring_output: pd.DataFrame\nreturn self\n</code></pre>"},{"location":"documentation/contributing/#step-7-strategy-output","title":"Step 7: Strategy Output","text":"<p>The function <code>scoring()</code> must set attribute <code>strategy_output</code> as dictionnary object, representing the final scores of the model:</p> <pre><code>from typing import Dict\nimport pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nself.processing_output: pd.DataFrame\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output: pd.DataFrame\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nself.scoring_output: pd.DataFrame\nself.strategy_output: Dict\nreturn self\n</code></pre>"},{"location":"documentation/workflow/","title":"Workflow","text":""},{"location":"documentation/workflow/#aws-cloudformation","title":"AWS CloudFormation","text":"<p>What is AWS CloudFormation?</p> <p>Documentation available on What is AWS CloudFormation? - AWS CloudFormation.</p> <p>Opendesk is organised the following way:</p> <pre><code>flowchart TD\n    A[(Data Provider)]--&gt;|File Upload|B[(AWS S3)]\n    subgraph AWS [AWS]\n    C[AWS Lambda]-.-&gt;|Trigger|B\n    C-.-&gt;|Metadata|D[AWS DynamoDB]\n    C-.-&gt;|Extract File|B\n    end\n    B--&gt;|Online Access|E[Terminal]</code></pre> <p>AWS CloudFormation is a service that provides a model-based approach to provisioning and managing AWS resources. It allows you to create templates that define the resources you need, such as Amazon EC2 instances or Amazon RDS DB instances, and CloudFormation takes care of provisioning and configuring those resources automatically. </p> <pre><code>sequenceDiagram\n    Data Provider-&gt;&gt;AWS S3: File Upload\n    AWS S3--&gt;&gt;AWS Lambda: Trigger Lambda Function\n    AWS Lambda--&gt;&gt;AWS DynamoDB: Metadata Store\n    AWS Lambda--&gt;&gt;AWS S3: Extract File Uploaded\n    AWS S3--&gt;&gt;AWS Lambda: Trigger Lambda Function\n    AWS Lambda--&gt;&gt;AWS S3: HTML Upload\n    AWS S3-&gt;&gt;Terminal: Online Access</code></pre> <p>This eliminates the need for manual resource creation and configuration, and also helps to manage the dependencies between resources. As a result, you can focus more on developing your applications that run on AWS, rather than spending time on resource management. The above scenarios illustrate how CloudFormation can be used to streamline the resource management process.</p>"},{"location":"documentation/workflow/#learn-more","title":"Learn More...","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/workflow/data/","title":"Data","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/workflow/extract/","title":"Extract","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/workflow/load/","title":"Load","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/workflow/scheduling/","title":"Scheduling","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"documentation/workflow/transform/","title":"Transform","text":"<p>Should you be interested in our approach and latest research on quantitative analysis, please feel free to contact us to obtain more detailed information about the PRO version of the package via LinkedIn.</p> <p>LinkedIn</p>"},{"location":"getting_started/","title":"Getting Started","text":"<p>This Get Started guide explains how <code>opendesk</code> works, how to install <code>opendesk</code> on your preferred operating system, and how to create your first <code>opendesk</code> strategy! </p> <p></p> <p>Main Concepts introduces you to <code>opendesk</code>'s base model and development flow. You'll learn what makes <code>opendesk</code> the most powerful way to build systematic strategies, including the ability to customize existing setup. </p> <p></p> <p>Create Blocks following the three-step process outlined in <code>opendesk</code>'s core guidelines: process raw data, rank and compare these results, create and map a sequence of scores.</p> <p></p> <p>Optimize Blocks, which is the process of selecting the optimal mix of assets in a portfolio, with respect to the alpha scores, in order to maximize returns while minimizing risk. This method has been implemented to streamline the process of optimization and facilitate the integration of backtesting.</p> <p></p> <p>Backtest Blocks through the <code>opendesk</code> API. It enables the systematic evaluation of one or multiple strategies simultaneously. It provides access to a comprehensive range of features, thereby affording users the ability to leverage its full functionality and holds significant potential for success within the financial markets.</p> <p></p> <p>Installation helps you set up your virtual environment and walks you through installing <code>opendesk</code> on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.</p>"},{"location":"getting_started/backtest_blocks/","title":"Backtest Blocks","text":"<p>Backtesting is a complex process that requires a deep understanding of investment concepts and programming. It involves testing a trading strategy using historical data to simulate transactions and evaluate the strategy's performance. </p> <p>Building a trading backtester involves gathering historical market data, defining trading rules, creating algorithms to execute these rules, avoiding data leaks, and generating performance results that take into account transaction costs.</p>"},{"location":"getting_started/backtest_blocks/#algorithmic-trading","title":"Algorithmic Trading","text":"<p>Our trading algorithms, which execute trading rules, are developed in Python. Defining the trading rules is a key element in setting up a trading backtester. The rules can also include macro or fundamental events such as earnings announcements, interest rate changes, or changes in economic policy.</p>"},{"location":"getting_started/backtest_blocks/#performance-measurement","title":"Performance Measurement","text":"<p>Path-Dependent Backtesting</p> <p>Path-dependent portfolio rebalancing refers to a portfolio rebalancing strategy in which the decision to rebalance the portfolio is dependent on the historical path of the portfolio's performance. Instead of rebalancing the portfolio at fixed intervals or based on predefined rules, the rebalancing decision is based on the path the portfolio has taken in the past. This approach takes into account the dynamic nature of financial markets and the nonlinearities in portfolio returns, and aims to improve the portfolio's risk-return tradeoff by adjusting the portfolio allocation in response to market conditions. In backtesting, it can be simulated by using historical data to test the performance of different rebalancing strategies based on the portfolio's path.</p> <p>Commonly used performance measures include return rate, Sharpe ratio, and maximum drawdown, among others. These measures allow for the evaluation of trading strategy performance and path-dependent optimization based on generated performance.</p>"},{"location":"getting_started/backtest_blocks/#open-source-resources","title":"Open-Source Resources","text":"<p>This platform aggregates a range of state-of-the-art tools and libraries and facilitates their automation through a unified interface. This allows users to easily access and utilize a wide range of advanced resources for their research or development efforts, streamlining the process of utilizing cutting-edge technologies.</p> <p>There is a wide range of options available for Python when it comes to selecting a backtesting framework. A comprehensive list of these libraries can be found on the GitHub page titled awesome-quant. We initially experimented with one called Backtesting and then subsequently tested a few others. For the purpose of backtesting, vectorbt can be considered as a suitable initial choice for integrating our model pipeline due to its fast performance and ease of use. It also actively managed and a more advance version has been created: vectorbt pro.</p>"},{"location":"getting_started/backtest_blocks/#integration","title":"Integration","text":"<p>vectorbt is a Python package for quantitative analysis that takes a novel approach to backtesting: it operates entirely on pandas and NumPy objects, and is accelerated by Numba to analyze any data at speed and scale. This allows for testing of many thousands of strategies in seconds.</p> <p>Vectorbt is a powerful tool that combines the functionality of a fast backtester with advanced data analysis capabilities. It enables users to analyze and evaluate a wide range of trading options, instruments, and time periods with ease, enabling them to identify patterns and optimize their strategy. It allows users to explore and understand complex phenomena in trading data, providing them with valuable insights that can inform their decision-making and potentially give them an informational advantage in the market.</p> <p>It utilizes a vectorized representation of trading strategies in order to optimize performance. This representation involves storing multiple strategy instances in a single multi-dimensional array, as opposed to the traditional object-oriented programming (OOP) approach of representing strategies as classes and data structures. The vectorized representation used by vectorbt allows for more efficient processing and comparison of strategies, and can particularly improve the speed of analysis when dealing with quadratic programming problems.</p> <p>Numba is a just-in-time (JIT) compiler for Python that is designed to improve the performance of Python code. It does this by compiling Python code to native machine instructions, which can be executed much faster than the interpreted code that is normally used in Python. Numba works by decorating functions or methods with a special <code>@jit</code> decorator, which tells the Numba compiler to compile the function for faster execution. Numba can be used to speed up code that makes heavy use of Python's scientific and numerical libraries, such as NumPy and SciPy, as well as code that performs CPU-bound operations. In addition to its JIT compiler, Numba also provides support for parallel programming through its <code>@vectorize</code> and <code>@guvectorize</code> decorators, which can be used to parallelize the execution of certain types of functions.</p>"},{"location":"getting_started/backtest_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>The <code>backtest</code> classmethod within the <code>Strategy</code> class utilizes the vectorbt library to test and evaluate the performance of a given strategy in the context of alpha blocks methodology.</p> <p>In the field of software engineering, it is recommended that a function be designed in a manner that promotes testability by adhering to the principles of cohesion. Specifically, this entails limiting the function's scope to a single, well-defined task, as well as minimizing the number of arguments it accepts. </p> <p>In cases where a function is required to operate on multiple arguments, it is suggested to employ techniques such as encapsulation by utilizing higher-level objects to group the arguments together. This allows for more robust and maintainable codebase.</p> <p>This is why we created <code>BacktestConfig</code>, a dataclass object, which encapsultes required arguments to create exposure and build portfolio at each rebalancing period.</p>"},{"location":"getting_started/backtest_blocks/#step-1-backtestconfig","title":"Step 1: BacktestConfig","text":"<p>The <code>backtest</code> method is initalized through the <code>BacktestConfig</code>, which facilitates feature integration. For example, variables <code>universe</code>, <code>model_data</code>, <code>steps</code>, <code>topdown</code> and <code>mapping_table</code> are set to match your requirement. All other variables are pre-set.</p> <pre><code>from opendesk.backtest import BacktestConfig\nconfig = BacktestConfig(\nsteps=[(\n\"my_block\", \nMyBlock, \n{\"mapping_score\": mapping_score}, \n{\"price_earnings\": price_earnings}\n)],     \nuniverse=stock_prices, \nmodel_data=model_data, \ntopdown=True, \nmapping_table=mapping_table,\nportfolio='optimize',\nbacktest_backup=\"discrete_allocation\"\n)\n</code></pre> <p>The backtest implementation initializes, fits and estimates exposures using the <code>fit()</code> and the <code>estimate()</code> methods. Then, it optimizes the portfolio at the stock level using the <code>portfolio()</code> and <code>optimize()</code> methods and finds weights that align with the desired level of risk. </p> <p>The strategy is rebalanced on a monthly basis, and the <code>discrete_allocation()</code> method is used as a fallback in the event that the optimizer is unable to deliver feasible weights.</p>"},{"location":"getting_started/backtest_blocks/#step-2-backtest","title":"Step 2: Backtest","text":"<p>The output is a <code>Portfolio</code> object, which allows users to leverage the entire vectorbtpro ecosystem.</p> <pre><code>backtest = Strategy.backtest(config)\nbacktest.plot_trade_signals().show()\n</code></pre> <p> </p> Figure 1: Example Backtest"},{"location":"getting_started/create_blocks/","title":"Create Blocks","text":"<p>Blocks are classes, which follow <code>opendesk</code> Development Guidelines, to ensure consistency and ease of use. Blocks must follow the \"PRS\" schema, which is \"Processing\" data, \"Ranking\" results, \"Scoring\" output. All classes must have the below three functions:</p> <ul> <li><code>processing()</code></li> <li><code>ranking()</code></li> <li><code>scoring()</code></li> </ul>"},{"location":"getting_started/create_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Dependencies</p> <p>For this step-by-step example, we are using the following dependencies: <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n</code></pre></p> <p>In this example, we want to create a valuation level strategy. This rule-based strategy implies exposing our portfolio to directional bets, depending on both the level of the price-earning ratio (PE) time-series and the cross-sectional comparison between assets.</p> <ol> <li>In the processing function, we will standardize features by removing the mean and scaling to unit variance. The standard score, \\(Z\\), of a sample \\(x\\) is calculated as \\(Z = \\frac{(x - \\mu)}{\\sigma}\\), where \\(\\mu\\) is the mean of the training samples and \\(\\sigma\\) is the standard deviation of the training samples.</li> <li>In the ranking function, we will rank in ascending order \\(PEs\\).</li> <li>In the scoring function, we will map top quantiles from both long and short sides. </li> </ol>"},{"location":"getting_started/create_blocks/#step-1-processing","title":"Step 1: Processing","text":"<p>To process data, we use the Scikit-Learn <code>StdandardScaler</code> transform. In practice, normalizing our time-series is as follow:</p> <pre><code>def processing(price_earnings: pd.DataFrame):\nprocessing_output = (\nStandardScaler()\n.set_output(transform=\"pandas\")\n.fit_transform(price_earnings)\n)\nreturn processing_output\n</code></pre> <p>Which returns a <code>pandas.DataFrame</code> object.</p>"},{"location":"getting_started/create_blocks/#step-2-ranking","title":"Step 2: Ranking","text":"<p>Then, the transformed results is ranked using <code>pandas.rank()</code> function:</p> <pre><code>def ranking():\nranking_output = (\nprocessing_output\n.iloc[-1]\n.rename(\"rank\")\n.rank(ascending=False)\n.sort_values()\n)\nreturn ranking_output\n</code></pre> <p>Which also returns a <code>pandas.DataFrame</code> object.</p>"},{"location":"getting_started/create_blocks/#step-3-scoring","title":"Step 3: Scoring","text":"<p>Finally, we produce the goal of this base score model is to produce quantile scores, as follow:</p> <pre><code>def scoring():\nquantiles = pd.qcut(ranking_output, q=5, labels=False)\nscoring_output = quantiles.map(mapping_score)\nreturn strategy_output\n</code></pre>"},{"location":"getting_started/create_blocks/#step-4-parameters","title":"Step 4: Parameters","text":"<p>Last, for reproductibility and ease, we aim to gather parameters in the <code>__init__</code> section:</p> <pre><code>from typing import Dict\nfrom opendesk.blueprints import BaseScoreModel\nclass MyBlock(BaseScoreModel):\ndef __init__(self, mapping_score: Dict)\nself.mapping_score = mapping_score\n</code></pre>"},{"location":"getting_started/create_blocks/#step-5-wrapping-things-up","title":"Step 5: Wrapping Things Up","text":"<pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom opendesk.blueprints import BaseScoreModel\nclass MyBlock(BaseScoreModel):\ndef __init__(self, mapping_score: Dict)\nself.mapping_score = mapping_score\ndef processing(self, price_earnings: pd.DataFrame) -&gt; \"MyBlock\":\nself.processing_output = (\nStandardScaler()\n.set_output(transform=\"pandas\")\n.fit_transform(price_earnings)\n)\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output = (\nprocessing_output\n.iloc[-1]\n.rename(\"rank\")\n.rank(ascending=False)\n.sort_values()\n)\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nquantiles = pd.qcut(ranking_output, q=5, labels=False)\nself.scoring_output = quantiles.map(mapping_score)\nself.strategy_output = scoring_output.to_dict()\nreturn self\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, you're going to need a few things:</p> <ul> <li>Your favorite IDE or text editor</li> <li>Python 3.10</li> <li>PIP</li> <li>Install C++:<ul> <li>MacOS: You need to install XCode Command Line Tools. They are required to compile some of Opendesk's Python dependencies during installation.</li> <li>Windows: For Windows users, download Visual Studio, with additional instructions.</li> </ul> </li> </ul> <p>Python</p> <p>Opendesk is built on Python. Other dependencies, such as C++, are employed to improve the speed at which the program runs.</p>"},{"location":"getting_started/installation/#set-up-your-virtual-environment","title":"Set up your virtual environment","text":"<p>Regardless of which package management tool you're using, we recommend running the commands on this page in a virtual environment. This ensures that the dependencies pulled in for Opendesk don't impact any other Python projects you're working on.</p> <p>Below are a few tools you can use for environment management:</p> <ul> <li>pipenv</li> <li>poetry</li> <li>venv</li> <li>virtualenv</li> <li>conda</li> </ul>"},{"location":"getting_started/installation/#install-opendesk","title":"Install Opendesk","text":"<p>Troubleshooting</p> <p>If any of these methods don\u2019t work, please raise an issue with the \u2018packaging\u2019 label on GitHub.</p> <p>prior to using the below commands, you may need to follow the installation instructions for cvxopt and cvxpy).</p>"},{"location":"getting_started/installation/#with-pip","title":"With Pip recommended","text":"<p>Installation can be done via pip:</p> <pre><code>$ pip install opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-poetry","title":"With Poetry recommended","text":"<p>Poetry is a relatively new kid on the block but has gained traction due to its ease of use and how it resolves several issues over Conda.</p> <p>Poetry will set up your virtual environment and install all required packages with one command <code>poetry install</code>. In addition, it can build installable packages with <code>poetry build</code>, which can be installed by pip. It uses the PEP Standard pyproject.toml file to define dependencies and build options for other tools (e.g. pytest arguments).</p> <p>It respects <code>.python_version</code> files, uses a lock file to define specific versions of packages and pip to install them - thus, you have reproducible environments installed quickly.</p> <p>The only downside is the slight learning curve in changing your workflow.</p> <pre><code>$ poetry add opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-conda","title":"With Conda","text":"<p>If you don't have Anaconda install yet, follow the steps provided on the Anaconda installation page.</p> <pre><code>$ conda install opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-git","title":"With Git","text":"<p>The alternative is to clone the project:</p> <pre><code>$ git clone https://github.com/ActurialCapital/opendesk.git\n$ cd myproject\n$ python setup.py install\n</code></pre>"},{"location":"getting_started/installation/#further-reading","title":"Further Reading","text":"<p>A pretty good guide/supporting argument to the recommended setup - My Python Development Environment, 2020 Edition - Jacob Kaplan-Moss.</p>"},{"location":"getting_started/main_concepts/","title":"Main Concepts","text":"<p>Using <code>opendesk</code> is easy. First, come up with an idea and make sure it can be organized in a clear and concise way. Then, build one or more \"blocks\" that bring this idea to life, following <code>opendesk</code> Development Guidelines. Finally, create a portfolio and test it out through backtesting to see how it performs.</p> <p>Building blocks is a methodology which involves a sequential application of multiple, modular strategies. These blocks are rule-based and can be easily modified, added, or removed from the sequence. Each block returns a score, and the overall approach is designed to facilitate the integration of research and the efficient testing and optimization of strategies and machine learning overlay, while also allowing for the risk-efficient combination of different sources of alpha. The building blocks approach is a comprehensive yet extensible method for data analysis and financial strategy development. </p>"},{"location":"getting_started/main_concepts/#features","title":"Features","text":"<ul> <li> Portfolio strategy first: <code>opendesk</code> focuses on portfolio strategy development</li> <li> Fast: <code>opendesk</code> system has been designed to achieve superior performance in the development, implementation, and evaluation of strategies, built with NumPy, Pandas and Polars</li> <li> Fast to code: Created through fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This make <code>opendesk</code>'s code more readable and concise, as it eliminates the need to create intermediate variables to store the results of temporary method calls</li> <li> Intuitive: Develop a plan, organize your ideas, conduct research, implement strategies based on the findings, and test various parameters to determine their impact in a single session</li> <li> Easy: <code>opendesk</code> user interface has been optimized for simplicity and ease of use, reducing the need for extensive training or consultation of documentation</li> <li> <p> Batteries included: No dependencies needed:</p> <ul> <li>Quantitative data pre-processing</li> <li>ML and scientific computing</li> <li>Large and diversified model library</li> <li>Flexible portfolio construction</li> <li>Fast backtesting capabilities</li> <li>AWS CloudFormation service integrated</li> <li>FastAPI web framework built-in</li> <li>LLM integration  </li> </ul> </li> </ul> <p>Our platform integrates a diverse set of state-of-the-art tools and libraries, including those for machine learning, data analysis, and scientific computing. These resources are carefully curated to ensure that they represent the most advanced and effective technologies currently available.</p>"},{"location":"getting_started/main_concepts/#blocks","title":"Blocks","text":"<p>The block sequence is a modular approach to creating investment strategies that involves breaking down the code pipeline into individual units, or \"blocks,\" that can be easily replaced or removed. Each block is designed to function as a rule-based strategy, contributing to the overall views and confidence assigned to a particular asset. </p> <p>This approach, which was first proposed by Kowalski<sup>1</sup> (1979), involves arranging blocks of code in a specific sequence in order to generate an output.The modular design of building sequences allows for easy expansion and integration of alpha sources. This approach has already been well-established in computer science and software engineering (Bass, Clements, &amp; Kazman, 2003<sup>2</sup>). It is escpecially powerful for integrating research, as it allows for the combination of alpha sources in a risk-sensitive manner and leads towards efficient backtesting and optimization.</p> <p>Did you know?</p> <p>There are several scientific references that support the use of modular approaches and building blocks in investment strategy development. For example, in their paper \"Modular Investment Strategies,\" published in the Journal of Financial Economics<sup>3</sup>, K. G. Rouwenhorst and G. S. Wu discuss the benefits of using a modular approach to constructing investment strategies. </p> <p>They argue that this approach allows for better risk management, as it allows for the independent testing and evaluation of each component of the strategy. Another scientific reference that supports the use of building blocks in investment strategy development is the paper \"Building Investment Strategies with Building Blocks,\" published in the Journal of Portfolio Management<sup>4</sup>. In this paper, the authors discuss the benefits of using a building block approach, including the ability to easily incorporate new research and the flexibility to adjust the strategy as market conditions change.</p>"},{"location":"getting_started/main_concepts/#scores","title":"Scores","text":"<p>Scores are numerical values that are used to evaluate or rank various options or candidates. In order to generalize the results, a score is generated for each block of data, which serves as a signal to adjust the level of exposure to a particular asset or group of assets:</p> <ul> <li>Scores allow for flexibility: By calculating scores, you can easily adjust the relative importance of different factors and criteria, or even add or remove factors as needed. This flexibility can be useful when an your goals or circumstances change.</li> <li> <p>Scores can be more easily customized: Scores can be customized to reflect your specific goals, risk tolerance, and other factors. This can make it easier for you to create portfolios that are tailored to your individual needs and preferences.</p> </li> <li> <p>Scores can help to incorporate subjective factors: Some investment decisions involve subjective judgment, such as whether a company's management team is strong or whether a particular market is likely to experience significant growth in the near future. Scores can be used to incorporate these subjective factors in a systematic way.</p> </li> <li> <p>Scores can be used to compare investments on a common scale: By assigning scores to different investments, you can more easily compare them to one another on a common scale. This can make it easier to identify the most attractive candidates for inclusion in a portfolio.</p> </li> <li> <p>Scores extend optimization processes: The optimization process can use these scores to select the investment portfolio that maximizes returns while minimizing risk, subject to any constraints or objectives that have been defined.</p> </li> </ul>"},{"location":"getting_started/main_concepts/#model-design","title":"Model Design","text":"<p>The building blocks approach involves sequentially executing multiple steps on a dataset, allowing for the modification of steps, step parameters, and the order in which they are executed. Each block represents a modular, rule-based strategy that can be removed or replaced in the sequence. These blocks return a score for each feature. This approach is designed to facilitate the integration of alpha sources, while also being easily extensible.</p> <pre><code>graph LR\n  id1[(Database)] --&gt; B[Strategy];\n  B --&gt; C[Block A] &amp; D[Block B] &amp; E[Block k];\n  C --&gt; I[Exposures];\n  D --&gt; I;\n  E --&gt; I;</code></pre>"},{"location":"getting_started/main_concepts/#speed","title":"Speed","text":""},{"location":"getting_started/main_concepts/#multiprocessing","title":"Multiprocessing","text":"<p>Using multiprocessing can be an efficient way to integrate the blocks. Multiprocessing refers to the ability of a computer to execute multiple processes concurrently, which can be useful for completing tasks more quickly. By utilizing multiprocessing to integrate each individual units, it is possible to speed up the process of constructing code pipelines and evaluating the performance of investment strategies or models.</p> <pre><code>graph LR\n  id1[(Database)] --&gt; B[Strategy];\n  B --&gt; C[Block A] &amp; D[Block B] &amp; E[Block k];\n  subgraph Multiprocessing\n  C --- F[/Score A/];\n  D --- G[/Score B/];\n  E --- H[/Score k/];\n  end\n  F --&gt; I[Exposures];\n  G --&gt; I;\n  H --&gt;I;</code></pre> AdvantagesLimitations <ol> <li>Improved performance: Multiprocessing can improve the performance of your program by allowing it to utilize multiple CPU cores, which can speed up the execution of CPU-bound and multithreaded programs.</li> <li>Better utilization of resources: Multiprocessing allows you to make use of all available CPU cores, which can be especially useful when running programs on a machine with multiple cores or on a multi-core server.</li> <li>Easier parallelization: Python's joblib module provides a high-level interface for creating and managing processes, making it easier to write parallelized programs. Joblib provides a  simple helper class to write parallel for loops using multiprocessing. The core idea is to write the code to be executed as a generator expression, and convert it to parallel computing.</li> </ol> <p>However, it is important to note that the efficiency of using multiprocessing will depend on the specific characteristics of the blocks and the hardware being used. It may be necessary to carefully analyze the performance of the code and the available resources in order to determine the most appropriate approach for integrating the blocks in the sequence.</p> <p>In the above example, 5 blocks (processes) were executed concurrently using backend <code>LokyBackend</code> with 8 concurrent workers<sup>5</sup>, each operating on a dataset of 1000 samples times 20 sectors.</p> <p>The elapsed time for this operation was 1.2 seconds.</p> <pre><code>$ strategy.fit(df).estimate(sum)\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: grey;\"&gt;Done 1 tasks&lt;/span&gt;\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: grey;\"&gt;Done 2 out of 5&lt;/span&gt;    \n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: grey;\"&gt;Done 3 out of 5&lt;/span&gt;\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: grey;\"&gt;Done 4 out of 5&lt;/span&gt; \n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: grey;\"&gt;Done 5 out of 5 | elapsed: 1.2s finished&lt;/span&gt;\n</code></pre>"},{"location":"getting_started/main_concepts/#polars","title":"Polars","text":"<p>In some instances, we use polars, a lightning-fast DataFrame library for Rust and Python. Polars is written in Rust, uncompromising in its choices to provide a feature-complete DataFrame API to the Rust ecosystem. We use it as a DataFrame library for your data models. Polars is built upon the safe Arrow2 implementation of the Apache Arrow specification, enabling efficient resource use and processing performance. By doing so it also integrates seamlessly with other tools in the Arrow ecosystem.</p> <p>For example, the <code>apply</code> function is used to aggregate scores from different blocks. Here, we tested 10000 rows x 10 columns:</p> <p>In pandas: <pre><code>df.apply(sum, axis=1).rename(\"sum\")\n</code></pre> Time to apply with pandas: <code>3.211 seconds</code></p> <p>In polars: <pre><code>import polars as pl\npd.Series(\nlist(pl.DataFrame(df).with_column(\npl.fold(0, lambda acc, s: acc + s, pl.all()).alias(\"sum\")\n)[:,-1])\n)\n</code></pre> Time to apply with polars: <code>1.012 seconds</code></p> <p>Both function return a <code>pandas.Series</code> object, however, Polars is 3.17x faster than pandas at <code>apply</code>. In a near future, if time complexity becomes an issue, we might turn the entire code base to polars.</p>"},{"location":"getting_started/main_concepts/#step-by-step-example","title":"Step-by-Step Example","text":"<p>As an example, we design the following strategy:</p> <ul> <li>Long-Short Equity</li> <li>Top-Down (from sector to stock level)</li> <li>Risk optimized (8% target volatility)</li> <li>Market neutral</li> <li>Sentiment-based, which integrates both sector momentum reversion and trend following techniques</li> </ul>"},{"location":"getting_started/main_concepts/#step-1-blocks","title":"Step 1: Blocks","text":"<p>To calculate scores, we follow a set of predefined rules that compute the deviation between long-term and short-term sector rankings. This generates a composite factor that averages the relative price return momentum within each sector.</p> <p>The Strategy class is initialized by defining the steps as outlined above and setting the <code>topdown</code> parameter to <code>True</code>. </p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, TrendFollowing\nstrategy = Strategy(\nsteps = [\n(\"reversion\", Reversion),\n(\"trend_following\", TrendFollowing)\n],\ntopdown=True, \nmapping_table=mapping_table\n)\n</code></pre> <p>Mapping Table</p> <p>Because we are implementing a top-down strategy (parameter <code>topdown=True</code>), the <code>mapping_table</code> is also passed. The mapping table is a dictionnary, which associates stocks with sectors, as follow:</p> <p> <pre><code>$ pd.Series(mapping_table)\n&lt;span style=\"color: grey;\"&gt;stock 1       sector 1\nstock 2       sector 2\nstock 3       sector 3\nstock 4       sector 4\nstock 5       sector 5\n                ...  \nstock 96      sector 6\nstock 97      sector 7\nstock 98      sector 8\nstock 99      sector 9\nstock 100     sector 10\nLength: 100, dtype: object\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"getting_started/main_concepts/#step-2-exposures","title":"Step 2: Exposures","text":"<p>The returns time-series (at the sector level) are fit using the <code>fit()</code> method, and the portfolio exposures (tilts) are estimated through <code>estimate()</code> by summing the individual scores:</p> <pre><code>$ strategy.fit(model_data).estimate(sum)\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: grey;\"&gt;1 tasks&lt;/span&gt;\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: grey;\"&gt;Done 2 out of 2 | elapsed: 0.5s&lt;/span&gt;\n&lt;span style=\"color: grey;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: grey;\"&gt;Done 2 out of 2 | elapsed: 0.5s finished&lt;/span&gt;\n</code></pre> <p>A summary breakdown and final tilts can be shown using the <code>breakdown</code> and <code>exposure</code> attributes, respectively:</p> Breakdown Exposures <p> <pre><code>$ strategy.breakdown\n&lt;span style=\"color: grey;\"&gt;           Trendfollowing  Reversion\nsector 1                1          1\nsector 2                0          1\nsector 3                0          0\nsector 4               -2          0\nsector 5               -1         -1\nsector 6                2         -2\nsector 7                2         -1\nsector 8               -2          0\nsector 9               -1          0\nsector 10               1          2\nName: sum, dtype: int64 \n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.exposures\n&lt;span style=\"color: grey;\"&gt;sector 1     2\nsector 2     1\nsector 3     0\nsector 4    -2\nsector 5    -2\nsector 6     0\nsector 7     1\nsector 8    -2\nsector 9    -1\nsector 10    3\nName: sum, dtype: int64\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"getting_started/main_concepts/#step-3-portfolio-construction","title":"Step 3: Portfolio Construction","text":"<p>To find individual stocks weights, you can use the <code>optimize()</code> method, which allows us to align the portfolio with your objectives and constraints. </p> <p>The modular structure of our code pipeline enables you to choose various optimization techniques, return estimation methods, and risk models within the <code>portfolio()</code> method. </p> <p>Additionally, the built-in functionality  incorporates custom objetives and constraints, such as lower and upper bounds for each individual stocks. You can adjust the weightings of you investment strategy in order to achieve a targeted level of volatility and market neutrality, as follow:</p> <pre><code>output = strategy.portfolio(stock_prices).optimize( # (1)\nexpected_returns=\"capm_return\", # (2)\noptimizer=\"efficient_semivariance\",  \ntarget=\"efficient_return\",\ntarget_return=0.01\nweight_bounds=(-1, 1) # (3)\nconstraints=[\nlambda w: w &lt;=  0.1, \nlambda w: w &gt;= -0.1\n]\n)\n</code></pre> <ol> <li>Because it is a top-down strategy, we want to optimize your portfolio (finding weights which align with both objectives and constraints) at the stock level</li> <li>Mean-variance optimization requires knowledge of the expected returns</li> <li>Minimum and maximum weight of each asset. When <code>weight_bounds=(-1, 1)</code>, it allows for portfolios with shorting</li> </ol> <p>Which returns an <code>OrderedDict</code> of <code>clean_weights</code>, a utility function provided by the PyPortfolioOpt library and integrated within the module. This function allows us to filter out any weightings with absolute values below a specified cutoff, setting them to zero, and rounding the remaining entities.</p> <pre><code>$ weights = pd.Series(output, name=\"weights\")\n&lt;span style=\"color: grey;\"&gt;asset 1     -0.09\nstock 2      0.09\nstock 3     -0.08\nstock 4      0.00\nstock 5      0.00\nstock 96     0.06\nstock 97    -0.04\nstock 98     0.00\nstock 99     0.00\nstock 100    0.00\nName: weights, Length: 100, dtype: float64\n&lt;/span&gt;\n</code></pre> <ol> <li> <p>Kowalski, M. (1979). Algorithm = logic + control. Communications of the ACM, 22(7), 424-436.\u00a0\u21a9</p> </li> <li> <p>Bass, L., Clements, P., &amp; Kazman, R. (2003). Software architecture in practice (2<sup>nd</sup> ed.). Boston, MA: Addison-Wesley.\u00a0\u21a9</p> </li> <li> <p>Rouwenhorst, K. G., &amp; Wu, G. S. (2001). Modular investment strategies. Journal of Financial Economics, 61(3), 369-403.\u00a0\u21a9</p> </li> <li> <p>Faber, M., &amp; O'Shaughnessy, J. (2013). Building investment strategies with building blocks. Journal of Portfolio Management, 39(4), 104-115.\u00a0\u21a9</p> </li> <li> <p>By default joblib.Parallel uses the 'loky' backend module to start separate Python worker processes to execute tasks concurrently on separate CPUs. More information available in the joblib documentation.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started/optimize_blocks/","title":"Optimize Blocks","text":"<p>Portfolio optimization is the process of selecting the optimal mix of assets in a portfolio in order to maximize returns while minimizing risk. One method of portfolio optimization involves the use of alpha scores. By selecting assets with high alpha scores, an investor can potentially achieve higher probability of positive returns while taking on the same level of risk. The optimal portfolio is typically determined through the use of statistical analysis and optimization techniques, such as mean-variance or Black-Litterman optimization.</p> <p>Portfolio optimization using alpha scores has been studied extensively in the financial literature. For example, in a study published in the Journal of Financial Economics<sup>1</sup>, Chen, Novy-Marx, and Zhang (2013) found that portfolios constructed using alpha scores significantly outperformed traditional market-cap weighted portfolios. Another study by Geczy, Musto, and Reed (2005) published in the Review of Financial Studies<sup>2</sup> found that portfolios constructed using alpha scores had higher Sharpe ratios (a measure of risk-adjusted returns) compared to traditional portfolios.</p> <p>Risk Considerations</p> <p>It's important to note that while alpha scores can be useful for portfolio optimization, they are not a guarantee of success. Like any investment strategy, portfolio optimization using alpha scores carries its own set of risks and uncertainties. It is always important for investors to carefully consider their investment goals and risk tolerance before making any investment decisions.</p> <p>There are several advanced techniques for portfolio optimization that take into account different factors such as risk, correlation between assets, liquidity, regulatory, tax or ethical constraints. Here are some of the most commonly used techniques:</p> <ul> <li>Markowitz portfolio optimization: This approach, proposed by Harry Markowitz in 1952, uses the efficient frontier theory to determine the optimal portfolio that offers the best expected return for a given level of risk</li> <li>Conditional variance portfolio optimization: This approach allows for changes in volatility and correlation between assets over time to adjust the portfolio accordingly</li> <li>Multi-objective optimization: This approach considers multiple objectives, such as return, risk, and liquidity, and finds a compromise between these objectives to obtain an optimal portfolio that meets multiple criteria.</li> <li>Robust optimization: This approach aims to find an optimal portfolio that is resistant to disturbances and uncertainties by considering different possible scenarios</li> <li>AI-based portfolio optimization: This approach uses machine learning algorithms to analyze historical financial market data and predict future trends, in order to construct an optimal portfolio that maximizes return while minimizing risk</li> </ul> <p>These different portfolio optimization techniques all have their advantages and limitations, and their choice depends on the specific goals and constraints of each investor.</p>"},{"location":"getting_started/optimize_blocks/#key-takeaways","title":"Key Takeaways","text":"<p>An interval query is used to assign scores to elements in a portfolio. These scores are then used to determine the relative weightings of the highest and lowest elements in the portfolio. The resulting portfolio reflects these weightings and adheres to any specified objectives and constraints.</p> <pre><code>flowchart TD\n    B[Strategy] --&gt; E[Exposures]\n    E -- Default Config --&gt; M[Map]\n    E -- Custom Config --&gt; M\n    M -- Bound Constraints --&gt; P[Portfolio]\n    P -- Objectives --&gt; O[Optimizer]\n    P -- Constraints --&gt; O\n    O -- Weights --&gt; P\n    P -. Analysis .-&gt; Backtests</code></pre> <p>Why Are We Not Creating Weights in the First Place?</p> <p>There are a few reasons why you might calculate scores rather than directly assigning \"weights\" to potential assets in their investment portfolios. </p> <p>One reason is that scores can be used to rank potential investments relative to one another, while weights are typically used to indicate the proportion of your total portfolio that should be allocated to a particular asset. This means that scores can be used to identify which assets are the most attractive candidates for inclusion in a portfolio, while weights are used to determine how much of an your capital should be allocated to each asset. </p> <p>Another reason is that scores can be based on a variety of different factors and criteria, whereas weights are usually based on a single factor (e.g., expected return or risk). By using scores, you can take a more holistic view of potential investments and consider multiple factors when making decisions. </p> <p>Furthermore, the allocation of weights in the portfolio would not consider the overall goals and limitations of the portfolio. It would only be based on the weights where alpha scores have been calculated, such as at the sector level.</p>"},{"location":"getting_started/optimize_blocks/#methods","title":"Methods","text":""},{"location":"getting_started/optimize_blocks/#mean-variance-optimisation","title":"Mean-Variance Optimisation","text":"<ul> <li><code>min_volatility</code> optimises for minimum volatility</li> <li><code>max_sharpe</code> optimises for maximal Sharpe ratio (a.k.a the tangency portfolio)</li> <li><code>max_quadratic_utility</code> maximises the quadratic utility, given some risk aversion</li> <li><code>efficient_risk</code> maximises return for a given target risk</li> <li><code>efficient_return</code> minimises risk for a given target return</li> </ul>"},{"location":"getting_started/optimize_blocks/#general-efficient-frontier","title":"General Efficient Frontier","text":""},{"location":"getting_started/optimize_blocks/#efficient-semi-variance","title":"Efficient Semi-variance","text":"<ul> <li><code>min_semivarianc</code>e minimises the portfolio semi-variance (downside deviation)</li> <li><code>max_quadratic_utility</code> maximises the \"downside quadratic utility\", given some risk aversion.</li> <li><code>efficient_risk</code> maximises return for a given target semi-deviation</li> <li><code>efficient_return</code> minimises semi-deviation for a given target return</li> </ul>"},{"location":"getting_started/optimize_blocks/#efficient-cvar","title":"Efficient CVaR","text":"<p>The Conditional Value-at-Risk - Expected Shortfall:</p> <ul> <li><code>min_cvar</code> minimises the CVaR</li> <li><code>efficient_risk</code> maximises return for a given CVaR</li> <li><code>efficient_return</code> minimises CVaR for a given target return</li> </ul>"},{"location":"getting_started/optimize_blocks/#efficient-cdar","title":"Efficient CDaR","text":"<p>Conditional Drawdown-at-Risk:</p> <ul> <li><code>min_cdar</code> minimises the CDaR</li> <li><code>efficient_risk</code> maximises return for a given CDaR</li> <li><code>efficient_return</code> minimises CDaR for a given target return</li> </ul>"},{"location":"getting_started/optimize_blocks/#kelly-criterion","title":"Kelly criterion","text":"<p>In probability theory, the Kelly criterion is a formula that determines the optimal theoretical size for a bet. It is valid when the expected returns are known. The Kelly bet size is found by maximizing the expected value of the logarithm of wealth, which is equivalent to maximizing the expected geometric growth rate, J. L. Kelly Jr (1956). The criterion is also known as the scientific gambling method, as it leads to higher wealth compared to any other strategy in the long run (i.e. the theoretical maximum return as the number of bets goes to infinity).</p>"},{"location":"getting_started/optimize_blocks/#hierarchical-risk-parity","title":"Hierarchical Risk Parity","text":"<p>Portfolio construction is perhaps the most recurrent financial problem. On a daily basis, investment managers must build portfolios that incorporate their views and forecasts on risks and returns. This is the primordial question that a 24 years old Harry Markowitz attempted to answer more than 6 decades ago. His monumental insight was to recognize that various levels of risk are associated with different optimal portfolios in terms of risk-adjusted returns, hence the notion of \u201cefficient frontier\u201d (Markowitz, 1952). One implication was that it is rarely optimal to allocate all assets to the investments with highest expected returns. Instead, we should take into account the correlations across alternative investments in order to build a diversified portfolio. </p> <p>A correlation matrix is a linear algebra object that measures the cosines of the angles between any two vectors in the vector space formed by the returns series (see Calkin and L\u00f3pez de Prado [2014a, 2015b]). One reason for the instability of quadratic optimizers is that the vector space is modelled as a complete (fully connected) graph, where every node is a potential candidate to substitute another. In algorithmic terms, inverting the matrix means evaluating the partial correlations across the complete graph.</p> <p>Hierarchical Risk Parity<sup>1</sup> (HRP) is a novel portfolio optimization method. HPR  applies modern mathematics (graph theory and machine learning techniques) to build a diversified portfolio based on the information contained in the covariance matrix. However, unlike quadratic optimizers, HRP does not require the invertibility of the covariance matrix. In fact, HRP can compute a portfolio on an ill-degenerated or even a singular covariance matrix, an impossible feat for quadratic optimizers Monte Carlo experiments show that HRP delivers lower out-of-sample variance than CLA, even though minimum-variance is CLA\u2019s optimization objective. HRP also produces less risky portfolios out-of-sample compared to traditional risk parity methods. The HRP method consists of several key steps:</p> <ol> <li>From a universe of assets, form a distance matrix based on the correlation of the assets</li> <li>Using this distance matrix, cluster the assets into a tree via hierarchical clustering</li> <li>Within each branch of the tree, form the minimum variance portfolio (normally between just two assets)</li> <li>Iterate over each level, optimally combining the mini-portfolios at each node</li> </ol> <p>The advantages of this are that it does not require the inversion of the covariance matrix as with traditional mean-variance optimization, and seems to produce diverse portfolios that perform well out of sample.</p>"},{"location":"getting_started/optimize_blocks/#black-litterman-allocation","title":"Black-Litterman allocation","text":"<p>Black-Litterman Approach</p> <p>This method is also particularly useful in the Black-Litterman (BL) model, which is a Bayesian approach to asset allocation. The BL model combines an initial estimate of returns with specific views on certain assets to generate a revised estimate of expected returns. This revised estimate, known as the posterior estimate, is then used to optimize the allocation of assets in accordance with a predetermined set of objectives (e.g., maximizing Sharpe ratio) and constraints.</p> <p>The Black-Litterman (BL) model takes a Bayesian approach to asset allocation. Specifically, it combines a prior estimate of returns with views on certain assets, to produce a posterior estimate of expected returns. It then optimises weights following a set of objectives (e.g. maximising Sharpe) and constraints. Effective portfolio construction is the ability to transfer investment skill efficiently into positions. </p>"},{"location":"getting_started/optimize_blocks/#from-signals-to-returns-how-to-make-signals-into-forecast-views","title":"From signals to returns: How to make signals into forecast views?","text":"<p>In the BL model, users can either provide absolute or relative views:</p> <ul> <li>Absolute views are statements: \"asset 1 indicates a return of 10%\" or \"asset 2 indicates a drop of 40%\"</li> <li>Relative views, on the other hand, are statements: \"asset 2 indicates it will outperform asset 1 by 3%\"</li> </ul> <p>These views need to be provided to the model in the form of a return estimate. These estimates can be provided for either all or any subset of investables used by the model. The process of converting asset views to returns is necessary in order to be used in an optimiser.</p>"},{"location":"getting_started/optimize_blocks/#confidence","title":"Confidence","text":"<p>The BL formula simply represents a weighted average between the prior estimate of returns and the views, where the weighting is determined by the confidence in the views</p> <ul> <li>Confidence is extracted from factor scores themselves</li> <li>Factor scores range from -1 to 1, with a score of 0 being the average or neutral value</li> <li>In a long/short strategy, 1 would be the highest confidence from the long leg and -1 would also be the highest confidence from the short leg. The closer a score is to the mean, the smaller the confidence in that score</li> </ul>"},{"location":"getting_started/optimize_blocks/#black-litterman-entropy-pooling","title":"Black-Litterman Entropy Pooling","text":"<p>Additionaly, this extended version of the BL method is particularly useful when managing a top-down strategy. It consists of several key steps::</p> <ol> <li>The Black-Litterman model is a mathematical framework for combining subjective views or forecasts with objective market data in order to generate improved portfolio weight estimates</li> <li>The entropy pooling approach is a specific method within the Black-Litterman framework for combining multiple views, where each view is represented as a ranking of assets rather than as a specific weighting</li> <li>In the entropy pooling approach, the subjective views are transformed into probability distributions over the set of assets, with the probability of each asset being proportional to its rank in the view</li> <li>These probability distributions are then combined using a process called entropy pooling, which involves taking the weighted average of the individual probability distributions while minimizing the amount of uncertainty or \"entropy\" in the resulting combined distribution</li> <li>The resulting combined probability distribution can then be used to generate improved portfolio weight estimates by solving a optimization problem that takes into account both the market data and the subjective views. The optimization problem can be formulated as a quadratic programming problem and solved using standard optimization techniques</li> </ol>"},{"location":"getting_started/optimize_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Portfolio construction, which involves optimizing the allocation of assets within a portfolio, can be a complex and nuanced process. We have developed a method that allows for greater flexibility and experimentation in the portfolio optimization process. This approach enables the exploration of a wide range of potential portfolio compositions, and the example provided illustrates this method applied from the initial stages of portfolio construction.</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion\nstrategy = Strategy([(\"reversion\", Reversion)]).fit(df).estimate(sum) # (1)\n</code></pre> <ol> <li>Calculate sentiment using Reversion Ranking Method.     More information provided in the Blocks.</li> </ol>"},{"location":"getting_started/optimize_blocks/#step-1-portfolio","title":"Step 1: Portfolio","text":"<p>We aim to find weights for a large universe of stocks:</p> <pre><code>strategy.portfolio(data=stock_prices) # (1)\n</code></pre> <ol> <li>pandas.DataFrame object, with specifiy the variation of stock prices over time.</li> </ol>"},{"location":"getting_started/optimize_blocks/#step-2-optimize","title":"Step 2: Optimize","text":"<p>The wrapper class inherite from <code>PortfolioConstruction</code>, which adds the <code>optimize</code> public method to your toolbox, allowing for the efficient computation of optimized asset weights. Constraints are lambda functions (e.i. all assets must be lower or equal to 10% of the total portfolio would simply translate to <code>[lambda w: w &lt;= .1]</code>. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint. Here is an example with \"min_volatility\", which finds the minimum risk portfolio:</p> <pre><code>weights = strategy.optimize(  \nweight_bounds=(-1, 1), # (1)\ntarget=\"min_volatility\",\nconstraints=[lambda w: w &lt;= .1]\n)\n</code></pre> <ol> <li><code>weight_bounds</code> parameter serves as a constraint by limiting the minimum and maximum weight of each asset in portfolios. Because it ranges from <code>-1</code> to <code>1</code>, it allows Long and Shorts.</li> </ol> <pre><code>$ pd.Series(weights, name=\"weights\")\n&lt;span style=\"color: grey;\"&gt;asset 1      0.10\nasset 2      0.03\nasset 3     -0.02\nasset 4      0.03\nasset 5     -0.05\nasset 96     0.00\nasset 97    -0.09\nasset 98     0.00\nasset 99    -0.07\nasset 100    0.00\nName: weights, Length: 100, dtype: float64\n&lt;/span&gt;\n</code></pre> <p> </p> Figure 1: Example Weights <ol> <li> <p>Chen, L., Novy-Marx, R., &amp; Zhang, L. (2013). Factor Premia and Interaction with the Market Portfolio. Journal of Financial Economics, 110(1), 1-35.\u00a0\u21a9\u21a9</p> </li> <li> <p>Geczy, C., Musto, D., &amp; Reed, A. (2005). A simple approach to performance attribution for hedge funds: the case of equity market neutral strategies. Review of Financial Studies, 18(2), 367-384.\u00a0\u21a9</p> </li> </ol>"},{"location":"terms/","title":"Terms","text":""},{"location":"terms/#private-licence","title":"Private Licence","text":"<p>ActurialCapital - Opendesk (opendesk) is hosted in a private repository on GitHub. The access to the repository is licensed and granted by invitation only on a paid basis. The users pay for the continued access to the repository, including updates, support, and maintenance.</p> <p>If your license expires, your software or system will not break down and will keep working fine with your most recent version by that time. However, if you discontinue your paid license you will lose any updates and (critical) bugfixes that will follow.</p>"},{"location":"terms/#important-note","title":"Important note","text":"<p>It is illegal to vendor, publish, distribute, or commercialize the opendesk source code without a separate permission. Violation of the licensing terms will trigger a ban followed by a legal pursuit.</p> <p>Any licensed user, developer, team, or company, having obtained paid access to the opendesk repository from us, can use opendesk as a dependency, subject to the terms and limitations of the paid subscription plans.</p> <p>Licensees can use, copy, and modify opendesk as long as they do not vendor, publish, distribute, or commercialize the source code of opendesk.</p> <p>It is allowed to specify opendesk as a dependency of your software as long as you do not include a copy of the opendesk source code in your software.</p> <p>If you are a software developer you should specify opendesk as your requirement. The end-user of your software is responsible for obtaining his own individual PRO license. The best practice is to make it clear in your docs or on your website.</p>"},{"location":"terms/license/","title":"License","text":"<p>Copyright \u00a9 ActurialCapital - Opendesk</p> <p>Permission is hereby granted to any person or organization obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software including the rights to use, copy and modify the Software, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>This ActurialCapital - Opendesk License DOES NOT grant a permission to publish, distribute, sublicense, and/or Sell copies of the Software or substantial portions of the Software, or to permit persons to whom the Software is furnished to do so. Permission to publish, distribute, sublicense, and/or Sell copies of the Software or substantial portions of the Software, and to permit persons to whom the Software is furnished to do so, is SUBJECT TO A SEPARATE AGREEMENT.</p> <p>For purposes of the foregoing, \u201cSell\u201d means practicing any or all of the rights granted to you under the License to provide to third parties, for a fee or other consideration (including without limitation fees for hosting or consulting/ support services related to the Software), a product or service whose value derives, entirely or substantially, from the functionality of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"}]}